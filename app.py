"""
GP_Partner_Ultimate - é«˜çº§æŠ•èµ„è¾…åŠ©ç³»ç»Ÿ
æ”¯æŒå¤šæ¨¡å¼åˆ‡æ¢ã€OpenRouter APIã€å†å²è®°å½•å’ŒæŠ¥å‘Šå¯¼å‡º
"""

import streamlit as st
import os
import json
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, List, Union
import re
import zipfile
import io
import hashlib
import time

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
try:
    from openai import OpenAI
    from dotenv import load_dotenv
    import PyPDF2
    from docx import Document
    from docx.shared import Pt, RGBColor
    from docx.enum.text import WD_ALIGN_PARAGRAPH
    import pandas as pd
except ImportError as e:
    st.error(f"âŒ ç¼ºå°‘å¿…è¦çš„ä¾èµ–åº“: {e}")
    st.stop()

# æ³¨ï¼šå·²æ”¾å¼ƒå‘é‡æ£€ç´¢æ–¹æ¡ˆï¼Œæ”¹ç”¨åŸºäº LLM æ ‡ç­¾æå–çš„è½»é‡çº§è®°å¿†ç³»ç»Ÿ

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="GP Partner Ultimate",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded"
)

# å¸¸é‡å®šä¹‰
PROMPTS_DIR = Path("prompts")
HISTORY_DIR = Path("history_data")
BASE_URL = "https://openrouter.ai/api/v1"
DEFAULT_MODEL = "google/gemini-flash-1.5"
KNOWLEDGE_BASE_FILE = Path("project_database.csv")
MEMORY_STORE_FILE = Path("memory_store.json")  # åŸºäºæ ‡ç­¾çš„è®°å¿†å­˜å‚¨æ–‡ä»¶
EVOLUTION_LOG_FILE = Path("evolution_log.md")  # AI è¿›åŒ–æ—¥å¿—æ–‡ä»¶
CONFIG_FILE = Path("config.json")  # é…ç½®æŒä¹…åŒ–æ–‡ä»¶

# ç¡®ä¿å¿…è¦çš„æ–‡ä»¶å¤¹å­˜åœ¨
HISTORY_DIR.mkdir(exist_ok=True)
PROMPTS_DIR.mkdir(exist_ok=True)


# ==================== é…ç½®æŒä¹…åŒ–ç³»ç»Ÿ ====================

def load_config() -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        if CONFIG_FILE.exists():
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                config = json.load(f)
                return config
        else:
            # è¿”å›é»˜è®¤é…ç½®
            return {
                "api_key": os.getenv("OPENROUTER_API_KEY", ""),
                "base_url": BASE_URL,
                "model": DEFAULT_MODEL
            }
    except Exception as e:
        print(f"âš ï¸ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return {
            "api_key": os.getenv("OPENROUTER_API_KEY", ""),
            "base_url": BASE_URL,
            "model": DEFAULT_MODEL
        }


def save_config():
    """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
    try:
        config = {
            "api_key": st.session_state.get("api_key_input", ""),
            "base_url": st.session_state.get("base_url_input", BASE_URL),
            "model": st.session_state.get("model_input", DEFAULT_MODEL)
        }
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {e}")


# ==================== åŸºäº LLM æ ‡ç­¾æå–çš„è½»é‡çº§è®°å¿†ç³»ç»Ÿ ====================

def extract_tags_from_text(text: str, client: OpenAI, model: str) -> List[str]:
    """ä½¿ç”¨ LLM ä»æ–‡æœ¬ä¸­æå–é¡¹ç›®æ ‡ç­¾"""
    try:
        tag_extraction_prompt = f"""è¯·ä»ä»¥ä¸‹é¡¹ç›®æ–‡æœ¬ä¸­æå–3-5ä¸ªæ ¸å¿ƒæ ‡ç­¾ï¼ˆå…³é”®è¯ï¼‰ï¼Œç”¨äºé¡¹ç›®åˆ†ç±»å’ŒåŒ¹é…ã€‚

æ–‡æœ¬å†…å®¹ï¼š
{text[:1000]}

è¦æ±‚ï¼š
1. æ ‡ç­¾åº”è¯¥æ˜¯é¡¹ç›®çš„æ ¸å¿ƒæŠ€æœ¯ã€è¡Œä¸šã€å•†ä¸šæ¨¡å¼ç­‰å…³é”®ç‰¹å¾
2. è¿”å›æ ¼å¼ï¼šçº¯æ–‡æœ¬ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¸è¦ä½¿ç”¨ä»»ä½•æ ‡è®°ç¬¦å·
3. ä¾‹å¦‚ï¼šAI, åŒ»ç–—, B2B, SaaS, å¤šæ¨¡æ€

æ ‡ç­¾ï¼š"""

        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é¡¹ç›®æ ‡ç­¾æå–åŠ©æ‰‹ã€‚åªéœ€è¿”å›æ ‡ç­¾ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€‚"},
                {"role": "user", "content": tag_extraction_prompt}
            ],
            temperature=0.3,
            max_tokens=100
        )
        
        tags_text = response.choices[0].message.content.strip()
        # è§£ææ ‡ç­¾
        tags = [tag.strip() for tag in tags_text.split(',') if tag.strip()]
        return tags[:5]  # æœ€å¤šè¿”å›5ä¸ªæ ‡ç­¾
    except Exception as e:
        print(f"âš ï¸ æ ‡ç­¾æå–å¤±è´¥: {e}")
        return []


class MemoryManager:
    """åŸºäº LLM æ ‡ç­¾æå–çš„è½»é‡çº§è®°å¿†ç³»ç»Ÿç®¡ç†å™¨ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    _instance = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(MemoryManager, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        
        self.memory_store_path = MEMORY_STORE_FILE
        self.memories = []
        self.enabled = True
        
        # åŠ è½½è®°å¿†åº“
        self._load_memories()
        self._initialized = True
    
    def _load_memories(self):
        """ä» JSON æ–‡ä»¶åŠ è½½è®°å¿†åº“"""
        try:
            if self.memory_store_path.exists():
                with open(self.memory_store_path, 'r', encoding='utf-8') as f:
                    self.memories = json.load(f)
            else:
                self.memories = []
        except Exception as e:
            print(f"âš ï¸ åŠ è½½è®°å¿†åº“å¤±è´¥: {e}")
            self.memories = []
    
    def _save_memories(self):
        """ä¿å­˜è®°å¿†åº“åˆ° JSON æ–‡ä»¶"""
        try:
            with open(self.memory_store_path, 'w', encoding='utf-8') as f:
                json.dump(self.memories, f, ensure_ascii=False, indent=2)
            return True
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜è®°å¿†åº“å¤±è´¥: {e}")
            return False
    
    def query_similar(self, text: str, client: OpenAI, model: str, top_k: int = 3) -> List[Dict]:
        """åŸºäºæ ‡ç­¾åŒ¹é…æŸ¥è¯¢ç›¸ä¼¼çš„å†å²é¡¹ç›®"""
        if not self.enabled or len(self.memories) == 0:
            return []
        
        try:
            # æå–å½“å‰é¡¹ç›®çš„æ ‡ç­¾
            query_tags = extract_tags_from_text(text, client, model)
            if not query_tags:
                return []
            
            # è®¡ç®—æ¯ä¸ªå†å²é¡¹ç›®çš„æ ‡ç­¾åŒ¹é…åº¦
            scored_projects = []
            for memory in self.memories:
                memory_tags = memory.get('tags', [])
                if not memory_tags:
                    continue
                
                # è®¡ç®—æ ‡ç­¾äº¤é›†æ•°é‡ï¼ˆç®€å•çš„æ ‡ç­¾åŒ¹é…ï¼‰
                common_tags = set(query_tags) & set(memory_tags)
                match_score = len(common_tags) / max(len(query_tags), len(memory_tags)) if max(len(query_tags), len(memory_tags)) > 0 else 0
                
                if match_score > 0:  # è‡³å°‘æœ‰ä¸€ä¸ªå…±åŒæ ‡ç­¾
                    scored_projects.append({
                        'name': memory.get('id', 'æœªçŸ¥é¡¹ç›®'),
                        'score': memory.get('score', 'N/A'),
                        'summary': memory.get('summary', ''),
                        'match_score': match_score,
                        'common_tags': list(common_tags)
                    })
            
            # æŒ‰åŒ¹é…åº¦æ’åºï¼Œè¿”å›å‰ top_k ä¸ª
            scored_projects.sort(key=lambda x: x['match_score'], reverse=True)
            return scored_projects[:top_k]
        except Exception as e:
            print(f"âš ï¸ æŸ¥è¯¢ç›¸ä¼¼é¡¹ç›®å¤±è´¥: {e}")
            return []
    
    def add_memory(self, name: str, summary: str, full_text: str, score, tags: List[str] = None, meta: Dict = None):
        """æ·»åŠ é¡¹ç›®åˆ°è®°å¿†åº“"""
        if not self.enabled:
            return False
        
        try:
            # å¦‚æœæœªæä¾›æ ‡ç­¾ï¼Œä½¿ç”¨ç©ºåˆ—è¡¨ï¼ˆæ ‡ç­¾ä¼šåœ¨å¤–éƒ¨é€šè¿‡ LLM æå–ï¼‰
            if tags is None:
                tags = []
            
            memory_entry = {
                "id": name,
                "summary": summary,
                "score": score,
                "tags": tags,
                "timestamp": datetime.now().isoformat()
            }
            
            if meta:
                memory_entry.update(meta)
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨åŒåé¡¹ç›®ï¼Œå¦‚æœå­˜åœ¨åˆ™æ›´æ–°ï¼Œå¦åˆ™æ·»åŠ 
            existing_index = None
            for i, mem in enumerate(self.memories):
                if mem.get('id') == name:
                    existing_index = i
                    break
            
            if existing_index is not None:
                self.memories[existing_index] = memory_entry
            else:
                self.memories.append(memory_entry)
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            return self._save_memories()
        except Exception as e:
            print(f"âš ï¸ æ·»åŠ è®°å¿†å¤±è´¥: {e}")
            return False
    
    def get_count(self) -> int:
        """è·å–çŸ¥è¯†åº“ä¸­çš„é¡¹ç›®æ•°é‡"""
        return len(self.memories) if self.enabled else 0


# å…¨å±€ MemoryManager å®ä¾‹
memory_manager = MemoryManager()


def load_prompt_files() -> Dict[str, str]:
    """æ‰«æ prompts æ–‡ä»¶å¤¹ï¼ŒåŠ è½½æ‰€æœ‰ .txt æ–‡ä»¶"""
    prompt_files = {}
    if not PROMPTS_DIR.exists():
        return prompt_files
    
    txt_files = list(PROMPTS_DIR.glob("*.txt"))
    for file_path in sorted(txt_files):
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read().strip()
                # ä½¿ç”¨æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ä½œä¸ºé”®
                name = file_path.stem
                prompt_files[name] = content
        except Exception as e:
            st.warning(f"âš ï¸ æ— æ³•è¯»å–æ–‡ä»¶ {file_path.name}: {e}")
    
    return prompt_files


def extract_text_from_pdf(file) -> str:
    """ä» PDF æ–‡ä»¶ä¸­æå–æ–‡æœ¬"""
    try:
        pdf_reader = PyPDF2.PdfReader(file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return text.strip()
    except Exception as e:
        st.error(f"âŒ PDF è§£æé”™è¯¯: {e}")
        return ""


def extract_text_from_docx(file) -> str:
    """ä» DOCX æ–‡ä»¶ä¸­æå–æ–‡æœ¬"""
    try:
        doc = Document(file)
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        return text.strip()
    except Exception as e:
        st.error(f"âŒ DOCX è§£æé”™è¯¯: {e}")
        return ""


def extract_text_from_uploaded_file(uploaded_file) -> Optional[str]:
    """æ ¹æ®æ–‡ä»¶ç±»å‹æå–æ–‡æœ¬"""
    file_extension = Path(uploaded_file.name).suffix.lower()
    
    if file_extension == ".pdf":
        return extract_text_from_pdf(uploaded_file)
    elif file_extension in [".docx", ".doc"]:
        return extract_text_from_docx(uploaded_file)
    else:
        st.error(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}")
        return None


def save_history_entry(mode_name: str, file_name: str, analysis_content: str):
    """ä¿å­˜å†å²è®°å½•åˆ° JSON æ–‡ä»¶"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    entry = {
        "mode": mode_name,
        "file_name": file_name,
        "timestamp": timestamp,
        "datetime": datetime.now().isoformat(),
        "content": analysis_content
    }
    
    filename = f"{timestamp}_{mode_name.replace(' ', '_')}.json"
    filepath = HISTORY_DIR / filename
    
    try:
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(entry, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        st.error(f"âŒ ä¿å­˜å†å²è®°å½•å¤±è´¥: {e}")
        return False


def load_history_entries() -> List[Dict]:
    """åŠ è½½æ‰€æœ‰å†å²è®°å½•"""
    entries = []
    if not HISTORY_DIR.exists():
        return entries
    
    json_files = sorted(HISTORY_DIR.glob("*.json"), reverse=True)
    for filepath in json_files:
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                entry = json.load(f)
                entries.append(entry)
        except Exception as e:
            st.warning(f"âš ï¸ æ— æ³•è¯»å–å†å²è®°å½• {filepath.name}: {e}")
    
    return entries


def clean_markdown_for_display(markdown_text: str) -> str:
    """æ¸…ç† Markdown æ–‡æœ¬ï¼Œç§»é™¤ Tags å¤´éƒ¨ã€JSON ä»£ç å—å’Œè¿›åŒ–å»ºè®®ï¼ˆç”¨äºæ˜¾ç¤ºå’Œå¯¼å‡ºï¼‰"""
    cleaned = markdown_text
    # ç§»é™¤ Tags å¤´éƒ¨
    cleaned = re.sub(r'---TAGS:\s*\[[^\]]+\]---\s*\n?', '', cleaned)
    # ç§»é™¤ ```json ... ``` ä»£ç å—
    pattern = r'```json\s*[\s\S]*?```'
    cleaned = re.sub(pattern, '', cleaned, flags=re.DOTALL)
    # ç§»é™¤è¿›åŒ–å»ºè®®ç« èŠ‚
    cleaned = re.sub(r'##\s*ğŸ§¬\s*è¿›åŒ–å»ºè®®\s*\n.*?(?=\n```json|\n```|$)', '', cleaned, flags=re.DOTALL)
    # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
    cleaned = re.sub(r'\n{3,}', '\n\n', cleaned)
    return cleaned.strip()


def markdown_to_docx(markdown_text: str, output_path: str):
    """å°† Markdown æ–‡æœ¬è½¬æ¢ä¸º Word æ–‡æ¡£"""
    # æ¸…ç† JSON ä»£ç å—
    markdown_text = clean_markdown_for_display(markdown_text)
    
    doc = Document()
    
    lines = markdown_text.split('\n')
    
    for line in lines:
        line = line.strip()
        
        if not line:
            # ç©ºè¡Œ
            doc.add_paragraph()
            continue
        
        # æ£€æŸ¥æ ‡é¢˜å±‚çº§
        if line.startswith('# '):
            # H1 æ ‡é¢˜
            heading = doc.add_heading(line[2:].strip(), level=1)
        elif line.startswith('## '):
            # H2 æ ‡é¢˜
            heading = doc.add_heading(line[3:].strip(), level=2)
        elif line.startswith('### '):
            # H3 æ ‡é¢˜
            heading = doc.add_heading(line[4:].strip(), level=3)
        elif line.startswith('#### '):
            # H4 æ ‡é¢˜
            heading = doc.add_heading(line[5:].strip(), level=4)
        elif line.startswith('- ') or line.startswith('* '):
            # åˆ—è¡¨é¡¹
            para = doc.add_paragraph(line[2:].strip(), style='List Bullet')
        elif line.startswith(('  - ', '  * ', '    - ', '    * ')):
            # åµŒå¥—åˆ—è¡¨é¡¹
            indent_level = len(line) - len(line.lstrip())
            para = doc.add_paragraph(line.lstrip()[2:].strip(), style='List Bullet')
            para.paragraph_format.left_indent = Pt(indent_level * 12)
        elif re.match(r'^\d+\.\s+', line):
            # æœ‰åºåˆ—è¡¨
            para = doc.add_paragraph(line, style='List Number')
        elif line.startswith('**') and line.endswith('**'):
            # ç²—ä½“æ–‡æœ¬
            para = doc.add_paragraph()
            para.add_run(line[2:-2]).bold = True
        elif line.startswith('*') and line.endswith('*') and not line.startswith('**'):
            # æ–œä½“æ–‡æœ¬
            para = doc.add_paragraph()
            para.add_run(line[1:-1]).italic = True
        else:
            # æ™®é€šæ®µè½ - å¤„ç†è¡Œå†…æ ¼å¼ï¼ˆç²—ä½“ã€æ–œä½“ç­‰ï¼‰
            para = doc.add_paragraph()
            # å¤„ç†ç²—ä½“ **text**
            if '**' in line:
                parts = re.split(r'(\*\*[^\*]+?\*\*)', line)
                for part in parts:
                    if part.startswith('**') and part.endswith('**'):
                        para.add_run(part[2:-2]).bold = True
                    elif part.strip():
                        # å¤„ç†æ–œä½“ *text*
                        if '*' in part and not part.startswith('*'):
                            italic_parts = re.split(r'(\*[^\*]+\*)', part)
                            for italic_part in italic_parts:
                                if italic_part.startswith('*') and italic_part.endswith('*') and len(italic_part) > 2:
                                    para.add_run(italic_part[1:-1]).italic = True
                                elif italic_part.strip():
                                    para.add_run(italic_part)
                        else:
                            para.add_run(part)
            else:
                # æ²¡æœ‰ç²—ä½“æ ‡è®°ï¼Œç›´æ¥æ·»åŠ æ–‡æœ¬
                para.add_run(line)
    
    try:
        doc.save(output_path)
        return True
    except Exception as e:
        st.error(f"âŒ ä¿å­˜ Word æ–‡æ¡£å¤±è´¥: {e}")
        return False


def enhance_system_prompt(base_prompt: str, similar_projects: List[Dict] = None) -> str:
    """å¢å¼ºç³»ç»Ÿæç¤ºè¯ï¼Œè¦æ±‚ LLM ä¸¥æ ¼æŒ‰ç…§ç‰¹å®šæ ¼å¼è¾“å‡º"""
    
    # è¾“å‡ºæ ¼å¼è¦æ±‚
    format_instruction = """

---
ã€ä¸¥æ ¼è¾“å‡ºæ ¼å¼è¦æ±‚ã€‘

ä½ çš„è¾“å‡ºå¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ä¸‰éƒ¨åˆ†ç»“æ„ï¼š

**ç¬¬ä¸€éƒ¨åˆ† - æ ‡ç­¾å¤´éƒ¨ï¼ˆå¿…é¡»ï¼‰**ï¼š
åœ¨å†…å®¹æœ€å¼€å§‹ï¼Œè¾“å‡ºä¸€è¡Œï¼š
---TAGS: ["æ ‡ç­¾1", "æ ‡ç­¾2", "æ ‡ç­¾3", "æ ‡ç­¾4", "æ ‡ç­¾5"]---

**ç¬¬äºŒéƒ¨åˆ† - åˆ†ææ­£æ–‡**ï¼š
æ­£å¸¸çš„ Markdown æ ¼å¼åˆ†ææŠ¥å‘Šï¼ŒåŒ…å«æ‰€æœ‰åˆ†æå†…å®¹ã€‚

**ç¬¬ä¸‰éƒ¨åˆ† - è¿›åŒ–å»ºè®®ï¼ˆå¿…é¡»ï¼‰**ï¼š
åœ¨æŠ¥å‘Šæœ€åï¼Œå¿…é¡»åŒ…å«ä¸€ä¸ªç« èŠ‚ï¼š
## ğŸ§¬ è¿›åŒ–å»ºè®®
[é’ˆå¯¹æœ¬æ¬¡åˆ†æï¼Œåæ€å½“å‰ System Prompt çš„ä¸è¶³ï¼Œå¹¶ç»™å‡ºå…·ä½“çš„ä¼˜åŒ–æŒ‡ä»¤å»ºè®®ã€‚å»ºè®®åº”è¯¥å…·ä½“ã€å¯æ“ä½œï¼Œä¾‹å¦‚ï¼š"å»ºè®®åœ¨æç¤ºè¯ä¸­å¢åŠ å¯¹XXèµ›é“çš„ç‰¹æ®Šå…³æ³¨"æˆ–"å»ºè®®æ˜ç¡®è¦æ±‚åˆ†æXXç»´åº¦çš„é£é™©"ç­‰]

**ç¬¬å››éƒ¨åˆ† - JSON æ•°æ®å—ï¼ˆå¿…é¡»ï¼‰**ï¼š
åœ¨è¿›åŒ–å»ºè®®ä¹‹åï¼Œé™„å¸¦ä¸€ä¸ª JSON æ•°æ®å—ï¼š

```json
{
  "project_name": "é¡¹ç›®åç§°",
  "industry": "æ‰€å±èµ›é“/è¡Œä¸šï¼ˆå¦‚ï¼šAI Agent, å…·èº«æ™ºèƒ½ï¼‰",
  "tags": ["æŠ€æœ¯æ ‡ç­¾1", "æŠ€æœ¯æ ‡ç­¾2", "æŠ€æœ¯æ ‡ç­¾3"],
  "stage": "èèµ„é˜¶æ®µï¼ˆå¦‚ï¼šAngel, Pre-A, Aè½®, Bè½®ç­‰ï¼‰",
  "score": 8,
  "summary": "ä¸€å¥è¯æ ¸å¿ƒè¯„ä»·ï¼ˆ50å­—ä»¥å†…ï¼‰",
  "risk_level": "High/Medium/Low"
}
```

æ³¨æ„ï¼š
- project_name: ä»å•†ä¸šè®¡åˆ’ä¹¦ä¸­æå–çš„é¡¹ç›®åç§°ï¼ˆå­—ç¬¦ä¸²ï¼‰
- industry: æ‰€å±èµ›é“/è¡Œä¸šï¼ˆå­—ç¬¦ä¸²ï¼Œå¦‚ï¼šAI Agent, å…·èº«æ™ºèƒ½, SaaS, åŒºå—é“¾ç­‰ï¼‰
- tags: æŠ€æœ¯æ ‡ç­¾åˆ—è¡¨ï¼ˆæ•°ç»„ï¼Œå¦‚ï¼š["RAG", "LLM", "SaaS", "å¤šæ¨¡æ€"]ï¼‰ï¼Œå¿…é¡»ä¸ç¬¬ä¸€éƒ¨åˆ†çš„ TAGS ä¿æŒä¸€è‡´
- stage: èèµ„é˜¶æ®µï¼ˆå­—ç¬¦ä¸²ï¼Œå¦‚ï¼šAngel, Pre-A, Aè½®, Bè½®ç­‰ï¼Œå¦‚æœæœªæåŠåˆ™å¡«å†™ "æœªæŠ«éœ²"ï¼‰
- score: æŠ•èµ„æ¨èè¯„åˆ†ï¼ˆæ•´æ•°ï¼Œ1-10åˆ†ï¼Œ10åˆ†ä¸ºæœ€é«˜ï¼‰ï¼Œå¿…é¡»åœ¨æ­£æ–‡ä¸­æ˜ç¡®æ˜¾ç¤º
- summary: ä¸€å¥è¯æ ¸å¿ƒè¯„ä»·ï¼ˆå­—ç¬¦ä¸²ï¼Œ50å­—ä»¥å†…ï¼Œç®€æ´æ¦‚æ‹¬é¡¹ç›®ä»·å€¼å’Œé£é™©ï¼‰
- risk_level: é£é™©ç­‰çº§ï¼ˆå­—ç¬¦ä¸²ï¼Œå¿…é¡»æ˜¯ "High", "Medium", "Low" ä¹‹ä¸€ï¼‰

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸Šæ ¼å¼è¾“å‡ºï¼Œä¸è¦é—æ¼ä»»ä½•éƒ¨åˆ†ã€‚
"""
    
    # å¦‚æœæœ‰ç›¸ä¼¼é¡¹ç›®ï¼Œæ·»åŠ å†å²é¡¹ç›®å‚è€ƒä¿¡æ¯
    if similar_projects and len(similar_projects) > 0:
        similar_info = "\n\n---\nã€å†å²é¡¹ç›®å‚è€ƒã€‘çŸ¥è¯†åº“ä¸­å‘ç°äº†ä¸æœ¬é¡¹ç›®ç›¸ä¼¼çš„è¿‡å¾€é¡¹ç›®ï¼ˆåŸºäºæ ‡ç­¾åŒ¹é…ï¼‰ï¼š\n"
        for proj in similar_projects:
            common_tags_str = ', '.join(proj.get('common_tags', [])) if proj.get('common_tags') else 'æ— '
            match_score = proj.get('match_score', 0)
            similar_info += f"- {proj['name']} (è¯„åˆ†: {proj['score']}, æ ¸å¿ƒè¯„ä»·: {proj['summary']}, å…±åŒæ ‡ç­¾: {common_tags_str}, åŒ¹é…åº¦: {match_score:.2%})\n"
        similar_info += "\nè¯·åœ¨åˆ†ææ—¶æ¨ªå‘å¯¹æ¯”ï¼ŒæŒ‡å‡ºæœ¬é¡¹ç›®çš„å·®å¼‚åŒ–ä¼˜åŠ¿æˆ–é‡å¤é€ è½®å­çš„é£é™©ã€‚å‚è€ƒè¿™äº›å†å²æ¡ˆä¾‹ï¼Œä½†ä¸è¦è¢«å…¶å±€é™ï¼Œé‡ç‚¹åˆ†æå½“å‰é¡¹ç›®çš„ç‹¬ç‰¹ä»·å€¼ã€‚\n"
        return base_prompt + similar_info + format_instruction
    
    return base_prompt + format_instruction


def extract_tags_from_response(response_text: str) -> List[str]:
    """ä» LLM å“åº”ä¸­æå– Tagsï¼ˆä»å¤´éƒ¨ ---TAGS: ... --- æ ¼å¼ï¼‰"""
    try:
        # åŒ¹é… ---TAGS: ["æ ‡ç­¾1", "æ ‡ç­¾2"]--- æ ¼å¼
        pattern = r'---TAGS:\s*(\[[^\]]+\])---'
        match = re.search(pattern, response_text)
        
        if match:
            tags_str = match.group(1)
            # è§£æ JSON æ•°ç»„
            tags = json.loads(tags_str)
            if isinstance(tags, list):
                return [str(tag).strip() for tag in tags if tag]
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä» JSON æ•°æ®å—ä¸­æå–
        json_data = extract_json_from_response(response_text)
        if json_data and 'tags' in json_data:
            tags = json_data.get('tags', [])
            if isinstance(tags, list):
                return [str(tag).strip() for tag in tags if tag]
        
        return []
    except Exception as e:
        print(f"âš ï¸ æå– Tags å¤±è´¥: {e}")
        return []


def extract_evolution_suggestion(response_text: str) -> Optional[str]:
    """ä» LLM å“åº”ä¸­æå–è¿›åŒ–å»ºè®®ï¼ˆ## ğŸ§¬ è¿›åŒ–å»ºè®® åçš„å†…å®¹ï¼‰"""
    try:
        # åŒ¹é… ## ğŸ§¬ è¿›åŒ–å»ºè®® åçš„å†…å®¹ï¼ˆç›´åˆ° JSON ä»£ç å—æˆ–æ–‡ä»¶æœ«å°¾ï¼‰
        pattern = r'##\s*ğŸ§¬\s*è¿›åŒ–å»ºè®®\s*\n(.*?)(?=\n```json|\n```|$)'
        match = re.search(pattern, response_text, re.DOTALL)
        
        if match:
            suggestion = match.group(1).strip()
            # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
            suggestion = re.sub(r'\n{3,}', '\n\n', suggestion)
            return suggestion if suggestion else None
        
        return None
    except Exception as e:
        print(f"âš ï¸ æå–è¿›åŒ–å»ºè®®å¤±è´¥: {e}")
        return None


def extract_score_enhanced(response_text: str, json_data: Optional[Dict] = None) -> Optional[Union[int, float, str]]:
    """å¢å¼ºçš„åˆ†æ•°æå–å‡½æ•°ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
    # é¦–å…ˆå°è¯•ä» JSON æ•°æ®ä¸­æå–
    if json_data and 'score' in json_data:
        score = json_data.get('score')
        if score and score != 'N/A':
            try:
                if isinstance(score, (int, float)):
                    return score
                score_str = str(score).strip()
                # å°è¯•è½¬æ¢ä¸ºæ•°å­—
                if score_str.replace('.', '').isdigit():
                    return float(score_str) if '.' in score_str else int(score_str)
            except:
                pass
    
    # åœ¨å‰ 1000 å­—ç¬¦ä¸­æœç´¢åˆ†æ•°
    search_text = response_text[:1000]
    
    # æ¨¡å¼1: Score: 8 æˆ– è¯„åˆ†ï¼š8.5
    patterns = [
        r'(?:Score|è¯„åˆ†|åˆ†æ•°|æŠ•èµ„è¯„åˆ†)[:ï¼š]\s*(\d+(?:\.\d+)?)',
        r'\[(\d+(?:\.\d+)?)åˆ†\]',
        r'è¯„åˆ†[ï¼š:]\s*(\d+(?:\.\d+)?)',
        r'(\d+(?:\.\d+)?)\s*åˆ†',
        r'(\d+(?:\.\d+)?)\s*/\s*10',  # x/10 æ ¼å¼
    ]
    
    for pattern in patterns:
        match = re.search(pattern, search_text, re.IGNORECASE)
        if match:
            try:
                score_val = float(match.group(1))
                if 1 <= score_val <= 10:
                    return int(score_val) if score_val.is_integer() else score_val
            except:
                continue
    
    # åœ¨æ•´ä¸ªæ–‡æœ¬ä¸­æœç´¢ x/10 æ ¼å¼ï¼ˆå…œåº•ç­–ç•¥ï¼‰
    x_10_pattern = r'(\d+(?:\.\d+)?)\s*/\s*10'
    matches = re.findall(x_10_pattern, response_text)
    if matches:
        for match in matches:
            try:
                score_val = float(match)
                if 1 <= score_val <= 10:
                    return int(score_val) if score_val.is_integer() else score_val
            except:
                continue
    
    return None


def parse_llm_response(response_text: str) -> Dict:
    """ç»Ÿä¸€è§£æ LLM å“åº”ï¼Œæå–æ‰€æœ‰ç»“æ„åŒ–ä¿¡æ¯"""
    result = {
        'tags': [],
        'json_data': None,
        'score': None,
        'evolution_suggestion': None,
        'body_content': response_text  # åŸå§‹å†…å®¹
    }
    
    # æå– Tags
    result['tags'] = extract_tags_from_response(response_text)
    
    # æå– JSON æ•°æ®
    result['json_data'] = extract_json_from_response(response_text)
    
    # æå–åˆ†æ•°ï¼ˆå¢å¼ºç‰ˆï¼‰
    result['score'] = extract_score_enhanced(response_text, result['json_data'])
    
    # æå–è¿›åŒ–å»ºè®®
    result['evolution_suggestion'] = extract_evolution_suggestion(response_text)
    
    # æå–æ­£æ–‡å†…å®¹ï¼ˆç§»é™¤ Tags å¤´éƒ¨ã€è¿›åŒ–å»ºè®®å’Œ JSONï¼‰
    body = response_text
    # ç§»é™¤ Tags å¤´éƒ¨
    body = re.sub(r'---TAGS:\s*\[[^\]]+\]---\s*\n?', '', body)
    # ç§»é™¤è¿›åŒ–å»ºè®®ç« èŠ‚
    body = re.sub(r'##\s*ğŸ§¬\s*è¿›åŒ–å»ºè®®\s*\n.*?(?=\n```json|\n```|$)', '', body, flags=re.DOTALL)
    # ç§»é™¤ JSON ä»£ç å—
    body = re.sub(r'```json\s*[\s\S]*?```', '', body)
    result['body_content'] = body.strip()
    
    return result


def save_evolution_suggestion(project_name: str, suggestion: str):
    """ä¿å­˜è¿›åŒ–å»ºè®®åˆ°æ—¥å¿—æ–‡ä»¶"""
    try:
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"\n\n---\n## {timestamp} - {project_name}\n\n{suggestion}\n"
        
        with open(EVOLUTION_LOG_FILE, 'a', encoding='utf-8') as f:
            f.write(log_entry)
        return True
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜è¿›åŒ–å»ºè®®å¤±è´¥: {e}")
        return False


def extract_json_from_response(response_text: str) -> Optional[Dict]:
    """ä» AI å“åº”ä¸­æå– JSON æ•°æ®å—"""
    try:
        # é¦–å…ˆå°è¯•åŒ¹é… ```json ... ``` ä»£ç å—ï¼ˆæ”¯æŒå¤šè¡Œï¼‰
        pattern = r'```json\s*([\s\S]*?)\s*```'
        matches = re.findall(pattern, response_text)
        
        if matches:
            # å–æœ€åä¸€ä¸ªåŒ¹é…ï¼ˆé€šå¸¸ JSON åœ¨æœ«å°¾ï¼‰
            json_str = matches[-1].strip()
            try:
                # è§£æ JSON
                json_data = json.loads(json_str)
                # éªŒè¯æ˜¯å¦åŒ…å«å¿…éœ€å­—æ®µ
                required_fields = ['project_name', 'industry', 'tags', 'stage', 'score', 'summary', 'risk_level']
                if all(field in json_data for field in required_fields):
                    return json_data
            except json.JSONDecodeError:
                # JSON è§£æå¤±è´¥ï¼Œå°è¯•æ¸…ç†åå†è§£æ
                # ç§»é™¤å¯èƒ½çš„æ³¨é‡Šæˆ–é¢å¤–å­—ç¬¦
                json_str = re.sub(r'//.*', '', json_str)  # ç§»é™¤å•è¡Œæ³¨é‡Š
                json_str = re.sub(r'/\*.*?\*/', '', json_str, flags=re.DOTALL)  # ç§»é™¤å¤šè¡Œæ³¨é‡Š
                try:
                    json_data = json.loads(json_str)
                    required_fields = ['project_name', 'industry', 'tags', 'stage', 'score', 'summary', 'risk_level']
                    if all(field in json_data for field in required_fields):
                        return json_data
                except:
                    pass
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»£ç å—ï¼Œå°è¯•ç›´æ¥æŸ¥æ‰¾ JSON å¯¹è±¡ï¼ˆå…œåº•æ–¹æ¡ˆï¼‰
        # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å®Œæ•´çš„ JSON å¯¹è±¡
        json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
        json_matches = re.findall(json_pattern, response_text, re.DOTALL)
        
        if json_matches:
            # ä»åå¾€å‰å°è¯•è§£æï¼ˆé€šå¸¸ JSON åœ¨æœ«å°¾ï¼‰
            for json_str in reversed(json_matches):
                try:
                    json_data = json.loads(json_str)
                    # éªŒè¯æ˜¯å¦åŒ…å«å¿…éœ€å­—æ®µ
                    required_fields = ['project_name', 'industry', 'tags', 'stage', 'score', 'summary', 'risk_level']
                    if all(field in json_data for field in required_fields):
                        return json_data
                except:
                    continue
        
        return None
    except Exception as e:
        # é™é»˜å¤±è´¥ï¼Œä¸ä¸­æ–­ä¸»æµç¨‹
        print(f"âš ï¸ æå– JSON å¤±è´¥: {e}")
        return None


def save_to_knowledge_base(json_data: Dict):
    """å°†æå–çš„ JSON æ•°æ®ä¿å­˜åˆ°é¡¹ç›®çŸ¥è¯†åº“ CSV æ–‡ä»¶"""
    try:
        # æ·»åŠ æ—¶é—´æˆ³
        json_data['timestamp'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # å°† tags åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼ˆCSV æ ¼å¼ï¼Œç”¨åˆ†å·åˆ†éš”ï¼‰
        if isinstance(json_data.get('tags'), list):
            json_data['tags'] = ';'.join(json_data['tags'])
        
        # å®šä¹‰ CSV åˆ—é¡ºåº
        columns = ['timestamp', 'project_name', 'industry', 'tags', 'stage', 'score', 'summary', 'risk_level']
        
        # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨
        for col in columns:
            if col not in json_data:
                json_data[col] = ''
        
        # åˆ›å»º DataFrame
        df_new = pd.DataFrame([json_data])
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if KNOWLEDGE_BASE_FILE.exists():
            # è¯»å–ç°æœ‰æ•°æ®
            df_existing = pd.read_csv(KNOWLEDGE_BASE_FILE, encoding='utf-8-sig')
            # åˆå¹¶æ•°æ®
            df_combined = pd.concat([df_existing, df_new], ignore_index=True)
        else:
            # åˆ›å»ºæ–°æ–‡ä»¶
            df_combined = df_new
        
        # æŒ‰ç…§åˆ—é¡ºåºé‡æ–°æ’åˆ—
        df_combined = df_combined[columns]
        
        # ä¿å­˜åˆ° CSVï¼ˆä½¿ç”¨ utf-8-sig ç¼–ç ä»¥æ”¯æŒä¸­æ–‡ï¼Œé¿å… Windows Excel ä¹±ç ï¼‰
        df_combined.to_csv(KNOWLEDGE_BASE_FILE, index=False, encoding='utf-8-sig')
        return True
    except Exception as e:
        # é™é»˜å¤±è´¥ï¼Œä¸ä¸­æ–­ä¸»æµç¨‹
        print(f"âš ï¸ ä¿å­˜çŸ¥è¯†åº“å¤±è´¥: {e}")
        return False


def load_knowledge_base() -> pd.DataFrame:
    """åŠ è½½é¡¹ç›®çŸ¥è¯†åº“"""
    try:
        if KNOWLEDGE_BASE_FILE.exists():
            df = pd.read_csv(KNOWLEDGE_BASE_FILE, encoding='utf-8-sig')
            # ç¡®ä¿ timestamp åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹ï¼Œä¾¿äºæ ¼å¼åŒ–
            if 'timestamp' in df.columns:
                df['timestamp'] = df['timestamp'].astype(str)
            return df
        else:
            return pd.DataFrame()
    except Exception as e:
        print(f"âš ï¸ åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {e}")
        return pd.DataFrame()


def format_timestamp_for_display(timestamp_str: str) -> str:
    """å°†æ—¶é—´æˆ³æ ¼å¼åŒ–ä¸º YYYY-MM-DD æ ¼å¼"""
    try:
        # å°è¯•è§£æå¤šç§æ—¶é—´æ ¼å¼
        if isinstance(timestamp_str, str):
            # å¤„ç† "YYYY-MM-DD HH:MM:SS" æ ¼å¼
            if ' ' in timestamp_str:
                date_part = timestamp_str.split(' ')[0]
                return date_part
            # å¤„ç† ISO æ ¼å¼
            elif 'T' in timestamp_str:
                date_part = timestamp_str.split('T')[0]
                return date_part
            # å¦‚æœå·²ç»æ˜¯ YYYY-MM-DD æ ¼å¼
            elif len(timestamp_str) >= 10:
                return timestamp_str[:10]
        return timestamp_str
    except:
        return timestamp_str


def get_recent_projects(limit: int = 5) -> List[Dict]:
    """è·å–æœ€è¿‘çš„é¡¹ç›®è®°å½•"""
    try:
        df = load_knowledge_base()
        if df.empty:
            return []
        
        # æŒ‰æ—¶é—´å€’åºæ’åˆ—ï¼Œå–æœ€æ–° N æ¡
        if 'timestamp' in df.columns and 'project_name' in df.columns:
            df_sorted = df.sort_values('timestamp', ascending=False)
            recent = df_sorted.head(limit)
            
            result = []
            for _, row in recent.iterrows():
                date_str = format_timestamp_for_display(str(row.get('timestamp', '')))
                project_name = str(row.get('project_name', 'æœªçŸ¥é¡¹ç›®'))
                result.append({
                    'date': date_str,
                    'name': project_name
                })
            return result
        return []
    except Exception as e:
        print(f"âš ï¸ è·å–æœ€è¿‘é¡¹ç›®å¤±è´¥: {e}")
        return []


def calculate_kb_statistics(df: pd.DataFrame) -> Dict:
    """è®¡ç®—çŸ¥è¯†åº“ç»Ÿè®¡æŒ‡æ ‡"""
    stats = {
        'total_projects': 0,
        'avg_score': 0.0,
        'top_industry': 'N/A',
        'industry_count': {}
    }
    
    try:
        if df.empty:
            return stats
        
        stats['total_projects'] = len(df)
        
        # è®¡ç®—å¹³å‡è¯„åˆ†
        if 'score' in df.columns:
            try:
                scores = pd.to_numeric(df['score'], errors='coerce').dropna()
                if len(scores) > 0:
                    stats['avg_score'] = round(scores.mean(), 1)
            except:
                pass
        
        # è®¡ç®—æœ€çƒ­èµ›é“
        if 'industry' in df.columns:
            industry_counts = df['industry'].value_counts()
            if len(industry_counts) > 0:
                stats['top_industry'] = industry_counts.index[0]
                stats['industry_count'] = industry_counts.to_dict()
        
        return stats
    except Exception as e:
        print(f"âš ï¸ è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡å¤±è´¥: {e}")
        return stats


def generate_file_id(file_name: str) -> str:
    """ä¸ºæ–‡ä»¶ç”Ÿæˆå”¯ä¸€ ID"""
    # ä½¿ç”¨æ–‡ä»¶åå’Œå½“å‰æ—¶é—´æˆ³ç”Ÿæˆå”¯ä¸€ ID
    content = f"{file_name}_{time.time()}"
    return hashlib.md5(content.encode()).hexdigest()[:12]


def process_single_file(
    uploaded_file,
    file_id: str,
    system_prompt: str,
    api_key: str,
    model: str,
    selected_mode: str,
    row_container,
    similar_projects: List[Dict] = None
) -> Optional[Dict]:
    """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼Œè¿”å›ç»“æœå­—å…¸"""
    try:
        # æå–æ–‡ä»¶æ–‡æœ¬
        file_text = extract_text_from_uploaded_file(uploaded_file)
        if not file_text:
            with row_container.container():
                st.error(f"âŒ æ— æ³•ä»æ–‡ä»¶ {uploaded_file.name} ä¸­æå–æ–‡æœ¬")
            return None
        
        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨ä¾§è¾¹æ é…ç½®çš„å€¼ï¼‰
        # æ³¨æ„ï¼šbase_url å’Œ api_key åº”è¯¥ä»ä¾§è¾¹æ çš„è¾“å…¥æ¡†è·å–
        # è¿™é‡Œéœ€è¦ä»è°ƒç”¨å‡½æ•°çš„å‚æ•°ä¸­è·å–ï¼Œæˆ–ä» session_state è¯»å–
        client = OpenAI(
            base_url=BASE_URL,  # Base URL æ˜¯å›ºå®šçš„ï¼Œä½¿ç”¨å¸¸é‡
            api_key=api_key
        )
        
        # system_prompt å·²ç»åœ¨å¤–éƒ¨å¢å¼ºï¼ˆåŒ…å« RAG ä¿¡æ¯ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
        enhanced_prompt = system_prompt
        
        # è°ƒç”¨ APIï¼ˆæµå¼ï¼‰
        stream = call_openrouter_api(client, enhanced_prompt, file_text, model)
        if not stream:
            with row_container.container():
                st.error(f"âŒ API è°ƒç”¨å¤±è´¥: {uploaded_file.name}")
            return None
        
        # æ”¶é›†å®Œæ•´å“åº”
        full_response = ""
        
        for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                full_response += chunk.choices[0].delta.content
        
        # è§£æ LLM å“åº”ï¼ˆæå– Tagsã€JSONã€åˆ†æ•°ã€è¿›åŒ–å»ºè®®ç­‰ï¼‰
        parsed = parse_llm_response(full_response)
        json_data = parsed.get('json_data')
        extracted_tags = parsed.get('tags', [])
        extracted_score = parsed.get('score')
        evolution_suggestion = parsed.get('evolution_suggestion')
        body_content = parsed.get('body_content', full_response)
        
        # å¦‚æœ JSON ä¸­æœ‰ tagsï¼Œä¼˜å…ˆä½¿ç”¨ JSON ä¸­çš„ï¼ˆæ›´å‡†ç¡®ï¼‰
        if json_data and json_data.get('tags'):
            extracted_tags = json_data.get('tags', [])
        
        # å¦‚æœ JSON ä¸­æœ‰ scoreï¼Œä¼˜å…ˆä½¿ç”¨ JSON ä¸­çš„
        if json_data and json_data.get('score') and json_data.get('score') != 'N/A':
            extracted_score = json_data.get('score')
        
        # ä¿å­˜åˆ° CSV çŸ¥è¯†åº“
        if json_data:
            save_to_knowledge_base(json_data)
        
        # ä¿å­˜å†å²è®°å½•ï¼ˆä¿å­˜å®Œæ•´å†…å®¹ï¼‰
        save_history_entry(selected_mode, uploaded_file.name, full_response)
        
        # ä¿å­˜è¿›åŒ–å»ºè®®
        if evolution_suggestion:
            project_name = json_data.get('project_name', uploaded_file.name) if json_data else uploaded_file.name
            save_evolution_suggestion(project_name, evolution_suggestion)
        
        # ç”Ÿæˆ Word æ–‡æ¡£ï¼ˆå†…å­˜ä¸­ï¼Œä½¿ç”¨æ¸…ç†åçš„æ­£æ–‡å†…å®¹ï¼‰
        word_buffer = io.BytesIO()
        temp_path = HISTORY_DIR / f"temp_{file_id}.docx"
        # Word å¯¼å‡ºæ—¶ä½¿ç”¨æ¸…ç†åçš„æ­£æ–‡ï¼ˆä¸åŒ…å« Tags å¤´éƒ¨å’Œè¿›åŒ–å»ºè®®ï¼‰
        markdown_to_docx(body_content, str(temp_path))
        
        with open(temp_path, "rb") as f:
            word_buffer.write(f.read())
        word_buffer.seek(0)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            temp_path.unlink()
        except:
            pass
        
        # è¿”å›ç»“æœå­—å…¸
        project_name = json_data.get('project_name', uploaded_file.name) if json_data else uploaded_file.name
        final_score = extracted_score if extracted_score is not None else (json_data.get('score', 'N/A') if json_data else 'N/A')
        
        return {
            'file_id': file_id,
            'file_name': uploaded_file.name,
            'markdown_content': body_content,  # ä½¿ç”¨æ¸…ç†åçš„æ­£æ–‡
            'full_response': full_response,  # ä¿ç•™å®Œæ•´å“åº”ç”¨äºå±•ç¤º
            'word_buffer': word_buffer,
            'json_data': json_data,
            'project_name': project_name,
            'score': final_score,
            'tags': extracted_tags,
            'evolution_suggestion': evolution_suggestion,
            'similar_projects': similar_projects if similar_projects else [],
            'status': 'completed',
            'timestamp': datetime.now().isoformat()
        }
    
    except Exception as e:
        row_container.error(f"âŒ å¤„ç†æ–‡ä»¶ {uploaded_file.name} æ—¶å‡ºé”™: {str(e)}")
        return None


def render_result_row(result: Dict, index: int) -> bool:
    """æ¸²æŸ“å•ä¸ªç»“æœè¡Œï¼Œè¿”å›æ˜¯å¦è¢«é€‰ä¸­"""
    col1, col2 = st.columns([0.05, 0.95])
    
    with col1:
        checkbox_key = f"select_{result['file_id']}"
        is_selected = st.checkbox(
            "",
            value=True,
            key=checkbox_key,
            label_visibility="collapsed"
        )
    
    with col2:
        # æ„å»º expander æ ‡é¢˜
        score_display = f"{result['score']}åˆ†" if isinstance(result['score'], (int, float)) else str(result['score'])
        expander_title = f"âœ… [{score_display}] {result['project_name']} (ç‚¹å‡»å±•å¼€è¯¦æƒ…)"
        
        with st.expander(expander_title, expanded=False):
            # ========== å¤§è„‘æ€è€ƒè·¯å¾„å±•ç¤º ==========
            st.info("ğŸ§  **å¤§è„‘æ€è€ƒè·¯å¾„**")
            
            # æ˜¾ç¤ºæå–åˆ°çš„æ ‡ç­¾
            tags = result.get('tags', [])
            if tags:
                tags_str = ', '.join([f"`{tag}`" for tag in tags])
                st.markdown(f"**æ ¸å¿ƒå…³é”®è¯**: {tags_str}")
            else:
                st.markdown("**æ ¸å¿ƒå…³é”®è¯**: æœªæå–åˆ°æ ‡ç­¾")
            
            # æ˜¾ç¤ºè®°å¿†æ¿€æ´»çŠ¶æ€
            similar_projects = result.get('similar_projects', [])
            if similar_projects and len(similar_projects) > 0:
                project_names = [proj.get('name', 'æœªçŸ¥é¡¹ç›®') for proj in similar_projects]
                common_tags_list = []
                for proj in similar_projects:
                    common_tags = proj.get('common_tags', [])
                    if common_tags:
                        common_tags_list.extend(common_tags)
                
                common_tags_str = ', '.join(set(common_tags_list)) if common_tags_list else 'æ— '
                st.success(f"ğŸ¯ **è®°å¿†æ¿€æ´»**: å‘ç°ä¸å†å²é¡¹ç›® `{', '.join(project_names)}` å­˜åœ¨å…³è”ï¼ˆå…±åŒæ ‡ç­¾: {common_tags_str}ï¼‰ï¼Œå·²è¿›è¡Œæ¨ªå‘å¯¹æ¯”ã€‚")
            else:
                st.info("ğŸŒ± **æ–°ç‰©ç§æ”¶å½•**: çŸ¥è¯†åº“ä¸­æš‚æ— åŒç±»ï¼Œå·²ä½œä¸ºç§å­å­˜å…¥å¤§è„‘ã€‚")
            
            st.divider()
            
            # æ˜¾ç¤ºæ¸…ç†åçš„ Markdown å†…å®¹ï¼ˆæ­£æ–‡ï¼‰
            cleaned_content = clean_markdown_for_display(result['markdown_content'])
            st.markdown(cleaned_content)
            
            # å•ç‹¬ä¸‹è½½æŒ‰é’®
            word_buffer = result.get('word_buffer')
            if word_buffer:
                word_buffer.seek(0)
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½æ­¤æŠ¥å‘Š",
                    data=word_buffer.read(),
                    file_name=f"{result['project_name']}_{datetime.now().strftime('%Y%m%d')}.docx",
                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                    key=f"download_row_{result['file_id']}"
                )
    
    return is_selected


def call_openrouter_api(client: OpenAI, system_prompt: str, user_content: str, model: str):
    """è°ƒç”¨ OpenRouter API è¿›è¡Œæµå¼å“åº”"""
    try:
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_content}
        ]
        
        stream = client.chat.completions.create(
            model=model,
            messages=messages,
            stream=True
        )
        
        return stream
    except Exception as e:
        st.error(f"âŒ API è°ƒç”¨é”™è¯¯: {e}")
        return None


# ==================== ä¾§è¾¹æ  ====================
with st.sidebar:
    st.header("âš™ï¸ æ§åˆ¶å°")
    
    # åˆå§‹åŒ– is_analyzing çŠ¶æ€
    if "is_analyzing" not in st.session_state:
        st.session_state.is_analyzing = False
    
    # é¡µé¢å¯¼èˆªï¼ˆå¦‚æœæ­£åœ¨åˆ†æåˆ™ç¦ç”¨ï¼‰
    page = st.sidebar.radio(
        "é¡µé¢å¯¼èˆª",
        options=["ğŸš€ AI åˆ†æå·¥ä½œå°", "ğŸ“‚ å…¨é‡çŸ¥è¯†åº“", "ğŸ“œ å†å²è®°å½•"],
        index=0,
        label_visibility="visible",
        disabled=st.session_state.is_analyzing
    )
    
    # å¦‚æœæ­£åœ¨åˆ†æï¼Œæ˜¾ç¤ºè­¦å‘Š
    if st.session_state.is_analyzing:
        st.warning("âš ï¸ åˆ†æä»»åŠ¡è¿è¡Œä¸­ï¼Œå¯¼èˆªå·²é”å®š...")
    
    st.divider()
    
    # API é…ç½®ï¼ˆæŒä¹…åŒ–ï¼‰
    st.subheader("API é…ç½®")
    
    # åŠ è½½é…ç½®ï¼ˆä»…åœ¨ç¬¬ä¸€æ¬¡æ—¶ï¼‰
    if "config_loaded" not in st.session_state:
        config = load_config()
        st.session_state["api_key_input"] = config.get("api_key", os.getenv("OPENROUTER_API_KEY", ""))
        st.session_state["base_url_input"] = config.get("base_url", BASE_URL)
        st.session_state["model_input"] = config.get("model", DEFAULT_MODEL)
        st.session_state["config_loaded"] = True
        
        # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œé¦–æ¬¡åŠ è½½æ—¶ä¿å­˜é»˜è®¤é…ç½®
        if not CONFIG_FILE.exists():
            save_config()
    
    # Base URLï¼ˆç¦ç”¨ï¼Œå›ºå®šå€¼ï¼Œä½†ä¿å­˜åœ¨é…ç½®ä¸­ï¼‰
    base_url = st.text_input(
        "Base URL",
        value=st.session_state.get("base_url_input", BASE_URL),
        disabled=True,
        key="base_url_input",
        help="OpenRouter API åŸºç¡€ URLï¼ˆå›ºå®šï¼‰"
    )
    
    # API Keyï¼ˆæ”¯æŒè‡ªåŠ¨ä¿å­˜ï¼‰
    api_key = st.text_input(
        "API Key",
        value=st.session_state.get("api_key_input", ""),
        type="password",
        key="api_key_input",
        on_change=save_config,
        help="ä»é…ç½®æ–‡ä»¶åŠ è½½æˆ–æ‰‹åŠ¨è¾“å…¥ï¼Œä¿®æ”¹åè‡ªåŠ¨ä¿å­˜"
    )
    
    # Modelï¼ˆæ”¯æŒè‡ªåŠ¨ä¿å­˜ï¼‰
    model = st.text_input(
        "Model",
        value=st.session_state.get("model_input", DEFAULT_MODEL),
        key="model_input",
        on_change=save_config,
        help="OpenRouter æ¨¡å‹åç§°ï¼Œä¿®æ”¹åè‡ªåŠ¨ä¿å­˜"
    )
    
    # æ˜¾ç¤ºé…ç½®çŠ¶æ€æç¤º
    if CONFIG_FILE.exists():
        st.caption("ğŸ’¾ é…ç½®å·²è‡ªåŠ¨ä¿å­˜")
    
    st.divider()
    
    # åˆ†ææ¨¡å¼é€‰æ‹©ï¼ˆä»…åœ¨å·¥ä½œå°é¡µé¢éœ€è¦ï¼Œä¸”åœ¨æœªåˆ†ææ—¶æ˜¾ç¤ºï¼‰
    if page == "ğŸš€ AI åˆ†æå·¥ä½œå°" and not st.session_state.is_analyzing:
        st.subheader("ğŸ“‹ åˆ†ææ¨¡å¼")
        prompt_files = load_prompt_files()
        
        if not prompt_files:
            st.warning("âš ï¸ è¯·åœ¨ prompts æ–‡ä»¶å¤¹æ”¾å…¥ .txt æç¤ºè¯æ–‡ä»¶")
            selected_mode = None
            system_prompt = None
        else:
            mode_names = list(prompt_files.keys())
            selected_mode = st.selectbox(
                "é€‰æ‹©åˆ†ææ¨¡å¼",
                options=mode_names,
                index=0
            )
            base_prompt = prompt_files.get(selected_mode, "")
            # å­˜å‚¨åŸºç¡€æç¤ºè¯ï¼Œç¨ååœ¨è°ƒç”¨ API æ—¶å¢å¼º
            system_prompt = base_prompt
    else:
        # çŸ¥è¯†åº“é¡µé¢ä¸éœ€è¦è¿™äº›å˜é‡ï¼Œä½†éœ€è¦åˆå§‹åŒ–ä»¥é¿å…é”™è¯¯
        # å¦‚æœæ­£åœ¨åˆ†æï¼Œä» session_state æ¢å¤
        if st.session_state.is_analyzing and "selected_mode" in st.session_state:
            selected_mode = st.session_state.selected_mode
            system_prompt = st.session_state.get("system_prompt", "")
        else:
            selected_mode = None
            system_prompt = None
    
    st.divider()
    
    # AI è¿›åŒ–æ—¥å¿—
    with st.expander("ğŸ§¬ æŸ¥çœ‹ AI è¿›åŒ–æ—¥å¿—", expanded=False):
        try:
            if EVOLUTION_LOG_FILE.exists():
                with open(EVOLUTION_LOG_FILE, 'r', encoding='utf-8') as f:
                    log_content = f.read()
                if log_content.strip():
                    st.markdown(log_content)
                else:
                    st.info("æš‚æ— è¿›åŒ–å»ºè®®ï¼Œåˆ†æé¡¹ç›®åä¼šè‡ªåŠ¨ç”Ÿæˆã€‚")
            else:
                st.info("æš‚æ— è¿›åŒ–å»ºè®®ï¼Œåˆ†æé¡¹ç›®åä¼šè‡ªåŠ¨ç”Ÿæˆã€‚")
        except Exception as e:
            st.warning(f"âš ï¸ è¯»å–è¿›åŒ–æ—¥å¿—å¤±è´¥: {e}")


# ==================== ä¸»ç•Œé¢ ====================
# æ ¹æ®é¡µé¢å¯¼èˆªæ˜¾ç¤ºä¸åŒå†…å®¹
if page == "ğŸš€ AI åˆ†æå·¥ä½œå°":
    # ========== AI åˆ†æå·¥ä½œå°é¡µé¢ ==========
    st.title("ğŸš€ AI åˆ†æå·¥ä½œå°")
    st.caption("é«˜çº§æŠ•èµ„è¾…åŠ©ç³»ç»Ÿ - æ”¯æŒæ‰¹é‡åˆ†æä¸æŠ¥å‘Šå¯¼å‡º")

    # æ˜¾ç¤ºå½“å‰æ¨¡å¼
    if selected_mode:
        st.info(f"ğŸ“‹ **å½“å‰åˆ†ææ¨¡å¼**: {selected_mode}")
    else:
        st.warning("âš ï¸ è¯·å…ˆåœ¨ä¾§è¾¹æ é€‰æ‹©åˆ†ææ¨¡å¼")

    st.divider()

    # åˆå§‹åŒ– session state
    if "processed_results" not in st.session_state:
        st.session_state["processed_results"] = []
    if "task_queue" not in st.session_state:
        st.session_state["task_queue"] = []
    if "processing_status" not in st.session_state:
        st.session_state["processing_status"] = {}

    # åˆå§‹åŒ– MemoryManagerï¼ˆè½»é‡çº§ï¼Œæ— éœ€é¢å¤–ä¾èµ–ï¼‰
    # memory_manager å·²åœ¨æ¨¡å—åŠ è½½æ—¶è‡ªåŠ¨åˆå§‹åŒ–

    # æ‰¹é‡æ–‡ä»¶ä¸Šä¼ ï¼ˆåˆ†æä¸­æ—¶ç¦ç”¨ï¼‰
    uploaded_files = st.file_uploader(
        "ğŸ“„ ä¸Šä¼ åˆ†ææ–‡ä»¶ï¼ˆæ”¯æŒæ‰¹é‡ï¼‰",
        type=["pdf", "docx", "doc"],
        help="æ”¯æŒ PDF å’Œ DOCX æ ¼å¼ï¼Œå¯åŒæ—¶ä¸Šä¼ å¤šä¸ªæ–‡ä»¶è¿›è¡Œæ‰¹é‡åˆ†æ",
        accept_multiple_files=True,
        disabled=st.session_state.is_analyzing
    )

    # å¼€å§‹æ‰¹é‡åˆ†ææŒ‰é’®ï¼ˆä»…åœ¨æœªåˆ†ææ—¶æ˜¾ç¤ºï¼‰
    if uploaded_files and len(uploaded_files) > 0 and not st.session_state.is_analyzing:
        if st.button("ğŸš€ å¼€å§‹æ‰¹é‡åˆ†æ", type="primary", disabled=not (selected_mode and api_key)):
            if not api_key:
                st.error("âŒ è¯·è¾“å…¥ API Key")
                st.stop()
            
            if not selected_mode:
                st.error("âŒ è¯·é€‰æ‹©åˆ†ææ¨¡å¼")
                st.stop()
            
            # è®¾ç½®åˆ†æçŠ¶æ€å¹¶ä¿å­˜é…ç½®åˆ° session_state
            st.session_state.is_analyzing = True
            st.session_state.selected_mode = selected_mode
            st.session_state.system_prompt = system_prompt
            
            # åˆå§‹åŒ–ä»»åŠ¡é˜Ÿåˆ—
            task_queue = []
            for i, uploaded_file in enumerate(uploaded_files):
                file_id = generate_file_id(f"{uploaded_file.name}_{i}_{time.time()}")
                task_queue.append({
                    'file_id': file_id,
                    'file': uploaded_file,
                    'index': i
                })
            
            st.session_state.task_queue = task_queue
            st.rerun()
    
    # å¦‚æœæ­£åœ¨åˆ†æï¼Œæ‰§è¡Œæ‰¹é‡å¤„ç†å¾ªç¯
    if st.session_state.is_analyzing and "task_queue" in st.session_state:
        task_queue = st.session_state.task_queue
        if len(task_queue) > 0:
            st.subheader("ğŸ“‹ ä»»åŠ¡é˜Ÿåˆ—")
            
            # åˆå§‹åŒ–æ‰€æœ‰è¡Œçš„å ä½ç¬¦ï¼ˆä»…åœ¨ç¬¬ä¸€æ¬¡æ¸²æŸ“æ—¶ï¼‰
            if "row_containers" not in st.session_state:
                st.session_state.row_containers = []
                for task in task_queue:
                    row_container = st.empty()
                    with row_container.container():
                        st.markdown(f"â³ **ç­‰å¾…å¤„ç†**: {task['file'].name}")
                    st.session_state.row_containers.append({
                        'file_id': task['file_id'],
                        'container': row_container,
                        'status': 'waiting'
                    })
            
            # è·å–å½“å‰å¤„ç†ç´¢å¼•
            if "current_task_index" not in st.session_state:
                st.session_state.current_task_index = 0
            
            current_idx = st.session_state.current_task_index
            
            if current_idx < len(task_queue):
                task = task_queue[current_idx]
                row_info = st.session_state.row_containers[current_idx]
                
                # æ›´æ–°å½“å‰è¡Œçš„çŠ¶æ€
                with row_info['container'].container():
                    st.markdown(f"ğŸ”„ **æ­£åœ¨æ·±å…¥åˆ†æ (å…³è”çŸ¥è¯†åº“ä¸­...)**: {task['file'].name}...")
                
                # æå–æ–‡ä»¶æ–‡æœ¬
                file_text = extract_text_from_uploaded_file(task['file'])
                
                if file_text:
                    # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼ˆç”¨äºæ ‡ç­¾æå–å’Œç›¸ä¼¼é¡¹ç›®æŸ¥è¯¢ï¼‰
                    client = OpenAI(
                        base_url=base_url,  # ä½¿ç”¨ä¾§è¾¹æ é…ç½®çš„å€¼
                        api_key=api_key
                    )
                    
                    # åŸºäºæ ‡ç­¾åŒ¹é…æ£€ç´¢ç›¸ä¼¼é¡¹ç›®
                    similar_projects = []
                    if memory_manager.enabled and memory_manager.get_count() > 0:
                        similar_projects = memory_manager.query_similar(file_text, client, model, top_k=3)
                    
                    # å¢å¼º Promptï¼ˆåŒ…å«ç›¸ä¼¼é¡¹ç›®ä¿¡æ¯ï¼‰
                    enhanced_prompt = enhance_system_prompt(
                        st.session_state.system_prompt,
                        similar_projects if similar_projects else None
                    )
                    
                    # å¤„ç†æ–‡ä»¶
                    result = process_single_file(
                        uploaded_file=task['file'],
                        file_id=task['file_id'],
                        system_prompt=enhanced_prompt,
                        api_key=api_key,
                        model=model,
                        selected_mode=st.session_state.selected_mode,
                        row_container=row_info['container'],
                        similar_projects=similar_projects
                    )
                    
                    if result:
                        # æå– JSON æ•°æ®
                        json_data = result.get('json_data')
                        
                        # å­˜å‚¨åˆ°è½»é‡çº§è®°å¿†åº“
                        if json_data and memory_manager.enabled:
                            project_name = json_data.get('project_name', task['file'].name)
                            summary = json_data.get('summary', '')
                            score = result.get('score', json_data.get('score', 'N/A'))
                            # ä½¿ç”¨ä»å“åº”ä¸­æå–çš„ tagsï¼ˆå·²é€šè¿‡ parse_llm_response æå–ï¼‰
                            tags = result.get('tags', [])
                            
                            # å¦‚æœä»ç„¶æ²¡æœ‰æ ‡ç­¾ï¼Œå°è¯•ä» JSON ä¸­è·å–
                            if not tags and json_data.get('tags'):
                                tags = json_data.get('tags', [])
                            
                            memory_manager.add_memory(
                                name=project_name,
                                summary=summary,
                                full_text=result.get('full_response', result['markdown_content']),  # ä½¿ç”¨å®Œæ•´å“åº”
                                score=score,
                                tags=tags,
                                meta={
                                    'industry': json_data.get('industry', ''),
                                    'stage': json_data.get('stage', ''),
                                    'risk_level': json_data.get('risk_level', '')
                                }
                            )
                        
                        # æ›´æ–°è¡Œå®¹å™¨ä¸ºæœ€ç»ˆç»“æœ
                        row_info['container'].empty()
                        with row_info['container'].container():
                            col1, col2 = st.columns([0.05, 0.95])
                            
                            with col1:
                                checkbox_key = f"select_{result['file_id']}"
                                st.checkbox(
                                    "",
                                    value=True,
                                    key=checkbox_key,
                                    label_visibility="collapsed"
                                )
                            
                            with col2:
                                score_display = f"{result['score']}åˆ†" if isinstance(result['score'], (int, float)) else str(result['score'])
                                expander_title = f"âœ… [{score_display}] {result['project_name']} (ç‚¹å‡»å±•å¼€è¯¦æƒ…)"
                                
                                with st.expander(expander_title, expanded=False):
                                    # ========== å¤§è„‘æ€è€ƒè·¯å¾„å±•ç¤º ==========
                                    st.info("ğŸ§  **å¤§è„‘æ€è€ƒè·¯å¾„**")
                                    
                                    # æ˜¾ç¤ºæå–åˆ°çš„æ ‡ç­¾
                                    tags = result.get('tags', [])
                                    if tags:
                                        tags_str = ', '.join([f"`{tag}`" for tag in tags])
                                        st.markdown(f"**æ ¸å¿ƒå…³é”®è¯**: {tags_str}")
                                    else:
                                        st.markdown("**æ ¸å¿ƒå…³é”®è¯**: æœªæå–åˆ°æ ‡ç­¾")
                                    
                                    # æ˜¾ç¤ºè®°å¿†æ¿€æ´»çŠ¶æ€
                                    similar_projects = result.get('similar_projects', [])
                                    if similar_projects and len(similar_projects) > 0:
                                        project_names = [proj.get('name', 'æœªçŸ¥é¡¹ç›®') for proj in similar_projects]
                                        common_tags_list = []
                                        for proj in similar_projects:
                                            common_tags = proj.get('common_tags', [])
                                            if common_tags:
                                                common_tags_list.extend(common_tags)
                                        
                                        common_tags_str = ', '.join(set(common_tags_list)) if common_tags_list else 'æ— '
                                        st.success(f"ğŸ¯ **è®°å¿†æ¿€æ´»**: å‘ç°ä¸å†å²é¡¹ç›® `{', '.join(project_names)}` å­˜åœ¨å…³è”ï¼ˆå…±åŒæ ‡ç­¾: {common_tags_str}ï¼‰ï¼Œå·²è¿›è¡Œæ¨ªå‘å¯¹æ¯”ã€‚")
                                    else:
                                        st.info("ğŸŒ± **æ–°ç‰©ç§æ”¶å½•**: çŸ¥è¯†åº“ä¸­æš‚æ— åŒç±»ï¼Œå·²ä½œä¸ºç§å­å­˜å…¥å¤§è„‘ã€‚")
                                    
                                    st.divider()
                                    
                                    # æ˜¾ç¤ºæ¸…ç†åçš„ Markdown å†…å®¹ï¼ˆæ­£æ–‡ï¼‰
                                    cleaned_content = clean_markdown_for_display(result['markdown_content'])
                                    st.markdown(cleaned_content)
                                    
                                    word_buffer = result.get('word_buffer')
                                    if word_buffer:
                                        word_buffer.seek(0)
                                        st.download_button(
                                            label="ğŸ“¥ ä¸‹è½½æ­¤æŠ¥å‘Š",
                                            data=word_buffer.read(),
                                            file_name=f"{result['project_name']}_{datetime.now().strftime('%Y%m%d')}.docx",
                                            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                                            key=f"download_{result['file_id']}"
                                        )
                        
                        # ä¿å­˜ç»“æœåˆ° session state
                        if "processed_results" not in st.session_state:
                            st.session_state.processed_results = []
                        st.session_state.processed_results.append(result)
                
                # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªä»»åŠ¡
                st.session_state.current_task_index += 1
                st.rerun()
            else:
                # æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                st.success(f"âœ… æˆåŠŸåˆ†æ {len(st.session_state.processed_results)}/{len(task_queue)} ä¸ªæ–‡ä»¶ï¼")
                
                # æ¸…ç†çŠ¶æ€
                st.session_state.is_analyzing = False
                if "row_containers" in st.session_state:
                    del st.session_state.row_containers
                if "current_task_index" in st.session_state:
                    del st.session_state.current_task_index
                if "task_queue" in st.session_state:
                    del st.session_state.task_queue
                
                st.rerun()

    # ä» session state æ¸²æŸ“å·²å¤„ç†çš„ç»“æœï¼ˆåˆ·æ–°åä»ç„¶æ˜¾ç¤ºï¼‰
    if st.session_state.get("processed_results"):
        st.divider()
        st.subheader("ğŸ“Š åˆ†æç»“æœåˆ—è¡¨")
        
        selected_results = []
        for result in st.session_state["processed_results"]:
            is_selected = render_result_row(result, len(selected_results))
            if is_selected:
                selected_results.append(result)
        
        # æ‰¹é‡ä¸‹è½½æŒ‰é’®
        st.divider()
        if selected_results:
            if st.button("ğŸ“¥ ä¸‹è½½é€‰ä¸­é¡¹ç›®ï¼ˆZIP å‹ç¼©åŒ…ï¼‰", type="primary"):
                zip_buffer = io.BytesIO()
                
                with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                    for result in selected_results:
                        word_buffer = result.get('word_buffer')
                        if word_buffer:
                            word_buffer.seek(0)
                            file_name = f"{result['project_name']}_{result['file_id']}.docx"
                            zip_file.writestr(file_name, word_buffer.read())
                
                zip_buffer.seek(0)
                
                st.download_button(
                    label=f"â¬‡ï¸ ç‚¹å‡»ä¸‹è½½ ZIP æ–‡ä»¶ï¼ˆ{len(selected_results)} ä¸ªæŠ¥å‘Šï¼‰",
                    data=zip_buffer.read(),
                    file_name=f"batch_reports_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip",
                    mime="application/zip",
                    key="download_zip"
                )
        
        # æ¸…é™¤æ‰€æœ‰ç»“æœæŒ‰é’®
        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰ç»“æœ"):
            st.session_state["processed_results"] = []
            st.session_state["task_queue"] = []
            st.session_state["processing_status"] = {}
            st.rerun()

elif page == "ğŸ“‚ å…¨é‡çŸ¥è¯†åº“":
    # ========== å…¨é‡çŸ¥è¯†åº“é¡µé¢ ==========
    st.title("ğŸ“‚ å…¨é‡é¡¹ç›®çŸ¥è¯†åº“")
    st.caption("æŸ¥çœ‹å’Œç®¡ç†æ‰€æœ‰å·²åˆ†æçš„é¡¹ç›®æ•°æ®")
    
    try:
        # åŠ è½½çŸ¥è¯†åº“æ•°æ®
        df_kb = load_knowledge_base()
        
        if df_kb.empty:
            st.info("ğŸ“­ çŸ¥è¯†åº“ä¸ºç©ºï¼Œæš‚æ— é¡¹ç›®æ•°æ®ã€‚è¯·å…ˆåœ¨å·¥ä½œå°åˆ†æé¡¹ç›®ï¼Œæ•°æ®ä¼šè‡ªåŠ¨ç§¯ç´¯åˆ°è¿™é‡Œã€‚")
        else:
            # æ ¼å¼åŒ–æ—¥æœŸåˆ—
            if 'timestamp' in df_kb.columns:
                df_kb['timestamp'] = df_kb['timestamp'].apply(format_timestamp_for_display)
            
            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            stats = calculate_kb_statistics(df_kb)
            
            # æ˜¾ç¤ºç»Ÿè®¡æŒ‡æ ‡
            st.divider()
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("ğŸ“Š å·²æ”¶å½•é¡¹ç›®", stats['total_projects'])
            
            with col2:
                st.metric("â­ å¹³å‡è¯„åˆ†", f"{stats['avg_score']:.1f}" if stats['avg_score'] > 0 else "N/A")
            
            with col3:
                st.metric("ğŸ”¥ æœ€çƒ­èµ›é“", stats['top_industry'])
            
            with col4:
                if stats['industry_count']:
                    top_count = list(stats['industry_count'].values())[0]
                    st.metric("ğŸ† æœ€é«˜èµ›é“é¡¹ç›®æ•°", top_count)
                else:
                    st.metric("ğŸ† æœ€é«˜èµ›é“é¡¹ç›®æ•°", 0)
            
            st.divider()
            
            # æ˜¾ç¤ºå…¨é‡æ•°æ®è¡¨æ ¼
            st.subheader("ğŸ“‹ é¡¹ç›®æ•°æ®è¡¨")
            st.caption("ğŸ’¡ æç¤º: è¡¨æ ¼æ”¯æŒæ’åºã€æœç´¢å’Œç­›é€‰åŠŸèƒ½")
            
            # é€‰æ‹©è¦æ˜¾ç¤ºçš„åˆ—
            display_columns = ['timestamp', 'project_name', 'industry', 'tags', 'stage', 'score', 'summary', 'risk_level']
            available_columns = [col for col in display_columns if col in df_kb.columns]
            
            if available_columns:
                # ä½¿ç”¨ st.dataframe å±•ç¤ºå…¨é‡æ•°æ®ï¼Œå¼€å¯æ’åºå’Œæœç´¢
                st.dataframe(
                    df_kb[available_columns],
                    use_container_width=True,
                    hide_index=True,
                    height=600  # å¢åŠ è¡¨æ ¼é«˜åº¦ä»¥ä¾¿æŸ¥çœ‹æ›´å¤šæ•°æ®
                )
            else:
                st.dataframe(
                    df_kb,
                    use_container_width=True,
                    hide_index=True,
                    height=600
                )
            
            # å¯¼å‡ºåŠŸèƒ½
            st.divider()
            st.subheader("ğŸ“¥ æ•°æ®å¯¼å‡º")
            col1, col2 = st.columns(2)
            
            with col1:
                # å¯¼å‡º CSV
                csv = df_kb.to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½ CSV æ–‡ä»¶",
                    data=csv,
                    file_name=f"project_database_{datetime.now().strftime('%Y%m%d')}.csv",
                    mime="text/csv",
                    help="ä¸‹è½½å®Œæ•´é¡¹ç›®æ•°æ®åº“ï¼ˆCSV æ ¼å¼ï¼Œæ”¯æŒ Excel æ‰“å¼€ï¼‰"
                )
            
            with col2:
                # å¯¼å‡º JSON
                json_str = df_kb.to_json(orient='records', force_ascii=False, indent=2)
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½ JSON æ–‡ä»¶",
                    data=json_str,
                    file_name=f"project_database_{datetime.now().strftime('%Y%m%d')}.json",
                    mime="application/json",
                    help="ä¸‹è½½å®Œæ•´é¡¹ç›®æ•°æ®åº“ï¼ˆJSON æ ¼å¼ï¼‰"
                )
            
            # èµ›é“åˆ†å¸ƒåˆ†æ
            if stats['industry_count'] and len(stats['industry_count']) > 0:
                st.divider()
                st.subheader("ğŸ“ˆ èµ›é“åˆ†å¸ƒ")
                industry_df = pd.DataFrame([
                    {'èµ›é“': industry, 'é¡¹ç›®æ•°': count}
                    for industry, count in stats['industry_count'].items()
                ]).sort_values('é¡¹ç›®æ•°', ascending=False)
                
                st.dataframe(
                    industry_df,
                    use_container_width=True,
                    hide_index=True
                )
    
    except Exception as e:
        st.error(f"âŒ åŠ è½½çŸ¥è¯†åº“æ•°æ®å¤±è´¥: {str(e)}")
        st.info("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥ project_database.csv æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®")

elif page == "ğŸ“œ å†å²è®°å½•":
    # ========== å†å²è®°å½•é¡µé¢ ==========
    st.title("ğŸ“œ å†å²è®°å½•")
    st.caption("æŸ¥çœ‹æ‰€æœ‰å·²ä¿å­˜çš„åˆ†æå†å²è®°å½•")
    
    try:
        history_entries = load_history_entries()
        
        # æ˜¾ç¤ºçŸ¥è¯†åº“ç»Ÿè®¡
        kb_count = memory_manager.get_count()
        
        if not history_entries:
            st.info("ğŸ“­ æš‚æ— å†å²è®°å½•ã€‚åˆ†æé¡¹ç›®åï¼Œè®°å½•ä¼šè‡ªåŠ¨ä¿å­˜åˆ°è¿™é‡Œã€‚")
        else:
            col1, col2 = st.columns(2)
            with col1:
                st.metric("ğŸ“š æ€»è®°å½•æ•°", len(history_entries))
            with col2:
                st.metric("ğŸ§  çŸ¥è¯†åº“å·²æ”¶å½•é¡¹ç›®", kb_count)
            st.divider()
            
            # æ˜¾ç¤ºå†å²è®°å½•åˆ—è¡¨
            for idx, entry in enumerate(history_entries):
                datetime_str = entry.get("datetime", entry.get("timestamp", ""))
                mode_name = entry.get("mode", "æœªçŸ¥æ¨¡å¼")
                file_name = entry.get("file_name", "æœªçŸ¥æ–‡ä»¶")
                
                # æ ¼å¼åŒ–æ˜¾ç¤ºæ—¶é—´
                try:
                    if "T" in datetime_str:
                        dt = datetime.fromisoformat(datetime_str)
                        display_time = dt.strftime("%Y-%m-%d %H:%M")
                    else:
                        display_time = datetime_str
                except:
                    display_time = datetime_str
                
                with st.expander(f"ğŸ“„ {display_time} | {mode_name} | {file_name}", expanded=False):
                    col1, col2 = st.columns([1, 1])
                    
                    with col1:
                        st.caption(f"ğŸ“‹ **åˆ†ææ¨¡å¼**: {mode_name}")
                        st.caption(f"ğŸ“ **æ–‡ä»¶å**: {file_name}")
                        st.caption(f"ğŸ•’ **åˆ†ææ—¶é—´**: {display_time}")
                    
                    with col2:
                        # ä¸‹è½½æŒ‰é’®
                        content = entry.get("content", "")
                        if content:
                            # ç”Ÿæˆ Word æ–‡æ¡£ï¼ˆä¸´æ—¶ï¼‰
                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            temp_filename = f"{timestamp}_{Path(file_name).stem}.docx"
                            temp_path = HISTORY_DIR / temp_filename
                            
                            if markdown_to_docx(content, str(temp_path)):
                                with open(temp_path, "rb") as f:
                                    st.download_button(
                                        label="ğŸ“¥ ä¸‹è½½ Word æŠ¥å‘Š",
                                        data=f.read(),
                                        file_name=temp_filename,
                                        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                                        key=f"hist_download_{idx}"
                                    )
                                
                                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                                try:
                                    temp_path.unlink()
                                except:
                                    pass
                    
                    st.divider()
                    
                    # æ˜¾ç¤ºåˆ†æå†…å®¹ï¼ˆæ¸…ç† JSON ä»£ç å—ï¼‰
                    if content:
                        cleaned_content = clean_markdown_for_display(content)
                        st.markdown(cleaned_content)
                
                if idx < len(history_entries) - 1:
                    st.divider()
    
    except Exception as e:
        st.error(f"âŒ åŠ è½½å†å²è®°å½•å¤±è´¥: {str(e)}")
        st.info("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥ history_data æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨ä¸”åŒ…å«æœ‰æ•ˆçš„ JSON æ–‡ä»¶")
