"""é›·è¾¾æŠ“å– + å‘¨æŠ¥ç”Ÿæˆä¸»ç¨‹åº"""
# é¦–å…ˆåŠ è½½ç¯å¢ƒå˜é‡ï¼ˆåœ¨ä»»ä½•å…¶ä»–å¯¼å…¥æˆ–è¯»å–ç¯å¢ƒå˜é‡ä¹‹å‰ï¼‰
from pathlib import Path
from dotenv import load_dotenv
# åŠ è½½é¡¹ç›®æ ¹ç›®å½•çš„ .env æ–‡ä»¶ï¼ˆradar/runner.py ä½äºé¡¹ç›®æ ¹ç›®å½•/radar/runner.pyï¼Œæ‰€ä»¥éœ€è¦å‘ä¸Šä¸¤çº§ï¼‰
load_dotenv(dotenv_path=Path(__file__).resolve().parents[1] / ".env", override=False)

import os
import yaml
import hashlib
import re
import requests
from datetime import datetime, timedelta, timezone
from dateutil.relativedelta import relativedelta
from urllib.parse import urlparse, urlunparse, parse_qs, urlencode
import sys

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥ä¾¿å¯¼å…¥
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from db.supabase_db import get_supabase_client


def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = os.path.join(os.path.dirname(__file__), "config.yaml")
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
            print(f"âœ… é…ç½®æ–‡ä»¶å·²åŠ è½½ (ç‰ˆæœ¬: {config['version']})")
            return config
    except FileNotFoundError:
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        raise
    except Exception as e:
        print(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        raise


def get_url_hash(url: str) -> str:
    """ç”Ÿæˆ URL çš„å“ˆå¸Œå€¼ç”¨äºå»é‡"""
    return hashlib.md5(url.encode()).hexdigest()


def search_serper(query: str, gl: str = "us", hl: str = "en", num: int = 10):
    """ä½¿ç”¨ Serper API æœç´¢"""
    api_key = os.getenv("SERPER_API_KEY")
    if not api_key:
        print("âŒ SERPER_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        return []
    
    url = "https://google.serper.dev/search"
    headers = {
        "X-API-KEY": api_key,
        "Content-Type": "application/json"
    }
    payload = {
        "q": query,
        "gl": gl,
        "hl": hl,
        "num": num
    }
    
    try:
        response = requests.post(url, json=payload, headers=headers, timeout=30)
        response.raise_for_status()
        data = response.json()
        organic_results = data.get("organic", [])
        
        # åœ¨è§£æ Serper ç»“æœæ—¶å°±èµ‹å€¼ source å­—æ®µ
        # ä¼˜å…ˆï¼šresult.get("source")ï¼Œå…œåº•ï¼š"serper"
        empty_source_count = 0
        for result in organic_results:
            original_source = result.get("source")
            if not original_source or original_source.strip() == "":
                empty_source_count += 1
            result["source"] = original_source or "serper"
        
        if empty_source_count > 0:
            print(f"    âš ï¸ æœç´¢ç»“æœä¸­æœ‰ {empty_source_count} æ¡è®°å½•çš„ source ä¸ºç©ºï¼ˆå·²è‡ªåŠ¨å¡«å……ä¸º 'serper'ï¼‰")
        
        return organic_results
    except Exception as e:
        print(f"âš ï¸ Serper æœç´¢å¤±è´¥ ({query}): {e}")
        return []


def normalize_url(u: str) -> str:
    """è§„èŒƒåŒ– URLï¼šå»æ‰è¿½è¸ªå‚æ•°ï¼Œç»Ÿä¸€æ ¼å¼
    
    Args:
        u: åŸå§‹ URL å­—ç¬¦ä¸²
        
    Returns:
        è§„èŒƒåŒ–åçš„ URL å­—ç¬¦ä¸²
    """
    if not u:
        return ""
    
    # strip()
    u = u.strip()
    if not u:
        return ""
    
    # æ—  scheme æ—¶è¡¥ https://
    if not u.startswith(('http://', 'https://')):
        u = 'https://' + u
    
    try:
        # urlparse
        parsed = urlparse(u)
        
        # scheme å¼ºåˆ¶ä¸º https
        scheme = 'https'
        
        # netloc å°å†™ï¼Œå»æ‰å‰ç¼€ www.
        netloc = parsed.netloc.lower()
        if netloc.startswith('www.'):
            netloc = netloc[4:]
        
        # path å»æ‰æœ«å°¾ /ï¼ˆæ ¹è·¯å¾„é™¤å¤–ï¼‰
        path = parsed.path
        if path != '/' and path.endswith('/'):
            path = path[:-1]
        
        # query å‚æ•°è¿‡æ»¤
        # åˆ é™¤è¿½è¸ªå‚æ•°ï¼šutm_*, ref, ref_src, fbclid, gclid, igshid, mc_cid, mc_eid, spm, source, mkt_tok
        query_dict = parse_qs(parsed.query, keep_blank_values=False)
        
        # è¿‡æ»¤è¿½è¸ªå‚æ•°
        tracking_params = [
            'ref', 'ref_src', 'fbclid', 'gclid', 'igshid', 
            'mc_cid', 'mc_eid', 'spm', 'source', 'mkt_tok'
        ]
        filtered_query = {}
        for key, values in query_dict.items():
            # è·³è¿‡ utm_* å¼€å¤´çš„å‚æ•°
            if key.startswith('utm_'):
                continue
            # è·³è¿‡å…¶ä»–è¿½è¸ªå‚æ•°
            if key.lower() in [p.lower() for p in tracking_params]:
                continue
            # ä¿ç•™å…¶ä»–å‚æ•°ï¼ˆå–ç¬¬ä¸€ä¸ªå€¼ï¼‰
            filtered_query[key] = values[0] if values else ''
        
        # æŒ‰ key æ’åºåé‡ç»„
        if filtered_query:
            sorted_params = sorted(filtered_query.items())
            query = urlencode(sorted_params)
        else:
            query = ''
        
        # ä¸¢å¼ƒ fragmentï¼ˆ#...ï¼‰
        fragment = ''
        
        # è¿”å›é‡ç»„åçš„ url
        normalized = urlunparse((scheme, netloc, path, parsed.params, query, fragment))
        return normalized
        
    except Exception as e:
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›åŸ URLï¼ˆè‡³å°‘ strip è¿‡ï¼‰
        print(f"âš ï¸ URL è§„èŒƒåŒ–å¤±è´¥ ({u[:50]}...): {e}")
        return u.strip()


def get_hostname(url: str) -> str:
    """ä» URL æå–ä¸»æœºå"""
    try:
        parsed = urlparse(url)
        return parsed.netloc or ""
    except:
        return ""


def clean_canonical_name(title: str) -> str:
    """æ¸…æ´—æ ‡é¢˜ï¼Œæå–è§„èŒƒåç§°ï¼ˆå»æ‰ç½‘ç«™åç¼€/åˆ†éš”ç¬¦ååŠæ®µï¼‰"""
    if not title:
        return ""
    
    # å¸¸è§åˆ†éš”ç¬¦æ¨¡å¼ï¼šå»é™¤ " - Company", " | TechCrunch", " - The Verge" ç­‰
    import re
    # åŒ¹é…åˆ†éš”ç¬¦åŠå…¶åçš„å†…å®¹ï¼ˆå¸¸è§åˆ†éš”ç¬¦ï¼š|ã€-ã€â€“ã€â€”ã€::ï¼‰
    patterns = [
        r'\s*[-â€“â€”]\s*[^|]+$',  # åŒ¹é… " - XXX" åˆ°æœ«å°¾
        r'\s*\|\s*[^|]+$',      # åŒ¹é… " | XXX" åˆ°æœ«å°¾
        r'\s*::\s*[^:]+$',      # åŒ¹é… " :: XXX" åˆ°æœ«å°¾
    ]
    
    cleaned = title
    for pattern in patterns:
        cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)
    
    # å»é™¤é¦–å°¾ç©ºæ ¼
    cleaned = cleaned.strip()
    
    # å¦‚æœæ¸…æ´—åä¸ºç©ºï¼Œè¿”å›åŸå§‹æ ‡é¢˜
    return cleaned if cleaned else title


def generate_canonical_name(item: dict, title: str, hostname: str) -> str:
    """ç”Ÿæˆ canonical_nameï¼ˆä¼˜å…ˆçº§ï¼šå·²æœ‰å­—æ®µ > æ¸…æ´— title > hostnameï¼‰"""
    # ä¼˜å…ˆçº§1: å·²æœ‰ç»“æ„åŒ–å­—æ®µ
    for field in ["company", "product", "name", "canonical_name"]:
        value = item.get(field, "").strip()
        if value:
            return value
    
    # ä¼˜å…ˆçº§2: æ¸…æ´— title
    if title:
        cleaned = clean_canonical_name(title)
        if cleaned:
            return cleaned
    
    # ä¼˜å…ˆçº§3: hostname
    if hostname:
        return hostname
    
    # å…œåº•
    return title or "(untitled)"


def generate_one_liner(item: dict, description: str, title: str = "") -> str:
    """ç”Ÿæˆ one_linerï¼ˆä¼˜å…ˆçº§ï¼šå·²æœ‰æ‘˜è¦ > description æˆªæ–­ > title æˆªæ–­ï¼‰"""
    # ä¼˜å…ˆçº§1: å·²æœ‰æ‘˜è¦å­—æ®µ
    for field in ["one_liner", "summary", "excerpt"]:
        value = item.get(field, "").strip()
        if value:
            # å¦‚æœå·²æœ‰æ‘˜è¦å¤ªé•¿ï¼Œæˆªæ–­åˆ°120å­—
            if len(value) > 120:
                return value[:117] + "..."
            return value
    
    # ä¼˜å…ˆçº§2: ä» description ç”Ÿæˆï¼ˆå»æ¢è¡Œï¼Œæˆªæ–­åˆ°80-120å­—ï¼‰
    if description:
        # å»é™¤æ¢è¡Œå’Œå¤šä½™ç©ºæ ¼
        cleaned = " ".join(description.split())
        
        # æˆªæ–­åˆ°120å­—
        if len(cleaned) > 120:
            # å°½é‡åœ¨æ ‡ç‚¹ç¬¦å·å¤„æˆªæ–­
            truncated = cleaned[:120]
            for punct in ['.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ', ';', 'ï¼›']:
                last_punct = truncated.rfind(punct)
                if last_punct > 80:  # å¦‚æœæ ‡ç‚¹ä½ç½®åœ¨80å­—ä¹‹å
                    return truncated[:last_punct + 1]
            return truncated[:117] + "..."
        elif len(cleaned) >= 80:
            return cleaned
        else:
            # å¦‚æœä¸è¶³80å­—ï¼Œä»ç„¶è¿”å›ï¼ˆè‡³å°‘æ¯”ç©ºå¥½ï¼‰
            return cleaned
    
    # ä¼˜å…ˆçº§3: ä» title ç”Ÿæˆï¼ˆä½œä¸ºæœ€åå…œåº•ï¼‰
    if title:
        cleaned = " ".join(title.split())
        if len(cleaned) > 120:
            return cleaned[:117] + "..."
        return cleaned
    
    # å…œåº•ï¼šè¿”å›é»˜è®¤å€¼
    return "æ— æè¿°"


def generate_evidence_urls(item: dict, url: str) -> list:
    """ç”Ÿæˆ evidence_urlsï¼ˆè‡³å°‘åŒ…å«è§„èŒƒåŒ–åçš„ urlï¼Œåˆå¹¶å…¶ä»–æ¥æºé“¾æ¥å¹¶è§„èŒƒåŒ–ï¼‰"""
    urls_list = []
    seen = set()
    
    # å¿…é¡»åŒ…å«ä¸» URLï¼ˆè§„èŒƒåŒ–åï¼‰
    if url:
        normalized = normalize_url(url)
        if normalized and normalized not in seen:
            urls_list.append(normalized)
            seen.add(normalized)
    
    # åˆå¹¶å…¶ä»–æ¥æºé“¾æ¥ï¼ˆæ¯æ¡éƒ½è§„èŒƒåŒ–ï¼‰
    for field in ["sources", "links", "evidence_urls", "related_urls"]:
        value = item.get(field)
        if value:
            if isinstance(value, list):
                for u in value:
                    if u:
                        normalized = normalize_url(str(u))
                        if normalized and normalized not in seen:
                            urls_list.append(normalized)
                            seen.add(normalized)
            elif isinstance(value, str):
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•æŒ‰é€—å·/åˆ†å·åˆ†å‰²
                for u in re.split(r'[,;]', value):
                    u = u.strip()
                    if u:
                        normalized = normalize_url(u)
                        if normalized and normalized not in seen:
                            urls_list.append(normalized)
                            seen.add(normalized)
    
    # æ–­è¨€ï¼šæœ€ç»ˆåˆ—è¡¨é•¿åº¦ >= 1
    assert len(urls_list) > 0, f"âŒ é”™è¯¯: evidence_urls ä¸ºç©º (url: {url[:50]}...)"
    
    # è¿”å›åˆ—è¡¨ï¼ˆä¿æŒé¡ºåºï¼Œå·²å»é‡ï¼‰
    return urls_list


def compute_dedupe_key(url: str, hostname: str, canonical_name: str, title: str) -> str:
    """è®¡ç®— dedupe_keyï¼ˆä»¥è§„èŒƒåŒ– URL ä¸ºä¸»ï¼‰
    
    Args:
        url: åŸå§‹ URL
        hostname: ä¸»æœºå
        canonical_name: è§„èŒƒåç§°
        title: æ ‡é¢˜
        
    Returns:
        dedupe_key å­—ç¬¦ä¸²ï¼ˆæ ¼å¼ï¼šurl:sha1 æˆ– ht:sha1ï¼‰
    """
    # è‹¥ normalize å url éç©ºï¼šdedupe_key = "url:" + sha1(normalized_url)
    normalized_url = normalize_url(url)
    if normalized_url:
        url_hash = hashlib.sha1(normalized_url.encode()).hexdigest()
        return f"url:{url_hash}"
    
    # å¦åˆ™ï¼šdedupe_key = "ht:" + sha1((hostname + "|" + (canonical_name or title)).lower().strip())
    key_str = f"{hostname}|{canonical_name or title}".lower().strip()
    key_hash = hashlib.sha1(key_str.encode()).hexdigest()
    return f"ht:{key_hash}"


def get_week_start() -> datetime:
    """è·å–æœ¬å‘¨ä¸€çš„æ—¥æœŸï¼ˆUTCï¼‰"""
    today = datetime.utcnow()
    # è·å–æœ¬å‘¨ä¸€
    days_since_monday = today.weekday()
    week_start = today - timedelta(days=days_since_monday)
    return week_start.replace(hour=0, minute=0, second=0, microsecond=0)


def upsert_radar_items(client, items: list, topic_name: str, query: str = ""):
    """å°†é›·è¾¾é¡¹å†™å…¥æ•°æ®åº“
    
    Args:
        client: Supabase å®¢æˆ·ç«¯
        items: æœç´¢ç»“æœåˆ—è¡¨
        topic_name: ä¸»é¢˜åç§°
        query: æœç´¢æŸ¥è¯¢å…³é”®è¯
    """
    if not client:
        return
    
    # ç»Ÿè®¡æœ¬è½®å†™å…¥çš„è®°å½•æ•°ï¼ˆç”¨äºéªŒè¯ï¼‰
    processed_count = 0
    
    for item in items:
        url = item.get("link", "")
        if not url:
            continue
        
        url_hash = get_url_hash(url)
        hostname = get_hostname(url)
        
        # åœ¨å†™å…¥å‰åŠ ç¡¬æ–­è¨€/é»˜è®¤å€¼ï¼ˆé˜²å›å½’ï¼‰
        # ç¡®ä¿æœ€ç»ˆå†™åº“å‰ source æ°¸ä¸ä¸ºç©º
        # ä¼˜å…ˆï¼šresult.get("source")ï¼Œå…œåº•ï¼š"serper"
        item["source"] = item.get("source") or "serper"
        source = item["source"]  # ä½¿ç”¨ item ä¸­çš„ sourceï¼ˆå·²ç¡®ä¿ä¸ä¸ºç©ºï¼‰
        
        # ç¡¬æ–­è¨€ï¼šç¡®ä¿ source ä¸ä¸ºç©ºï¼ˆé˜²å›å½’ï¼‰
        assert source and source.strip() != "", f"âŒ é”™è¯¯: source å­—æ®µä¸ºç©º (url: {url[:50]}...)"
        
        data = {
            "url_hash": url_hash,
            "url": url,
            "title": item.get("title", ""),
            "snippet": item.get("snippet", ""),
            "topic": topic_name,
            "hostname": hostname,
            "source": source,  # ç¡®ä¿æ°¸ä¸ä¸º None
            "fetched_at": datetime.now(timezone.utc).isoformat()
        }
        
        # åœ¨ insert å‰æ‰“å°è°ƒè¯•ä¿¡æ¯
        debug_info = {
            "url": url[:80] + "..." if len(url) > 80 else url,
            "source": source,
            "hostname": hostname,
            "query": query[:50] + "..." if len(query) > 50 else query
        }
        print(f"    ğŸ“ Insert radar_item: {debug_info}")
        
        try:
            # ä½¿ç”¨ upsertï¼ˆæŒ‰ url_hash å»é‡ï¼‰
            client.table("radar_items").upsert(data, on_conflict="url_hash").execute()
        except Exception as e:
            print(f"âš ï¸ å†™å…¥ radar_items å¤±è´¥ ({url_hash[:8]}): {e}")
    
        processed_count += 1
    
    # éªŒè¯ï¼šä¿è¯æœ€ç»ˆå†™å…¥çš„ source å­—æ®µæ°¸ä¸ä¸ºç©ºï¼ˆæ‰€æœ‰ç©ºå€¼å·²å¡«å……ä¸º 'serper'ï¼‰
    print(f"    âœ… æœ¬è½®æˆåŠŸå¤„ç† {processed_count} æ¡è®°å½•ï¼Œæ‰€æœ‰è®°å½•çš„ source å­—æ®µå‡æœ‰æ•ˆ")


def upsert_deals(client, items: list, topic_name: str):
    """å°† deals å†™å…¥æ•°æ®åº“ï¼ˆæŒ‰ dedupe_key å»é‡ï¼Œæ”¯æŒåˆå¹¶æ›´æ–°å’ŒäºŒæ¬¡å‘ç°ç­–ç•¥ï¼‰
    
    Returns:
        dict: ç»Ÿè®¡ä¿¡æ¯ {"processed": int, "reactivated": int, "errors": int}
    """
    if not client:
        return {"processed": 0, "reactivated": 0, "errors": 0}
    
    # ç»Ÿè®¡æœ¬è½®å†™å…¥çš„è®°å½•æ•°ï¼ˆç”¨äºéªŒè¯ï¼‰
    processed_count = 0
    reactivated_count = 0
    error_count = 0
    now = datetime.now(timezone.utc)
    now_iso = now.isoformat()  # è‡ªåŠ¨åŒ…å« +00:00 æ—¶åŒºä¿¡æ¯
    
    for item in items:
        url = item.get("link", "")
        title = item.get("title", "")
        if not url or not title:
            continue
        
        hostname = get_hostname(url)
        description = item.get("snippet", "")
        
        # ç”Ÿæˆä¸‰ä¸ªå¿…éœ€å­—æ®µ
        canonical_name = generate_canonical_name(item, title, hostname)
        one_liner = generate_one_liner(item, description, title)
        new_evidence_urls = generate_evidence_urls(item, url)
        
        # è§„èŒƒåŒ– URL
        normalized_url = normalize_url(url)
        
        # è®¡ç®— dedupe_keyï¼ˆåŸºäºè§„èŒƒåŒ– URLï¼‰
        dedupe_key = compute_dedupe_key(url, hostname, canonical_name, title)
        
        # éªŒè¯å­—æ®µéç©º
        assert canonical_name and canonical_name.strip() != "", f"âŒ é”™è¯¯: canonical_name ä¸ºç©º (url: {url[:50]}...)"
        assert one_liner and one_liner.strip() != "", f"âŒ é”™è¯¯: one_liner ä¸ºç©º (url: {url[:50]}...)"
        assert new_evidence_urls and len(new_evidence_urls) > 0, f"âŒ é”™è¯¯: evidence_urls ä¸ºç©º (url: {url[:50]}...)"
        
        # æŸ¥è¯¢ç°æœ‰è®°å½•ï¼ˆåŸºäº dedupe_keyï¼‰
        existing_deal = None
        try:
            existing_response = client.table("deals")\
                .select("id,status,evidence_urls,first_seen_at,last_seen_at,seen_count,score,dismissed_reason,dismissed_at")\
                .eq("dedupe_key", dedupe_key)\
                .limit(1)\
                .execute()
            
            if existing_response.data and len(existing_response.data) > 0:
                existing_deal = existing_response.data[0]
        except Exception as e:
            print(f"âš ï¸ æŸ¥è¯¢ç°æœ‰è®°å½•å¤±è´¥ ({dedupe_key[:8]}): {e}")
            error_count += 1
            continue  # æŸ¥è¯¢å¤±è´¥ï¼Œè·³è¿‡è¯¥è®°å½•
        
        # Archived ä¿æŠ¤ï¼šå¦‚æœç°æœ‰è®°å½•çŠ¶æ€ä¸º archivedï¼Œå®Œå…¨è·³è¿‡æ›´æ–°
        if existing_deal:
            existing_status = existing_deal.get("status")
            if existing_status == "archived":
                deal_id = existing_deal.get("id")
                print(f"    â¸ï¸ Skip archived deal {deal_id} (dedupe_key: {dedupe_key[:16]}...)")
                continue  # å®Œå…¨è·³è¿‡ archived è®°å½•çš„æ›´æ–°
        
        # åˆå¹¶ evidence_urlsï¼ˆå¹¶é›†å»é‡ï¼Œä¿æŒæœ€å¤§ N æ¡ï¼‰
        EVIDENCE_URLS_MAX = 20  # æœ€å¤§ä¿ç•™ evidence_urls æ•°é‡
        if existing_deal:
            existing_evidence_urls = existing_deal.get("evidence_urls", [])
            if not isinstance(existing_evidence_urls, list):
                existing_evidence_urls = []
            
            # åˆå¹¶ï¼šæ–°å€¼ âˆª æ—§å€¼ï¼ˆå»é‡ï¼Œä¿æŒé¡ºåºï¼‰
            merged_evidence_urls = list(existing_evidence_urls)
            seen = set(existing_evidence_urls)
            for u in new_evidence_urls:
                if u not in seen:
                    merged_evidence_urls.append(u)
                    seen.add(u)
            
            # é™åˆ¶æœ€å¤§æ•°é‡ï¼ˆä¿ç•™æœ€æ–°çš„ï¼‰
            if len(merged_evidence_urls) > EVIDENCE_URLS_MAX:
                merged_evidence_urls = merged_evidence_urls[-EVIDENCE_URLS_MAX:]
            
            evidence_urls = merged_evidence_urls
            new_evidence_count = len([u for u in new_evidence_urls if u not in existing_evidence_urls])
        else:
            evidence_urls = new_evidence_urls[:EVIDENCE_URLS_MAX]  # æ–°è®°å½•ä¹Ÿé™åˆ¶æ•°é‡
            new_evidence_count = len(new_evidence_urls)
        
        # è®¡ç®— seen_count
        if existing_deal:
            seen_count = (existing_deal.get("seen_count") or 0) + 1
            first_seen_at = existing_deal.get("first_seen_at") or now_iso
        else:
            seen_count = 1
            first_seen_at = now_iso
        
        # è§„åˆ™ Aï¼šç¦æ­¢æ”¹åŠ¨ shortlisted/archived çš„ status
        # è§„åˆ™ Bï¼šè‡ªåŠ¨å¤æ´»ï¼ˆ7 å¤©ï¼‰- å½“ dismissed è®°å½•åœ¨ 7 å¤©å†…å†æ¬¡å‡ºç°æ—¶è‡ªåŠ¨å¤æ´»
        status = None  # None è¡¨ç¤ºä¸æ›´æ–° statusï¼ˆä¿æŒåŸå€¼ï¼‰
        dismissed_reason = None
        dismissed_at = None
        
        existing_status = existing_deal.get("status") if existing_deal else None
        
        # è§„åˆ™ Aï¼šç¦æ­¢æ”¹åŠ¨ shortlisted çš„ status
        # æ³¨æ„ï¼šarchived çŠ¶æ€å·²åœ¨å‰é¢å¤„ç†ï¼ˆç›´æ¥è·³è¿‡ï¼‰ï¼Œè¿™é‡Œåªå¤„ç† shortlisted
        if existing_status == "shortlisted":
            # ä¿æŒäººå·¥çŠ¶æ€ï¼Œä¸æ›´æ–° status
            pass
        elif existing_status == "dismissed":
            # è§„åˆ™ Bï¼šè‡ªåŠ¨å¤æ´»ï¼ˆ7 å¤©ï¼‰
            # å¦‚æœ now() - dismissed_at <= 7 daysï¼Œåˆ™è‡ªåŠ¨å¤æ´»
            dismissed_at_str = existing_deal.get("dismissed_at")
            
            if dismissed_at_str:
                try:
                    # è§£æ dismissed_atï¼ˆISO æ ¼å¼ï¼Œå¯èƒ½å¸¦æ—¶åŒºï¼‰
                    dismissed_at_str_clean = dismissed_at_str.replace('Z', '+00:00')
                    dismissed_at_dt = datetime.fromisoformat(dismissed_at_str_clean)
                    
                    # å¦‚æœæœ‰æ—¶åŒºä¿¡æ¯ï¼Œè½¬æ¢ä¸º UTCï¼ˆnow æ˜¯ UTCï¼‰
                    if dismissed_at_dt.tzinfo:
                        dismissed_at_utc = dismissed_at_dt.astimezone(tz=None).replace(tzinfo=None)
                    else:
                        dismissed_at_utc = dismissed_at_dt
                    
                    # è®¡ç®—æ—¶é—´å·®ï¼šå¦‚æœ now() - dismissed_at <= 7 daysï¼Œåˆ™å¤æ´»
                    time_diff = now - dismissed_at_utc
                    within_7_days = time_diff <= timedelta(days=7)
                    
                    if within_7_days:
                        # è‡ªåŠ¨å¤æ´»ï¼šåœ¨ 7 å¤©å†…å†æ¬¡å‡ºç°ï¼Œç›´æ¥å¤æ´»
                        status = "new"
                        dismissed_reason = None
                        dismissed_at = None
                        reactivated_count += 1
                        print(f"    ğŸ”„ è§„åˆ™ Bï¼šè‡ªåŠ¨å¤æ´» dismissed è®°å½• ({dedupe_key[:8]}) - è·ç¦» dismissed æ—¶é—´ {time_diff.days} å¤©ï¼ˆ<= 7 å¤©ï¼‰")
                    else:
                        # è¶…è¿‡ 7 å¤©ï¼Œä¸è‡ªåŠ¨å¤æ´»
                        print(f"    â¸ï¸ è·³è¿‡å¤æ´» ({dedupe_key[:8]}) - è·ç¦» dismissed æ—¶é—´ {time_diff.days} å¤©ï¼ˆ> 7 å¤©ï¼‰")
                except Exception as e:
                    print(f"    âš ï¸ è§£æ dismissed_at å¤±è´¥ ({dedupe_key[:8]}): {e}")
                    # å¦‚æœè§£æå¤±è´¥ï¼Œä¿å®ˆå¤„ç†ï¼šä¸å¤æ´»
            else:
                # å¦‚æœæ²¡æœ‰ dismissed_atï¼Œè¯´æ˜å¯èƒ½æ˜¯æ—§æ•°æ®ï¼Œå…è®¸å¤æ´»
                status = "new"
                dismissed_reason = None
                dismissed_at = None
                reactivated_count += 1
                print(f"    ğŸ”„ è§„åˆ™ Bï¼šè‡ªåŠ¨å¤æ´» dismissed è®°å½• ({dedupe_key[:8]}) - æ—  dismissed_atï¼ˆæ—§æ•°æ®ï¼‰")
        
        # è§„åˆ™ Aï¼šæ„å»º upsert æ•°æ®
        # æ€»æ˜¯æ›´æ–°ï¼šseen_count, last_seen_at
        # first_seen_at åªåœ¨æ–°å»ºæ—¶å†™
        data = {
            "dedupe_key": dedupe_key,
            "title": title,
            "url": normalized_url,
            "description": description,
            "canonical_name": canonical_name,
            "one_liner": one_liner,
            "evidence_urls": evidence_urls,  # å»é‡è¿½åŠ ï¼Œå·²é™åˆ¶æœ€å¤§æ•°é‡
            "topic": topic_name,
            "hostname": hostname,
            "last_seen_at": now_iso,  # è§„åˆ™ Aï¼šæ€»æ˜¯æ›´æ–°
            "seen_count": seen_count,  # è§„åˆ™ Aï¼šæ€»æ˜¯æ›´æ–° seen_count + 1
            "updated_at": now_iso
        }
        
        # è§„åˆ™ Aï¼šfirst_seen_at åªåœ¨æ–°å»ºæ—¶å†™
        if not existing_deal:
            data["created_at"] = now_iso
            data["first_seen_at"] = first_seen_at
            # ä¸æ˜¾å¼è®¾ç½® statusï¼Œè®©æ•°æ®åº“é»˜è®¤å€¼ 'new' ç”Ÿæ•ˆ
        else:
            # æ›´æ–°æ—¶ï¼Œä¸è¦†ç›– first_seen_atï¼ˆä¿æŒåŸå€¼ï¼‰
            pass
        
        # è§„åˆ™ A + è§„åˆ™ Bï¼šçŠ¶æ€æ›´æ–°é€»è¾‘
        # å…³é”®ï¼šç¦æ­¢æ”¹åŠ¨ shortlisted çš„ status
        # æ³¨æ„ï¼šarchived çŠ¶æ€å·²åœ¨å‰é¢å¤„ç†ï¼ˆç›´æ¥è·³è¿‡ï¼‰ï¼Œè¿™é‡Œåªå¤„ç† shortlisted
        if existing_status == "shortlisted":
            # è§„åˆ™ Aï¼šä¿æŒäººå·¥çŠ¶æ€ï¼Œç»ä¸æ›´æ–° status
            # seen_count å’Œ last_seen_at ä»ç„¶ä¼šæ›´æ–°ï¼ˆå·²åœ¨ä¸Šé¢çš„ data ä¸­è®¾ç½®ï¼‰
            pass
        elif status is not None:
            # è§„åˆ™ Bï¼šå¦‚æœçŠ¶æ€éœ€è¦æ›´æ–°ï¼ˆä»…å¤æ´» dismissed æ—¶ï¼‰
            data["status"] = status
            data["dismissed_reason"] = dismissed_reason
            data["dismissed_at"] = dismissed_at
        
        # å¦‚æœæœ‰ score å­—æ®µï¼Œä¹Ÿæ›´æ–°
        if "score" in item:
            data["score"] = item.get("score")
        
        # å…¥åº“å‰æ‰“å°è°ƒè¯•ä¿¡æ¯
        debug_info = {
            "dedupe_key": dedupe_key[:16] + "...",
            "canonical_name": canonical_name[:50] + "..." if len(canonical_name) > 50 else canonical_name,
            "evidence_urls_count": len(evidence_urls),
            "new_evidence_count": new_evidence_count,
            "seen_count": seen_count,
            "status": status if status else (existing_deal.get("status") if existing_deal else "new"),
            "is_new": not existing_deal
        }
        print(f"    ğŸ“ Upsert deal: {debug_info}")
        
        try:
            # ä½¿ç”¨ upsertï¼ˆæŒ‰ dedupe_key å»é‡ï¼‰
            client.table("deals").upsert(data, on_conflict="dedupe_key").execute()
            processed_count += 1
        except Exception as e:
            print(f"âš ï¸ å†™å…¥ deals å¤±è´¥ ({dedupe_key[:8]}): {e}")
    
    print(f"    âœ… æœ¬è½®æˆåŠŸå¤„ç† {processed_count} æ¡ deals è®°å½•")
    if reactivated_count > 0:
        print(f"    ğŸ”„ å…¶ä¸­ {reactivated_count} æ¡ dismissed è®°å½•å·²è‡ªåŠ¨å¤æ´»")
    if error_count > 0:
        print(f"    âš ï¸ å…¶ä¸­ {error_count} æ¡è®°å½•å†™å…¥å¤±è´¥")
    
    return {
        "processed": processed_count,
        "reactivated": reactivated_count,
        "errors": error_count
    }


def reactivate_dismissed_deals(client) -> int:
    """DB ä¾§å…œåº•ï¼šè‡ªåŠ¨å¤æ´» 7 å¤©å†…çš„ dismissed è®°å½•
    
    Args:
        client: Supabase å®¢æˆ·ç«¯
        
    Returns:
        int: å¤æ´»çš„è®°å½•æ•°
    """
    if not client:
        return 0
    
    try:
        # æ‰§è¡Œ SQL æ›´æ–°ï¼šå¤æ´» 7 å¤©å†…çš„ dismissed è®°å½•
        # æ³¨æ„ï¼šSupabase Python å®¢æˆ·ç«¯ä¸ç›´æ¥æ”¯æŒ raw SQLï¼Œéœ€è¦ä½¿ç”¨ RPC æˆ–ç›´æ¥æ›´æ–°
        # è¿™é‡Œä½¿ç”¨æŸ¥è¯¢ + æ›´æ–°çš„æ–¹å¼
        
        # æŸ¥è¯¢éœ€è¦å¤æ´»çš„è®°å½•
        response = client.table("deals")\
            .select("id")\
            .eq("status", "dismissed")\
            .gte("last_seen_at", (datetime.now(timezone.utc) - timedelta(days=7)).isoformat())\
            .execute()
        
        deal_ids = [deal.get("id") for deal in (response.data if hasattr(response, 'data') else [])]
        
        if not deal_ids:
            return 0
        
        # æ‰¹é‡æ›´æ–°
        updated_count = 0
        for deal_id in deal_ids:
            try:
                client.table("deals")\
                    .update({
                        "status": "new",
                        "dismissed_reason": None,
                        "dismissed_at": None,
                        "updated_at": datetime.now(timezone.utc).isoformat()
                    })\
                    .eq("id", deal_id)\
                    .execute()
                updated_count += 1
            except Exception as e:
                print(f"    âš ï¸ å¤æ´»è®°å½•å¤±è´¥ (id: {deal_id}): {e}")
        
        if updated_count > 0:
            print(f"    ğŸ”„ DB ä¾§å…œåº•ï¼šè‡ªåŠ¨å¤æ´» {updated_count} æ¡ dismissed è®°å½•ï¼ˆ7 å¤©å†…ï¼‰")
        
        return updated_count
        
    except Exception as e:
        print(f"    âš ï¸ DB ä¾§å¤æ´»æ£€æŸ¥å¤±è´¥: {e}")
        return 0


def health_check_deals(client, run_started_at: datetime = None) -> dict:
    """DB å¥åº·æ£€æŸ¥ï¼šæ£€æŸ¥ evidence_urlsã€seen_countã€last_seen_atã€archived ä¿æŠ¤
    
    Args:
        client: Supabase å®¢æˆ·ç«¯
        run_started_at: æœ¬æ¬¡è¿è¡Œå¼€å§‹æ—¶é—´ï¼ˆç”¨äº archived éªŒæ”¶æ£€æŸ¥ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨è¿‡å» 60 åˆ†é’Ÿï¼‰
        
    Returns:
        dict: å¥åº·æ£€æŸ¥ç»“æœï¼ŒåŒ…å« archived_updated_last_2h å­—æ®µï¼ˆå¿…å®šè¿”å› dictï¼‰
    """
    if not client:
        return {"evidence_over_20": 0, "seen_count_null": 0, "latest_last_seen_at": None, "archived_updated_last_2h": 0}
    
    try:
        # æŸ¥è¯¢æ‰€æœ‰è®°å½•è¿›è¡Œç»Ÿè®¡
        response = client.table("deals")\
            .select("evidence_urls,seen_count,last_seen_at")\
            .execute()
        
        deals = response.data if hasattr(response, 'data') else []
        
        evidence_over_20 = 0
        seen_count_null = 0
        latest_last_seen_at = None
        
        for deal in deals:
            # æ£€æŸ¥ evidence_urls è¶…è¿‡ 20 æ¡
            evidence_urls = deal.get("evidence_urls", [])
            if isinstance(evidence_urls, list) and len(evidence_urls) > 20:
                evidence_over_20 += 1
            
            # æ£€æŸ¥ seen_count ä¸º null
            if deal.get("seen_count") is None:
                seen_count_null += 1
            
            # æ‰¾æœ€æ–°çš„ last_seen_at
            last_seen = deal.get("last_seen_at")
            if last_seen:
                if latest_last_seen_at is None or last_seen > latest_last_seen_at:
                    latest_last_seen_at = last_seen
        
        # Archived ä¿æŠ¤éªŒæ”¶ï¼šæ£€æŸ¥è‡ªæœ¬æ¬¡è¿è¡Œå¼€å§‹æ—¶é—´ä»¥æ¥æ˜¯å¦æœ‰ archived è®°å½•è¢«æ›´æ–°
        archived_updated_last_2h = 0
        threshold_time_str = None
        try:
            # ä½¿ç”¨ run_started_at ä½œä¸ºé˜ˆå€¼ï¼ˆå¦‚æœæä¾›ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨è¿‡å» 60 åˆ†é’Ÿ
            if run_started_at:
                # ç¡®ä¿ run_started_at æ˜¯ timezone-aware
                if run_started_at.tzinfo is None:
                    threshold_time = run_started_at.replace(tzinfo=timezone.utc)
                else:
                    threshold_time = run_started_at
                threshold_time_str = threshold_time.isoformat()
            else:
                # ä½¿ç”¨è¿‡å» 60 åˆ†é’Ÿä½œä¸ºé˜ˆå€¼
                threshold_time = datetime.now(timezone.utc) - timedelta(minutes=60)
                threshold_time_str = threshold_time.isoformat()
            
            archived_response = client.table("deals")\
                .select("id")\
                .eq("status", "archived")\
                .gt("last_seen_at", threshold_time_str)\
                .execute()
            
            archived_updated_last_2h = len(archived_response.data if hasattr(archived_response, 'data') else [])
            
            # æ‰“å°é˜ˆå€¼æ—¶é—´ç”¨äºè°ƒè¯•
            threshold_display = threshold_time.strftime('%Y-%m-%d %H:%M:%S UTC')
            print(f"    ğŸ“… Archived éªŒæ”¶é˜ˆå€¼æ—¶é—´: {threshold_display}")
        except Exception as e:
            print(f"    âš ï¸ Archived ä¿æŠ¤éªŒæ”¶æ£€æŸ¥å¤±è´¥: {e}")
        
        result = {
            "evidence_over_20": evidence_over_20,
            "seen_count_null": seen_count_null,
            "latest_last_seen_at": latest_last_seen_at,
            "archived_updated_last_2h": archived_updated_last_2h
        }
        
        return result
        
    except Exception as e:
        print(f"    âš ï¸ å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        return {"evidence_over_20": 0, "seen_count_null": 0, "latest_last_seen_at": None, "archived_updated_last_2h": 0}


def generate_weekly_report(client, config: dict) -> str:
    """ç”Ÿæˆæœ¬å‘¨å‘¨æŠ¥ markdown"""
    if not client:
        return ""
    
    week_start = get_week_start()
    week_end = week_start + timedelta(days=6)
    
    # è·å–æœ¬å‘¨çš„ top N é›·è¾¾é¡¹
    try:
        response = client.table("radar_items")\
            .select("*")\
            .gte("fetched_at", week_start.isoformat())\
            .lte("fetched_at", week_end.isoformat())\
            .order("fetched_at", desc=True)\
            .limit(config["report"]["top_n"])\
            .execute()
        
        items = response.data if hasattr(response, 'data') else []
    except Exception as e:
        print(f"âš ï¸ è·å–å‘¨æŠ¥æ•°æ®å¤±è´¥: {e}")
        items = []
    
    # æŒ‰ topic åˆ†ç»„
    topics_dict = {}
    for item in items:
        topic = item.get("topic", "å…¶ä»–")
        if topic not in topics_dict:
            topics_dict[topic] = []
        topics_dict[topic].append(item)
    
    # ç”Ÿæˆ markdown
    report_lines = [
        f"# é›·è¾¾å‘¨æŠ¥ - {week_start.strftime('%Y-%m-%d')} è‡³ {week_end.strftime('%Y-%m-%d')}",
        "",
        f"ç”Ÿæˆæ—¶é—´: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC",
        "",
        f"## æ€»è§ˆ",
        f"- æœ¬å‘¨æ•è·é¡¹ç›®æ•°: {len(items)}",
        f"- æ¶‰åŠä¸»é¢˜æ•°: {len(topics_dict)}",
        "",
    ]
    
    # æŒ‰ä¸»é¢˜è¾“å‡º
    for topic, topic_items in topics_dict.items():
        report_lines.extend([
            f"## {topic}",
            ""
        ])
        
        for i, item in enumerate(topic_items, 1):
            title = item.get("title", "æ— æ ‡é¢˜")
            url = item.get("url", "")
            snippet = item.get("snippet", "")
            hostname = item.get("hostname", "")
            
            report_lines.extend([
                f"### {i}. {title}",
                f"",
                f"- **æ¥æº**: [{hostname}]({url})",
                f"- **æ‘˜è¦**: {snippet[:200]}{'...' if len(snippet) > 200 else ''}",
                f""
            ])
    
    return "\n".join(report_lines)


def upsert_weekly_report(client, report_content: str) -> bool:
    """å°†å‘¨æŠ¥å†™å…¥æ•°æ®åº“ï¼ˆæŒ‰ week_start å»é‡ï¼‰
    
    Args:
        client: Supabase å®¢æˆ·ç«¯
        report_content: å‘¨æŠ¥ markdown å†…å®¹
    
    Returns:
        bool: æˆåŠŸè¿”å› Trueï¼Œå¤±è´¥è¿”å› False
    """
    if not client:
        print("âŒ å†™å…¥å‘¨æŠ¥å¤±è´¥: Supabase å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
        return False
    
    week_start = get_week_start()
    
    # ç»™æ­£æ–‡å…œåº•ï¼Œé¿å…ç©ºå€¼
    report_md = report_content or ""
    
    # ç¡¬æ–­è¨€ï¼šç¡®ä¿ markdown ä¸ä¸ºç©ºï¼ˆè¡¨ç»“æ„è¦æ±‚ NOT NULLï¼‰
    assert report_md is not None, "âŒ é”™è¯¯: report_md ä¸èƒ½ä¸º None"
    
    data = {
        "week_start": week_start.isoformat(),
        "markdown": report_md,  # è¡¨ç»“æ„è¦æ±‚ markdown å­—æ®µ NOT NULL
        "content": report_md,   # å¦‚æœéœ€è¦ä¿ç•™ contentï¼Œè®©å®ƒç­‰äºåŒä¸€ä»½æ–‡æœ¬
        "created_at": datetime.now(timezone.utc).isoformat(),
        "updated_at": datetime.now(timezone.utc).isoformat()
    }
    
    try:
        # ä½¿ç”¨ upsertï¼ˆæŒ‰ week_start å»é‡ï¼Œè¡¨ä¸Šåº”æœ‰å”¯ä¸€çº¦æŸï¼‰
        client.table("weekly_reports").upsert(data, on_conflict="week_start").execute()
        print(f"âœ… å‘¨æŠ¥å·²ä¿å­˜: {week_start.strftime('%Y-%m-%d')} (markdown é•¿åº¦: {len(report_md)} å­—ç¬¦)")
        return True
    except Exception as e:
        print(f"âŒ å†™å…¥ weekly_reports å¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°
    
    Returns:
        int: æˆåŠŸè¿”å› 0ï¼Œå¤±è´¥è¿”å›é 0
    """
    start_time = datetime.now(timezone.utc)
    print("ğŸš€ å¯åŠ¨é›·è¾¾æŠ“å–ä»»åŠ¡...")
    print(f"   å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')} UTC")
    
    # åŠ è½½é…ç½®
    try:
        config = load_config()
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥ï¼Œä»»åŠ¡ç»ˆæ­¢")
        return 1
    
    # è·å– Supabase å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨ service role ä»¥æ‹¥æœ‰æ›´é«˜æƒé™ï¼‰
    client = get_supabase_client(use_service_role=True)
    if not client:
        print("âŒ æ— æ³•è¿æ¥åˆ° Supabaseï¼Œä»»åŠ¡ç»ˆæ­¢")
        return 1
    
    # ç›‘æ§ç»Ÿè®¡ä¿¡æ¯
    total_items_fetched = 0  # æœ¬æ¬¡æŠ“å–åˆ°çš„æ€»æ¡æ•°
    total_deals_upserted = 0  # æœ¬æ¬¡ upsert çš„æ€»æ¡æ•°
    total_deals_errors = 0  # æœ¬æ¬¡é”™è¯¯çš„æ€»æ¡æ•°
    total_deals_reactivated = 0  # æœ¬æ¬¡å¤æ´»çš„æ€»æ¡æ•°
    
    # éå†æ‰€æœ‰ä¸»é¢˜
    for topic in config["topics"]:
        topic_name = topic["name"]
        queries = topic["queries"]
        weight = topic.get("weight", 1.0)
        
        print(f"\nğŸ“Œ å¤„ç†ä¸»é¢˜: {topic_name} (æƒé‡: {weight})")
        
        # éå†æ‰€æœ‰æŸ¥è¯¢
        for query in queries:
            print(f"  ğŸ” æœç´¢: {query}")
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºä¸­æ–‡æŸ¥è¯¢
            has_chinese = any('\u4e00' <= char <= '\u9fff' for char in query)
            
            if has_chinese:
                # ä¸­æ–‡æŸ¥è¯¢ä½¿ç”¨ä¸­å›½åŒºé…ç½®
                results = search_serper(
                    query,
                    gl=config["search"]["gl_cn"],
                    hl=config["search"]["hl_cn"],
                    num=config["search"]["results_per_query"]
                )
            else:
                # è‹±æ–‡æŸ¥è¯¢ä½¿ç”¨ç¾å›½åŒºé…ç½®
                results = search_serper(
                    query,
                    gl=config["search"]["gl"],
                    hl=config["search"]["hl"],
                    num=config["search"]["results_per_query"]
                )
            
            if results:
                fetched_count = len(results)
                total_items_fetched += fetched_count
                print(f"    âœ… æ‰¾åˆ° {fetched_count} æ¡ç»“æœ")
                
                # å†™å…¥é›·è¾¾é¡¹ï¼ˆä¼ å…¥ query ç”¨äºè°ƒè¯•æ‰“å°ï¼‰
                upsert_radar_items(client, results, topic_name, query)
                
                # å†™å…¥ dealsï¼ˆè¿”å›ç»Ÿè®¡ä¿¡æ¯ï¼‰
                stats = upsert_deals(client, results, topic_name)
                if stats:
                    total_deals_upserted += stats.get("processed", 0)
                    total_deals_errors += stats.get("errors", 0)
                    total_deals_reactivated += stats.get("reactivated", 0)
            else:
                print(f"    âš ï¸ æœªæ‰¾åˆ°ç»“æœ")
    
    # ç”Ÿæˆå¹¶ä¿å­˜å‘¨æŠ¥
    print(f"\nğŸ“ ç”Ÿæˆå‘¨æŠ¥...")
    report_content = generate_weekly_report(client, config)
    if report_content:
        report_saved = upsert_weekly_report(client, report_content)
        if report_saved:
            print(f"âœ… å‘¨æŠ¥å·²ç”Ÿæˆå¹¶ä¿å­˜")
        else:
            print(f"âŒ å‘¨æŠ¥ç”ŸæˆæˆåŠŸï¼Œä½†ä¿å­˜å¤±è´¥")
            return 1
    else:
        print(f"âš ï¸ å‘¨æŠ¥ç”Ÿæˆå¤±è´¥æˆ–æ•°æ®ä¸ºç©º")
        # æ³¨æ„ï¼šæ•°æ®ä¸ºç©ºä¸ç®—å¤±è´¥ï¼Œå¯èƒ½æ˜¯æœ¬å‘¨æ²¡æœ‰æ–°æ•°æ®
    
    # (1) DB ä¾§å…œåº•ï¼šè‡ªåŠ¨å¤æ´» 7 å¤©å†…çš„ dismissed è®°å½•
    print(f"\nğŸ”„ æ‰§è¡Œ DB ä¾§å¤æ´»å…œåº•æ£€æŸ¥...")
    db_reactivated_count = reactivate_dismissed_deals(client)
    total_deals_reactivated += db_reactivated_count
    
    # (2) æœ€å°åŒ–ç›‘æ§æ—¥å¿—
    end_time = datetime.now(timezone.utc)
    duration = end_time - start_time
    duration_seconds = duration.total_seconds()
    
    print(f"\n" + "=" * 60)
    print(f"ğŸ“Š æœ¬æ¬¡è¿è¡Œç›‘æ§æ—¥å¿—")
    print(f"=" * 60)
    print(f"  æœ¬æ¬¡æŠ“å–åˆ°: {total_items_fetched} æ¡")
    print(f"  æœ¬æ¬¡ upsert: {total_deals_upserted} æ¡")
    print(f"  å‡ºç°é”™è¯¯: {total_deals_errors} æ¡")
    print(f"  è¢«å¤æ´»: {total_deals_reactivated} æ¡ï¼ˆå« DB ä¾§å…œåº• {db_reactivated_count} æ¡ï¼‰")
    print(f"  æœ¬æ¬¡è¿è¡Œè€—æ—¶: {duration_seconds:.2f} ç§’ ({duration_seconds/60:.2f} åˆ†é’Ÿ)")
    print(f"  æœ€åæˆåŠŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')} UTC")
    print(f"=" * 60)
    
    # (3) DB å¥åº·æ£€æŸ¥ï¼ˆæ¯å¤©è·‘ä¸€æ¬¡ï¼Œå¯é€‰ï¼šåªåœ¨ç‰¹å®šæ—¶é—´è¿è¡Œï¼‰
    # è¿™é‡Œæ¯æ¬¡éƒ½è¿è¡Œï¼Œå®é™…å¯ä»¥æ ¹æ®éœ€è¦æ”¹ä¸ºæ¯å¤©ä¸€æ¬¡
    print(f"\nğŸ¥ æ‰§è¡Œ DB å¥åº·æ£€æŸ¥...")
    health_result = health_check_deals(client, run_started_at=start_time)
    if health_result:
        print(f"    ğŸ“Š å¥åº·æ£€æŸ¥ç»“æœ:")
        print(f"      - evidence_urls è¶…è¿‡ 20 æ¡: {health_result.get('evidence_over_20', 0)} æ¡")
        print(f"      - seen_count ä¸º null: {health_result.get('seen_count_null', 0)} æ¡")
        print(f"      - æœ€æ–° last_seen_at: {health_result.get('latest_last_seen_at', 'N/A')}")
        print(f"      - archived è®°å½•åœ¨è¿‡å» 2 å°æ—¶å†…è¢«æ›´æ–°: {health_result.get('archived_updated_last_2h', 0)} æ¡")
        
        # å¦‚æœæœ‰å¼‚å¸¸ï¼Œæ‰“å°è­¦å‘Š
        if health_result.get('evidence_over_20', 0) > 0:
            print(f"    âš ï¸ è­¦å‘Šï¼šå‘ç° {health_result.get('evidence_over_20')} æ¡è®°å½•çš„ evidence_urls è¶…è¿‡ 20 æ¡")
        if health_result.get('seen_count_null', 0) > 0:
            print(f"    âš ï¸ è­¦å‘Šï¼šå‘ç° {health_result.get('seen_count_null')} æ¡è®°å½•çš„ seen_count ä¸º null")
        
        # Archived ä¿æŠ¤éªŒæ”¶ï¼šå¦‚æœè¿‡å» 2 å°æ—¶å†…æœ‰ archived è®°å½•è¢«æ›´æ–°ï¼Œè¿”å›é 0 é€€å‡ºç 
        archived_updated_count = health_result.get('archived_updated_last_2h', 0)
        if archived_updated_count > 0:
            print(f"\nâŒ éªŒæ”¶å¤±è´¥ï¼šå‘ç° {archived_updated_count} æ¡ archived è®°å½•åœ¨è¿‡å» 2 å°æ—¶å†…è¢«æ›´æ–°")
            print(f"   è¿™è¿åäº† archived ä¿æŠ¤è§„åˆ™ï¼Œå¯èƒ½å­˜åœ¨ä»£ç å›å½’")
            print(f"   è¯·æ£€æŸ¥ runner ä»£ç ä¸­çš„ archived ä¿æŠ¤é€»è¾‘")
            return 1
    
    print(f"\nâœ… é›·è¾¾æŠ“å–ä»»åŠ¡å®Œæˆï¼")
    return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code if exit_code is not None else 0)
