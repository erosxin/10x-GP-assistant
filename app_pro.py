"""
GP_Partner_Pro - é«˜çº§æŠ•èµ„è¾…åŠ©ç³»ç»Ÿï¼ˆä¸»åŠ¨å­¦ä¹ ç‰ˆæœ¬ï¼‰
æ”¯æŒå¤šæ¨¡å¼åˆ‡æ¢ã€OpenRouter APIã€å†å²è®°å½•å’ŒæŠ¥å‘Šå¯¼å‡º
é›†æˆä¸»åŠ¨å­¦ä¹  Agentï¼šRSS æŠ“å–ã€ç½‘ç»œæœç´¢ã€LLM æ€»ç»“ã€çŸ¥è¯†åº“æ£€ç´¢
"""

import streamlit as st
import os
import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Dict, List, Union
import re
import zipfile
import io
import hashlib
import time

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥ï¼ˆå¿…éœ€ä¾èµ–ï¼‰
try:
    from openai import OpenAI
    from dotenv import load_dotenv
    import PyPDF2
    from docx import Document
    from docx.shared import Pt, RGBColor
    from docx.enum.text import WD_ALIGN_PARAGRAPH
    import pandas as pd
except ImportError as e:
    st.error(f"âŒ ç¼ºå°‘å¿…è¦çš„ä¾èµ–åº“: {e}")
    st.stop()

# ç«‹å³åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆåœ¨å¯¼å…¥å…¶ä»–æ¨¡å—ä¹‹å‰ï¼‰
# ç¡®ä¿åœ¨ Windows å’Œå…¶ä»–å¹³å°ä¸Šéƒ½èƒ½æ­£ç¡®è¯»å–é¡¹ç›®æ ¹ç›®å½•çš„ .env æ–‡ä»¶
load_dotenv(dotenv_path=Path(__file__).parent / '.env', override=False)

# è½¯ä¾èµ–å¯¼å…¥ï¼ˆå¯é€‰ï¼Œç”¨äºä¸»åŠ¨å­¦ä¹ åŠŸèƒ½ï¼‰
HAS_DUCKDUCKGO = False
HAS_FEEDPARSER = False
HAS_SUPABASE = False
try:
    from duckduckgo_search import DDGS
    HAS_DUCKDUCKGO = True
except ImportError:
    pass

try:
    import feedparser
    HAS_FEEDPARSER = True
except ImportError:
    pass

try:
    from db.supabase_db import get_supabase_client
    HAS_SUPABASE = True
except ImportError:
    pass

# æ³¨ï¼šå·²æ”¾å¼ƒå‘é‡æ£€ç´¢æ–¹æ¡ˆï¼Œæ”¹ç”¨åŸºäº LLM æ ‡ç­¾æå–çš„è½»é‡çº§è®°å¿†ç³»ç»Ÿ
# æ³¨æ„ï¼šç¯å¢ƒå˜é‡å·²åœ¨ä¸Šé¢åŠ è½½ï¼ˆåœ¨å¯¼å…¥ dotenv ä¹‹åç«‹å³è°ƒç”¨ï¼‰

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="GP Partner Pro",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# å¸¸é‡å®šä¹‰
PROMPTS_DIR = Path("prompts")
HISTORY_DIR = Path("history_data")
BASE_URL = "https://openrouter.ai/api/v1"
DEFAULT_MODEL = "google/gemini-flash-1.5"
KNOWLEDGE_BASE_FILE = Path("project_database.csv")
MEMORY_STORE_FILE = Path("memory_store.json")  # åŸºäºæ ‡ç­¾çš„è®°å¿†å­˜å‚¨æ–‡ä»¶
EVOLUTION_LOG_FILE = Path("evolution_log.md")  # AI è¿›åŒ–æ—¥å¿—æ–‡ä»¶
CONFIG_FILE = Path("config.json")  # é…ç½®æŒä¹…åŒ–æ–‡ä»¶
KNOWLEDGE_BRAIN_FILE = Path("knowledge_brain.json")  # ä¸»åŠ¨å­¦ä¹ çŸ¥è¯†åº“æ–‡ä»¶

# ç¡®ä¿å¿…è¦çš„æ–‡ä»¶å¤¹å­˜åœ¨
HISTORY_DIR.mkdir(exist_ok=True)
PROMPTS_DIR.mkdir(exist_ok=True)


# ==================== é…ç½®æŒä¹…åŒ–ç³»ç»Ÿ ====================

def load_config() -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        if CONFIG_FILE.exists():
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                config = json.load(f)
                return config
        else:
            # è¿”å›é»˜è®¤é…ç½®
            return {
                "api_key": os.getenv("OPENROUTER_API_KEY", ""),
                "base_url": BASE_URL,
                "model": DEFAULT_MODEL,
                "http_proxy": "http://127.0.0.1:7890"
            }
    except Exception as e:
        print(f"âš ï¸ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return {
            "api_key": os.getenv("OPENROUTER_API_KEY", ""),
            "base_url": BASE_URL,
            "model": DEFAULT_MODEL,
            "http_proxy": "http://127.0.0.1:7890"
        }


def save_config():
    """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
    try:
        config = {
            "api_key": st.session_state.get("api_key_input", ""),
            "base_url": st.session_state.get("base_url_input", BASE_URL),
            "model": st.session_state.get("model_input", DEFAULT_MODEL),
            "http_proxy": st.session_state.get("http_proxy_input", "http://127.0.0.1:7890")
        }
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {e}")


# ==================== åŸºäº LLM æ ‡ç­¾æå–çš„è½»é‡çº§è®°å¿†ç³»ç»Ÿ ====================

def extract_tags_from_text(text: str, client: OpenAI, model: str) -> List[str]:
    """ä½¿ç”¨ LLM ä»æ–‡æœ¬ä¸­æå–é¡¹ç›®æ ‡ç­¾"""
    try:
        tag_extraction_prompt = f"""è¯·ä»ä»¥ä¸‹é¡¹ç›®æ–‡æœ¬ä¸­æå–3-5ä¸ªæ ¸å¿ƒæ ‡ç­¾ï¼ˆå…³é”®è¯ï¼‰ï¼Œç”¨äºé¡¹ç›®åˆ†ç±»å’ŒåŒ¹é…ã€‚

æ–‡æœ¬å†…å®¹ï¼š
{text[:1000]}

è¦æ±‚ï¼š
1. æ ‡ç­¾åº”è¯¥æ˜¯é¡¹ç›®çš„æ ¸å¿ƒæŠ€æœ¯ã€è¡Œä¸šã€å•†ä¸šæ¨¡å¼ç­‰å…³é”®ç‰¹å¾
2. è¿”å›æ ¼å¼ï¼šçº¯æ–‡æœ¬ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¸è¦ä½¿ç”¨ä»»ä½•æ ‡è®°ç¬¦å·
3. ä¾‹å¦‚ï¼šAI, åŒ»ç–—, B2B, SaaS, å¤šæ¨¡æ€

æ ‡ç­¾ï¼š"""

        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é¡¹ç›®æ ‡ç­¾æå–åŠ©æ‰‹ã€‚åªéœ€è¿”å›æ ‡ç­¾ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€‚"},
                {"role": "user", "content": tag_extraction_prompt}
            ],
            temperature=0.3,
            max_tokens=100
        )
        
        tags_text = response.choices[0].message.content.strip()
        # è§£ææ ‡ç­¾
        tags = [tag.strip() for tag in tags_text.split(',') if tag.strip()]
        return tags[:5]  # æœ€å¤šè¿”å›5ä¸ªæ ‡ç­¾
    except Exception as e:
        print(f"âš ï¸ æ ‡ç­¾æå–å¤±è´¥: {e}")
        return []


class MemoryManager:
    """åŸºäº LLM æ ‡ç­¾æå–çš„è½»é‡çº§è®°å¿†ç³»ç»Ÿç®¡ç†å™¨ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    _instance = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(MemoryManager, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        
        self.memory_store_path = MEMORY_STORE_FILE
        self.memories = []
        self.enabled = True
        
        # åŠ è½½è®°å¿†åº“
        self._load_memories()
        self._initialized = True
    
    def _load_memories(self):
        """ä» JSON æ–‡ä»¶åŠ è½½è®°å¿†åº“"""
        try:
            if self.memory_store_path.exists():
                with open(self.memory_store_path, 'r', encoding='utf-8') as f:
                    self.memories = json.load(f)
            else:
                self.memories = []
        except Exception as e:
            print(f"âš ï¸ åŠ è½½è®°å¿†åº“å¤±è´¥: {e}")
            self.memories = []
    
    def _save_memories(self):
        """ä¿å­˜è®°å¿†åº“åˆ° JSON æ–‡ä»¶"""
        try:
            with open(self.memory_store_path, 'w', encoding='utf-8') as f:
                json.dump(self.memories, f, ensure_ascii=False, indent=2)
            return True
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜è®°å¿†åº“å¤±è´¥: {e}")
            return False
    
    def query_similar(self, text: str, client: OpenAI, model: str, top_k: int = 3) -> List[Dict]:
        """åŸºäºæ ‡ç­¾åŒ¹é…æŸ¥è¯¢ç›¸ä¼¼çš„å†å²é¡¹ç›®"""
        if not self.enabled or len(self.memories) == 0:
            return []
        
        try:
            # æå–å½“å‰é¡¹ç›®çš„æ ‡ç­¾
            query_tags = extract_tags_from_text(text, client, model)
            if not query_tags:
                return []
            
            # è®¡ç®—æ¯ä¸ªå†å²é¡¹ç›®çš„æ ‡ç­¾åŒ¹é…åº¦
            scored_projects = []
            for memory in self.memories:
                memory_tags = memory.get('tags', [])
                if not memory_tags:
                    continue
                
                # è®¡ç®—æ ‡ç­¾äº¤é›†æ•°é‡ï¼ˆç®€å•çš„æ ‡ç­¾åŒ¹é…ï¼‰
                common_tags = set(query_tags) & set(memory_tags)
                match_score = len(common_tags) / max(len(query_tags), len(memory_tags)) if max(len(query_tags), len(memory_tags)) > 0 else 0
                
                if match_score > 0:  # è‡³å°‘æœ‰ä¸€ä¸ªå…±åŒæ ‡ç­¾
                    scored_projects.append({
                        'name': memory.get('id', 'æœªçŸ¥é¡¹ç›®'),
                        'score': memory.get('score', 'N/A'),
                        'summary': memory.get('summary', ''),
                        'match_score': match_score,
                        'common_tags': list(common_tags)
                    })
            
            # æŒ‰åŒ¹é…åº¦æ’åºï¼Œè¿”å›å‰ top_k ä¸ª
            scored_projects.sort(key=lambda x: x['match_score'], reverse=True)
            return scored_projects[:top_k]
        except Exception as e:
            print(f"âš ï¸ æŸ¥è¯¢ç›¸ä¼¼é¡¹ç›®å¤±è´¥: {e}")
            return []
    
    def add_memory(self, name: str, summary: str, full_text: str, score, tags: List[str] = None, meta: Dict = None):
        """æ·»åŠ é¡¹ç›®åˆ°è®°å¿†åº“"""
        if not self.enabled:
            return False
        
        try:
            # å¦‚æœæœªæä¾›æ ‡ç­¾ï¼Œä½¿ç”¨ç©ºåˆ—è¡¨ï¼ˆæ ‡ç­¾ä¼šåœ¨å¤–éƒ¨é€šè¿‡ LLM æå–ï¼‰
            if tags is None:
                tags = []
            
            memory_entry = {
                "id": name,
                "summary": summary,
                "score": score,
                "tags": tags,
                "timestamp": datetime.now().isoformat()
            }
            
            if meta:
                memory_entry.update(meta)
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨åŒåé¡¹ç›®ï¼Œå¦‚æœå­˜åœ¨åˆ™æ›´æ–°ï¼Œå¦åˆ™æ·»åŠ 
            existing_index = None
            for i, mem in enumerate(self.memories):
                if mem.get('id') == name:
                    existing_index = i
                    break
            
            if existing_index is not None:
                self.memories[existing_index] = memory_entry
            else:
                self.memories.append(memory_entry)
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            return self._save_memories()
        except Exception as e:
            print(f"âš ï¸ æ·»åŠ è®°å¿†å¤±è´¥: {e}")
            return False
    
    def get_count(self) -> int:
        """è·å–çŸ¥è¯†åº“ä¸­çš„é¡¹ç›®æ•°é‡"""
        return len(self.memories) if self.enabled else 0


# å…¨å±€ MemoryManager å®ä¾‹
memory_manager = MemoryManager()


# ==================== ä¸»åŠ¨å­¦ä¹  Agent - KnowledgeManager ====================

class KnowledgeManager:
    """ä¸»åŠ¨å­¦ä¹ çŸ¥è¯†ç®¡ç†å™¨ï¼šRSS æŠ“å–ã€æœç´¢ã€LLM æ€»ç»“ã€çŸ¥è¯†åº“æ£€ç´¢"""
    
    def __init__(self):
        self.knowledge_file = KNOWLEDGE_BRAIN_FILE
        self.knowledge_base = []  # å­˜å‚¨å­¦ä¹ åˆ°çš„çŸ¥è¯†æ¡ç›®
        self._load_knowledge()
    
    def _load_knowledge(self):
        """åŠ è½½çŸ¥è¯†åº“"""
        try:
            if self.knowledge_file.exists():
                with open(self.knowledge_file, 'r', encoding='utf-8') as f:
                    self.knowledge_base = json.load(f)
            else:
                self.knowledge_base = []
        except Exception as e:
            print(f"âš ï¸ åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {e}")
            self.knowledge_base = []
    
    def _save_knowledge(self):
        """ä¿å­˜çŸ¥è¯†åº“"""
        try:
            with open(self.knowledge_file, 'w', encoding='utf-8') as f:
                json.dump(self.knowledge_base, f, ensure_ascii=False, indent=2)
            return True
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜çŸ¥è¯†åº“å¤±è´¥: {e}")
            return False
    
    def fetch_rss(self, rss_url: str, max_items: int = 10) -> List[Dict]:
        """ä» RSS æºæŠ“å–å†…å®¹"""
        if not HAS_FEEDPARSER:
            return []
        
        try:
            feed = feedparser.parse(rss_url)
            items = []
            for entry in feed.entries[:max_items]:
                items.append({
                    'title': entry.get('title', ''),
                    'link': entry.get('link', ''),
                    'summary': entry.get('summary', ''),
                    'published': entry.get('published', ''),
                    'source': rss_url
                })
            return items
        except Exception as e:
            print(f"âš ï¸ RSS æŠ“å–å¤±è´¥ ({rss_url}): {e}")
            return []
    
    def optimize_query_with_llm(self, query: str, client: OpenAI, model: str) -> str:
        """é¢å‘æœºæ„ä¿¡æºç”Ÿæˆè‹±æ–‡æœç´¢è¯"""
        try:
            # è·å–å½“å‰å¹´ä»½ï¼ˆç”¨äºé»˜è®¤ï¼‰
            from datetime import datetime
            current_year = datetime.now().year
            next_year = current_year + 1
            
            prompt = f"""Translate this topic into a SINGLE, CONCISE English search query optimized for institutional sources (a16z, YC, TechCrunch, FT, Bloomberg).

User topic: {query}

CRITICAL RULES:
1. 3-6 key terms only.
2. MUST include year (e.g., 2025, 2026). If user didn't specify, use {current_year} or {next_year}.
3. Use ONE investment research keyword: "thesis" / "outlook" / "funding" / "investment trends" / "market landscape"
4. Focus on core entity + year + investment keyword (e.g., "Y Combinator AI investment trends 2025")
5. Output ONLY the query string, no explanations.

Examples:
- "Y Combinator AI 2025" â†’ "Y Combinator AI investment trends 2025"
- "A16Z funding" â†’ "a16z funding outlook {current_year}"
- "AI market" â†’ "AI investment trends {current_year}"

Optimized search query:"""
            
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "You are a search query optimizer for institutional investment research. Output ONLY the optimized English search query (3-6 key terms with year and investment keyword), no explanations."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=60
            )
            
            optimized = response.choices[0].message.content.strip()
            # ç§»é™¤å¯èƒ½çš„å¼•å·
            optimized = optimized.strip('"').strip("'")
            
            # éªŒè¯ï¼šå¦‚æœæŸ¥è¯¢å¤ªçŸ­æˆ–æ²¡æœ‰å¹´ä»½ï¼Œå°è¯•æ‰‹åŠ¨æ·»åŠ 
            words = optimized.split()
            has_year = any(word.isdigit() and len(word) == 4 for word in words)
            if not has_year and len(words) < 6:
                # æ·»åŠ é»˜è®¤å¹´ä»½
                optimized = f"{optimized} {current_year}"
                words = optimized.split()
            
            # é¢å¤–éªŒè¯ï¼šå¦‚æœä¼˜åŒ–åçš„æŸ¥è¯¢ä»ç„¶å¤ªé•¿ï¼ˆè¶…è¿‡6ä¸ªå•è¯ï¼‰ï¼Œæ‰‹åŠ¨æˆªå–å‰6ä¸ª
            if len(words) > 6:
                optimized = " ".join(words[:6])
                print(f"âš ï¸ æŸ¥è¯¢è¿‡é•¿ï¼Œå·²æˆªå–å‰6ä¸ªè¯: '{optimized}'")
            
            return optimized if optimized else query
        except Exception as e:
            print(f"âš ï¸ æŸ¥è¯¢ä¼˜åŒ–å¤±è´¥: {e}")
            return query  # å¤±è´¥æ—¶è¿”å›åŸæŸ¥è¯¢
    
    def search_web(self, query: str, max_results: int = 5, proxy: str = None, client: OpenAI = None, model: str = None, region: str = "us-en") -> List[Dict]:
        """
        æœºæ„çº§æœç´¢ï¼šä¼˜å…ˆæ–°é—»æºï¼Œåå‘è¿‡æ»¤ä¸­æ–‡å†…å®¹å†œåœº
        """
        if not HAS_DUCKDUCKGO:
            return []
        
        # å…³é”®ä¿®å¤ï¼šç¡®ä¿ä½¿ç”¨ä¼ å…¥çš„ queryï¼ˆåº”è¯¥æ˜¯è‹±æ–‡ï¼‰ï¼Œä¸å†é‡æ–°ç¿»è¯‘
        optimized_query = query.strip() if query else ""
        
        # å¦‚æœæä¾›äº† LLM å®¢æˆ·ç«¯å’Œæ¨¡å‹ï¼Œä¸”æŸ¥è¯¢å¯èƒ½æ˜¯ä¸­æ–‡ï¼Œæ‰è¿›è¡Œä¼˜åŒ–
        if client and model:
            has_chinese = any('\u4e00' <= char <= '\u9fff' for char in optimized_query) if optimized_query else False
            if has_chinese or len(optimized_query) <= 2:
                print(f"âš ï¸ [WARNING] æ£€æµ‹åˆ°ä¸­æ–‡æŸ¥è¯¢æˆ–å¼‚å¸¸æŸ¥è¯¢: '{optimized_query}'ï¼Œå°è¯•äºŒæ¬¡ç¿»è¯‘...")
                optimized_query = self.optimize_query_with_llm(query, client, model)
                print(f"ğŸ” æŸ¥è¯¢ä¼˜åŒ–: '{query}' -> '{optimized_query}'")
        
        # éªŒè¯æŸ¥è¯¢æœ‰æ•ˆæ€§
        if not optimized_query or len(optimized_query.strip()) < 2:
            print(f"âŒ [ERROR] æŸ¥è¯¢æ— æ•ˆ: '{optimized_query}'ï¼Œæ— æ³•æœç´¢")
            return []
        
        # (1) ç»Ÿä¸€ä»£ç†æ ¼å¼
        proxy_str = None
        if proxy and proxy.strip():
            proxy_str = proxy.strip()
            if not proxy_str.startswith("http"):
                proxy_str = f"http://{proxy_str}"
        
        # (2) æ„é€ "åå‘ç«™ç‚¹è¿‡æ»¤" query
        chinese_sites = [
            "-site:zhihu.com",
            "-site:baidu.com",
            "-site:zhidao.baidu.com",
            "-site:wenku.baidu.com",
            "-site:csdn.net",
            "-site:juejin.cn",
            "-site:weibo.com",
            "-site:bilibili.com"
        ]
        filtered_query = f"{optimized_query} {' '.join(chinese_sites)}"
        
        original_query_for_log = optimized_query
        print(f"ğŸ” [Search] åŸå§‹æŸ¥è¯¢: '{original_query_for_log}'")
        print(f"ğŸ” [Search] æœ€ç»ˆæŸ¥è¯¢ï¼ˆå¸¦åå‘è¿‡æ»¤ï¼‰: '{filtered_query[:100]}...' (é•¿åº¦: {len(filtered_query)})")
        print(f"ğŸ” [Search] é…ç½®: region={region}, max_results={max_results}, proxy={proxy_str}")
        
        def _normalize_result(r: Dict, query_used: str, source_type: str) -> Dict:
            """å­—æ®µå½’ä¸€åŒ–ï¼šå…¼å®¹ä¸åŒç‰ˆæœ¬çš„ duckduckgo_search"""
            return {
                'title': r.get('title', r.get('headline', '')),
                'url': r.get('href', r.get('url', r.get('link', ''))),
                'snippet': r.get('body', r.get('snippet', r.get('summary', ''))),
                'source': r.get('source', '') if source_type == 'news' else '',
                'date': r.get('date', '') if source_type == 'news' else '',
                'query': query_used,
                'original_query': query,
                'source_type': source_type
            }
        
        def _ddg_news(q: str) -> List[Dict]:
            """å†…éƒ¨æ‰§è¡Œå‡½æ•°ï¼šæ–°é—»æœç´¢"""
            try:
                with DDGS(proxy=proxy_str, timeout=30) as ddgs:
                    raw_results = list(ddgs.news(
                        keywords=q,
                        region=region,
                        safesearch='off',
                        max_results=max_results
                    ))
                    normalized = [_normalize_result(r, q, 'news') for r in raw_results]
                    print(f"ğŸ“° [News] å‘½ä¸­ {len(normalized)} æ¡æ–°é—»ç»“æœ")
                    return normalized
            except Exception as inner_e:
                print(f"âš ï¸ [News] æ–°é—»æœç´¢å¤±è´¥: {inner_e}")
                return []
        
        def _ddg_text(q: str) -> List[Dict]:
            """å†…éƒ¨æ‰§è¡Œå‡½æ•°ï¼šæ–‡æœ¬æœç´¢"""
            try:
                with DDGS(proxy=proxy_str, timeout=30) as ddgs:
                    raw_results = list(ddgs.text(
                        keywords=q,
                        region=region,
                        safesearch='off',
                        max_results=max_results
                    ))
                    normalized = [_normalize_result(r, q, 'text') for r in raw_results]
                    print(f"ğŸ“„ [Text] å‘½ä¸­ {len(normalized)} æ¡æ–‡æœ¬ç»“æœ")
                    return normalized
            except Exception as inner_e:
                print(f"âš ï¸ [Text] æ–‡æœ¬æœç´¢å¤±è´¥: {inner_e}")
                return []

        # (3) æœç´¢ç­–ç•¥ï¼šä¼˜å…ˆ newsï¼Œç©ºç»“æœå† text
        results = []
        news_count = 0
        text_count = 0
        used_fallback = False
        
        # ç¬¬ä¸€è½®ï¼šä½¿ç”¨å¸¦åå‘è¿‡æ»¤çš„å®Œæ•´æŸ¥è¯¢
        results = _ddg_news(filtered_query)
        news_count = len(results)
        
        if not results:
            print("âš ï¸ [Strategy] æ–°é—»æœç´¢æ— ç»“æœï¼Œåˆ‡æ¢åˆ°æ–‡æœ¬æœç´¢...")
            results = _ddg_text(filtered_query)
            text_count = len(results)
        
        # é™çº§é‡è¯•æœºåˆ¶ï¼šå¦‚æœä»ä¸ºç©ºï¼Œç®€åŒ–æŸ¥è¯¢åå†æ¬¡å°è¯•
        if not results and len(optimized_query.split()) > 3:
            used_fallback = True
            simplified_query_base = " ".join(optimized_query.split()[:3])
            simplified_query = f"{simplified_query_base} {' '.join(chinese_sites)}"
            print(f"âš ï¸ [Fallback] è§¦å‘é™çº§æŸ¥è¯¢: '{simplified_query_base}' (å¸¦åå‘è¿‡æ»¤)")
            
            results = _ddg_news(simplified_query)
            if results:
                news_count = len(results)
            else:
                results = _ddg_text(simplified_query)
                if results:
                    text_count = len(results)
        
        # æœ€ç»ˆæ—¥å¿—æ±‡æ€»
        print(f"âœ… [Summary] æœç´¢å®Œæˆ: News={news_count}, Text={text_count}, æ€»è®¡={len(results)}, é™çº§={used_fallback}")

        return results
    
    def summarize_with_llm(self, content: str, client: OpenAI, model: str, topic: str = "") -> Optional[str]:
        """ä½¿ç”¨ LLM æ€»ç»“å†…å®¹ï¼ˆè§’è‰²æ‰®æ¼”ï¼šé«˜çº§æŠ•èµ„åˆ†æå¸ˆï¼‰"""
        try:
            # æ–°çš„è§’è‰²æ‰®æ¼” System Prompt
            system_prompt = """You are an elite **Investment Analyst** at a top-tier AI Incubator. You report directly to the **Senior Investment Director**.
Your goal is to scan external information and extract high-value **Investment Signals**, **Market Alpha**, and **Strategic Threats**.

**Evaluation Criteria (The "VC Filter"):**
1. **Signal vs. Noise:** Ignore generic news, marketing fluff, or consumer-level tutorials. Focus on *industry shifts, funding dynamics, emerging tech stacks, and competitor moves*.
2. **Relevance:** If the content offers NO value to an investor (e.g., a basic tutorial on "how to use ChatGPT" when asking about "Agent Trends", or games like 'é£çµæœˆå½±', unrelated entertainment, car models when asking about YC/Y Combinator), output exactly: `Irrelevant`.

**Tone:**
Professional, sharp, critical, and forward-looking. No filler words."""

            user_prompt = f"""**Task:**
Analyze the provided search result text related to the user's topic: '{topic}'.

**Content to Analyze:**
{content[:3000]}

**Output Format (If relevant):**
Write a concise **Investment Memo** (in Chinese) containing:
- **ğŸ’¡ æ ¸å¿ƒæƒ…æŠ¥ (Core Intelligence):** The key fact/news.
- **ğŸ“‰ èµ›é“å½±å“ (Market Impact):** How this affects the AI ecosystem/startups.
- **âš”ï¸ æœºä¼šä¸é£é™© (Opportunities & Risks):** What should the Director pay attention to?
- **ğŸ“ ä¸€å¥è¯æ€»ç»“ (Key Takeaway):** 20 words max, punchy style.

**Important:** 
- If the content is NOT relevant to investment analysis (e.g., games, tutorials, unrelated content), output ONLY the string: `Irrelevant`
- If relevant, provide the Investment Memo in the format above."""

            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.5,  # ç¨å¾®æé«˜æ¸©åº¦ï¼Œè®©è¾“å‡ºæ›´æœ‰åˆ›é€ æ€§
                max_tokens=500  # å¢åŠ  token é™åˆ¶ï¼Œå› ä¸º Investment Memo æ ¼å¼æ›´è¯¦ç»†
            )
            
            summary = response.choices[0].message.content.strip()
            
            # æ£€æŸ¥æ˜¯å¦è¢«æ ‡è®°ä¸ºä¸ç›¸å…³ï¼ˆæ— æŠ•èµ„ä»·å€¼ï¼‰
            if summary.lower() == 'irrelevant' or len(summary) < 10:
                return None  # è¿”å› None è¡¨ç¤ºå†…å®¹ä¸ç›¸å…³/æ— æŠ•èµ„ä»·å€¼
            
            return summary
        except Exception as e:
            print(f"âš ï¸ LLM æ€»ç»“å¤±è´¥: {e}")
            return None
    
    def add_knowledge(self, title: str, content: str, source: str, summary: str = "", tags: List[str] = None):
        """æ·»åŠ çŸ¥è¯†æ¡ç›®"""
        try:
            entry = {
                'id': f"{title}_{datetime.now().isoformat()}",
                'title': title,
                'content': content,
                'summary': summary,
                'source': source,
                'tags': tags or [],
                'timestamp': datetime.now().isoformat()
            }
            self.knowledge_base.append(entry)
            self._save_knowledge()
            return True
        except Exception as e:
            print(f"âš ï¸ æ·»åŠ çŸ¥è¯†å¤±è´¥: {e}")
            return False
    
    def query_knowledge(self, query: str, top_k: int = 5) -> List[Dict]:
        """æŸ¥è¯¢çŸ¥è¯†åº“ï¼ˆåŸºäºå…³é”®è¯åŒ¹é…ï¼‰"""
        if not self.knowledge_base:
            return []
        
        try:
            query_lower = query.lower()
            scored = []
            for entry in self.knowledge_base:
                score = 0
                # æ ‡é¢˜åŒ¹é…
                if query_lower in entry.get('title', '').lower():
                    score += 3
                # å†…å®¹åŒ¹é…
                if query_lower in entry.get('content', '').lower():
                    score += 2
                # æ ‡ç­¾åŒ¹é…
                for tag in entry.get('tags', []):
                    if query_lower in tag.lower():
                        score += 1
                
                if score > 0:
                    scored.append({
                        'entry': entry,
                        'score': score
                    })
            
            # æŒ‰åˆ†æ•°æ’åº
            scored.sort(key=lambda x: x['score'], reverse=True)
            return [item['entry'] for item in scored[:top_k]]
        except Exception as e:
            print(f"âš ï¸ æŸ¥è¯¢çŸ¥è¯†åº“å¤±è´¥: {e}")
            return []
    
    def get_count(self) -> int:
        """è·å–çŸ¥è¯†åº“æ¡ç›®æ•°é‡"""
        return len(self.knowledge_base)
    
    def clear_all(self) -> bool:
        """æ¸…ç©ºæ‰€æœ‰çŸ¥è¯†åº“æ¡ç›®"""
        try:
            self.knowledge_base = []
            return self._save_knowledge()
        except Exception as e:
            print(f"âš ï¸ æ¸…ç©ºçŸ¥è¯†åº“å¤±è´¥: {e}")
            return False


# å…¨å±€ KnowledgeManager å®ä¾‹
knowledge_manager = KnowledgeManager()


def load_prompt_files() -> Dict[str, str]:
    """æ‰«æ prompts æ–‡ä»¶å¤¹ï¼ŒåŠ è½½æ‰€æœ‰ .txt æ–‡ä»¶"""
    prompt_files = {}
    if not PROMPTS_DIR.exists():
        return prompt_files
    
    txt_files = list(PROMPTS_DIR.glob("*.txt"))
    for file_path in sorted(txt_files):
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read().strip()
                # ä½¿ç”¨æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ä½œä¸ºé”®
                name = file_path.stem
                prompt_files[name] = content
        except Exception as e:
            st.warning(f"âš ï¸ æ— æ³•è¯»å–æ–‡ä»¶ {file_path.name}: {e}")
    
    return prompt_files


def extract_text_from_pdf(file) -> str:
    """ä» PDF æ–‡ä»¶ä¸­æå–æ–‡æœ¬"""
    try:
        pdf_reader = PyPDF2.PdfReader(file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return text.strip()
    except Exception as e:
        st.error(f"âŒ PDF è§£æé”™è¯¯: {e}")
        return ""


def extract_text_from_docx(file) -> str:
    """ä» DOCX æ–‡ä»¶ä¸­æå–æ–‡æœ¬"""
    try:
        doc = Document(file)
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        return text.strip()
    except Exception as e:
        st.error(f"âŒ DOCX è§£æé”™è¯¯: {e}")
        return ""


def extract_text_from_uploaded_file(uploaded_file) -> Optional[str]:
    """æ ¹æ®æ–‡ä»¶ç±»å‹æå–æ–‡æœ¬"""
    file_extension = Path(uploaded_file.name).suffix.lower()
    
    if file_extension == ".pdf":
        return extract_text_from_pdf(uploaded_file)
    elif file_extension in [".docx", ".doc"]:
        return extract_text_from_docx(uploaded_file)
    else:
        st.error(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}")
        return None


def save_history_entry(mode_name: str, file_name: str, analysis_content: str):
    """ä¿å­˜å†å²è®°å½•åˆ° JSON æ–‡ä»¶"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    entry = {
        "mode": mode_name,
        "file_name": file_name,
        "timestamp": timestamp,
        "datetime": datetime.now().isoformat(),
        "content": analysis_content
    }
    
    filename = f"{timestamp}_{mode_name.replace(' ', '_')}.json"
    filepath = HISTORY_DIR / filename
    
    try:
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(entry, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        st.error(f"âŒ ä¿å­˜å†å²è®°å½•å¤±è´¥: {e}")
        return False


def load_history_entries() -> List[Dict]:
    """åŠ è½½æ‰€æœ‰å†å²è®°å½•"""
    entries = []
    if not HISTORY_DIR.exists():
        return entries
    
    json_files = sorted(HISTORY_DIR.glob("*.json"), reverse=True)
    for filepath in json_files:
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                entry = json.load(f)
                entries.append(entry)
        except Exception as e:
            st.warning(f"âš ï¸ æ— æ³•è¯»å–å†å²è®°å½• {filepath.name}: {e}")
    
    return entries


def clean_markdown_for_display(markdown_text: str) -> str:
    """æ¸…ç† Markdown æ–‡æœ¬ï¼Œç§»é™¤ Tags å¤´éƒ¨ã€JSON ä»£ç å—å’Œè¿›åŒ–å»ºè®®ï¼ˆç”¨äºæ˜¾ç¤ºå’Œå¯¼å‡ºï¼‰"""
    cleaned = markdown_text
    # ç§»é™¤ Tags å¤´éƒ¨
    cleaned = re.sub(r'---TAGS:\s*\[[^\]]+\]---\s*\n?', '', cleaned)
    # ç§»é™¤ ```json ... ``` ä»£ç å—
    pattern = r'```json\s*[\s\S]*?```'
    cleaned = re.sub(pattern, '', cleaned, flags=re.DOTALL)
    # ç§»é™¤è¿›åŒ–å»ºè®®ç« èŠ‚
    cleaned = re.sub(r'##\s*ğŸ§¬\s*è¿›åŒ–å»ºè®®\s*\n.*?(?=\n```json|\n```|$)', '', cleaned, flags=re.DOTALL)
    # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
    cleaned = re.sub(r'\n{3,}', '\n\n', cleaned)
    return cleaned.strip()


def markdown_to_docx(markdown_text: str, output_path: str):
    """å°† Markdown æ–‡æœ¬è½¬æ¢ä¸º Word æ–‡æ¡£"""
    # æ¸…ç† JSON ä»£ç å—
    markdown_text = clean_markdown_for_display(markdown_text)
    
    doc = Document()
    
    lines = markdown_text.split('\n')
    
    for line in lines:
        line = line.strip()
        
        if not line:
            # ç©ºè¡Œ
            doc.add_paragraph()
            continue
        
        # æ£€æŸ¥æ ‡é¢˜å±‚çº§
        if line.startswith('# '):
            # H1 æ ‡é¢˜
            heading = doc.add_heading(line[2:].strip(), level=1)
        elif line.startswith('## '):
            # H2 æ ‡é¢˜
            heading = doc.add_heading(line[3:].strip(), level=2)
        elif line.startswith('### '):
            # H3 æ ‡é¢˜
            heading = doc.add_heading(line[4:].strip(), level=3)
        elif line.startswith('#### '):
            # H4 æ ‡é¢˜
            heading = doc.add_heading(line[5:].strip(), level=4)
        elif line.startswith('- ') or line.startswith('* '):
            # åˆ—è¡¨é¡¹
            para = doc.add_paragraph(line[2:].strip(), style='List Bullet')
        elif line.startswith(('  - ', '  * ', '    - ', '    * ')):
            # åµŒå¥—åˆ—è¡¨é¡¹
            indent_level = len(line) - len(line.lstrip())
            para = doc.add_paragraph(line.lstrip()[2:].strip(), style='List Bullet')
            para.paragraph_format.left_indent = Pt(indent_level * 12)
        elif re.match(r'^\d+\.\s+', line):
            # æœ‰åºåˆ—è¡¨
            para = doc.add_paragraph(line, style='List Number')
        elif line.startswith('**') and line.endswith('**'):
            # ç²—ä½“æ–‡æœ¬
            para = doc.add_paragraph()
            para.add_run(line[2:-2]).bold = True
        elif line.startswith('*') and line.endswith('*') and not line.startswith('**'):
            # æ–œä½“æ–‡æœ¬
            para = doc.add_paragraph()
            para.add_run(line[1:-1]).italic = True
        else:
            # æ™®é€šæ®µè½ - å¤„ç†è¡Œå†…æ ¼å¼ï¼ˆç²—ä½“ã€æ–œä½“ç­‰ï¼‰
            para = doc.add_paragraph()
            # å¤„ç†ç²—ä½“ **text**
            if '**' in line:
                parts = re.split(r'(\*\*[^\*]+?\*\*)', line)
                for part in parts:
                    if part.startswith('**') and part.endswith('**'):
                        para.add_run(part[2:-2]).bold = True
                    elif part.strip():
                        # å¤„ç†æ–œä½“ *text*
                        if '*' in part and not part.startswith('*'):
                            italic_parts = re.split(r'(\*[^\*]+\*)', part)
                            for italic_part in italic_parts:
                                if italic_part.startswith('*') and italic_part.endswith('*') and len(italic_part) > 2:
                                    para.add_run(italic_part[1:-1]).italic = True
                                elif italic_part.strip():
                                    para.add_run(italic_part)
                        else:
                            para.add_run(part)
            else:
                # æ²¡æœ‰ç²—ä½“æ ‡è®°ï¼Œç›´æ¥æ·»åŠ æ–‡æœ¬
                para.add_run(line)
    
    try:
        doc.save(output_path)
        return True
    except Exception as e:
        st.error(f"âŒ ä¿å­˜ Word æ–‡æ¡£å¤±è´¥: {e}")
        return False


def enhance_system_prompt(base_prompt: str, similar_projects: List[Dict] = None) -> str:
    """å¢å¼ºç³»ç»Ÿæç¤ºè¯ï¼Œè¦æ±‚ LLM ä¸¥æ ¼æŒ‰ç…§ç‰¹å®šæ ¼å¼è¾“å‡º"""
    
    # è¾“å‡ºæ ¼å¼è¦æ±‚
    format_instruction = """

---
ã€ä¸¥æ ¼è¾“å‡ºæ ¼å¼è¦æ±‚ã€‘

ä½ çš„è¾“å‡ºå¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ä¸‰éƒ¨åˆ†ç»“æ„ï¼š

**ç¬¬ä¸€éƒ¨åˆ† - æ ‡ç­¾å¤´éƒ¨ï¼ˆå¿…é¡»ï¼‰**ï¼š
åœ¨å†…å®¹æœ€å¼€å§‹ï¼Œè¾“å‡ºä¸€è¡Œï¼š
---TAGS: ["æ ‡ç­¾1", "æ ‡ç­¾2", "æ ‡ç­¾3", "æ ‡ç­¾4", "æ ‡ç­¾5"]---

**ç¬¬äºŒéƒ¨åˆ† - åˆ†ææ­£æ–‡**ï¼š
æ­£å¸¸çš„ Markdown æ ¼å¼åˆ†ææŠ¥å‘Šï¼ŒåŒ…å«æ‰€æœ‰åˆ†æå†…å®¹ã€‚

**ç¬¬ä¸‰éƒ¨åˆ† - è¿›åŒ–å»ºè®®ï¼ˆå¿…é¡»ï¼‰**ï¼š
åœ¨æŠ¥å‘Šæœ€åï¼Œå¿…é¡»åŒ…å«ä¸€ä¸ªç« èŠ‚ï¼š
## ğŸ§¬ è¿›åŒ–å»ºè®®
[é’ˆå¯¹æœ¬æ¬¡åˆ†æï¼Œåæ€å½“å‰ System Prompt çš„ä¸è¶³ï¼Œå¹¶ç»™å‡ºå…·ä½“çš„ä¼˜åŒ–æŒ‡ä»¤å»ºè®®ã€‚å»ºè®®åº”è¯¥å…·ä½“ã€å¯æ“ä½œï¼Œä¾‹å¦‚ï¼š"å»ºè®®åœ¨æç¤ºè¯ä¸­å¢åŠ å¯¹XXèµ›é“çš„ç‰¹æ®Šå…³æ³¨"æˆ–"å»ºè®®æ˜ç¡®è¦æ±‚åˆ†æXXç»´åº¦çš„é£é™©"ç­‰]

**ç¬¬å››éƒ¨åˆ† - JSON æ•°æ®å—ï¼ˆå¿…é¡»ï¼‰**ï¼š
åœ¨è¿›åŒ–å»ºè®®ä¹‹åï¼Œé™„å¸¦ä¸€ä¸ª JSON æ•°æ®å—ï¼š

```json
{
  "project_name": "é¡¹ç›®åç§°",
  "industry": "æ‰€å±èµ›é“/è¡Œä¸šï¼ˆå¦‚ï¼šAI Agent, å…·èº«æ™ºèƒ½ï¼‰",
  "tags": ["æŠ€æœ¯æ ‡ç­¾1", "æŠ€æœ¯æ ‡ç­¾2", "æŠ€æœ¯æ ‡ç­¾3"],
  "stage": "èèµ„é˜¶æ®µï¼ˆå¦‚ï¼šAngel, Pre-A, Aè½®, Bè½®ç­‰ï¼‰",
  "score": 8,
  "summary": "ä¸€å¥è¯æ ¸å¿ƒè¯„ä»·ï¼ˆ50å­—ä»¥å†…ï¼‰",
  "risk_level": "High/Medium/Low"
}
```

æ³¨æ„ï¼š
- project_name: ä»å•†ä¸šè®¡åˆ’ä¹¦ä¸­æå–çš„é¡¹ç›®åç§°ï¼ˆå­—ç¬¦ä¸²ï¼‰
- industry: æ‰€å±èµ›é“/è¡Œä¸šï¼ˆå­—ç¬¦ä¸²ï¼Œå¦‚ï¼šAI Agent, å…·èº«æ™ºèƒ½, SaaS, åŒºå—é“¾ç­‰ï¼‰
- tags: æŠ€æœ¯æ ‡ç­¾åˆ—è¡¨ï¼ˆæ•°ç»„ï¼Œå¦‚ï¼š["RAG", "LLM", "SaaS", "å¤šæ¨¡æ€"]ï¼‰ï¼Œå¿…é¡»ä¸ç¬¬ä¸€éƒ¨åˆ†çš„ TAGS ä¿æŒä¸€è‡´
- stage: èèµ„é˜¶æ®µï¼ˆå­—ç¬¦ä¸²ï¼Œå¦‚ï¼šAngel, Pre-A, Aè½®, Bè½®ç­‰ï¼Œå¦‚æœæœªæåŠåˆ™å¡«å†™ "æœªæŠ«éœ²"ï¼‰
- score: æŠ•èµ„æ¨èè¯„åˆ†ï¼ˆæ•´æ•°ï¼Œ1-10åˆ†ï¼Œ10åˆ†ä¸ºæœ€é«˜ï¼‰ï¼Œå¿…é¡»åœ¨æ­£æ–‡ä¸­æ˜ç¡®æ˜¾ç¤º
- summary: ä¸€å¥è¯æ ¸å¿ƒè¯„ä»·ï¼ˆå­—ç¬¦ä¸²ï¼Œ50å­—ä»¥å†…ï¼Œç®€æ´æ¦‚æ‹¬é¡¹ç›®ä»·å€¼å’Œé£é™©ï¼‰
- risk_level: é£é™©ç­‰çº§ï¼ˆå­—ç¬¦ä¸²ï¼Œå¿…é¡»æ˜¯ "High", "Medium", "Low" ä¹‹ä¸€ï¼‰

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸Šæ ¼å¼è¾“å‡ºï¼Œä¸è¦é—æ¼ä»»ä½•éƒ¨åˆ†ã€‚
"""
    
    # å¦‚æœæœ‰ç›¸ä¼¼é¡¹ç›®ï¼Œæ·»åŠ å†å²é¡¹ç›®å‚è€ƒä¿¡æ¯
    if similar_projects and len(similar_projects) > 0:
        similar_info = "\n\n---\nã€å†å²é¡¹ç›®å‚è€ƒã€‘çŸ¥è¯†åº“ä¸­å‘ç°äº†ä¸æœ¬é¡¹ç›®ç›¸ä¼¼çš„è¿‡å¾€é¡¹ç›®ï¼ˆåŸºäºæ ‡ç­¾åŒ¹é…ï¼‰ï¼š\n"
        for proj in similar_projects:
            common_tags_str = ', '.join(proj.get('common_tags', [])) if proj.get('common_tags') else 'æ— '
            match_score = proj.get('match_score', 0)
            similar_info += f"- {proj['name']} (è¯„åˆ†: {proj['score']}, æ ¸å¿ƒè¯„ä»·: {proj['summary']}, å…±åŒæ ‡ç­¾: {common_tags_str}, åŒ¹é…åº¦: {match_score:.2%})\n"
        similar_info += "\nè¯·åœ¨åˆ†ææ—¶æ¨ªå‘å¯¹æ¯”ï¼ŒæŒ‡å‡ºæœ¬é¡¹ç›®çš„å·®å¼‚åŒ–ä¼˜åŠ¿æˆ–é‡å¤é€ è½®å­çš„é£é™©ã€‚å‚è€ƒè¿™äº›å†å²æ¡ˆä¾‹ï¼Œä½†ä¸è¦è¢«å…¶å±€é™ï¼Œé‡ç‚¹åˆ†æå½“å‰é¡¹ç›®çš„ç‹¬ç‰¹ä»·å€¼ã€‚\n"
        return base_prompt + similar_info + format_instruction
    
    return base_prompt + format_instruction


def extract_tags_from_response(response_text: str) -> List[str]:
    """ä» LLM å“åº”ä¸­æå– Tagsï¼ˆä»å¤´éƒ¨ ---TAGS: ... --- æ ¼å¼ï¼‰"""
    try:
        # åŒ¹é… ---TAGS: ["æ ‡ç­¾1", "æ ‡ç­¾2"]--- æ ¼å¼
        pattern = r'---TAGS:\s*(\[[^\]]+\])---'
        match = re.search(pattern, response_text)
        
        if match:
            tags_str = match.group(1)
            # è§£æ JSON æ•°ç»„
            tags = json.loads(tags_str)
            if isinstance(tags, list):
                return [str(tag).strip() for tag in tags if tag]
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä» JSON æ•°æ®å—ä¸­æå–
        json_data = extract_json_from_response(response_text)
        if json_data and 'tags' in json_data:
            tags = json_data.get('tags', [])
            if isinstance(tags, list):
                return [str(tag).strip() for tag in tags if tag]
        
        return []
    except Exception as e:
        print(f"âš ï¸ æå– Tags å¤±è´¥: {e}")
        return []


def extract_evolution_suggestion(response_text: str) -> Optional[str]:
    """ä» LLM å“åº”ä¸­æå–è¿›åŒ–å»ºè®®ï¼ˆ## ğŸ§¬ è¿›åŒ–å»ºè®® åçš„å†…å®¹ï¼‰"""
    try:
        # åŒ¹é… ## ğŸ§¬ è¿›åŒ–å»ºè®® åçš„å†…å®¹ï¼ˆç›´åˆ° JSON ä»£ç å—æˆ–æ–‡ä»¶æœ«å°¾ï¼‰
        pattern = r'##\s*ğŸ§¬\s*è¿›åŒ–å»ºè®®\s*\n(.*?)(?=\n```json|\n```|$)'
        match = re.search(pattern, response_text, re.DOTALL)
        
        if match:
            suggestion = match.group(1).strip()
            # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
            suggestion = re.sub(r'\n{3,}', '\n\n', suggestion)
            return suggestion if suggestion else None
        
        return None
    except Exception as e:
        print(f"âš ï¸ æå–è¿›åŒ–å»ºè®®å¤±è´¥: {e}")
        return None


def extract_score_enhanced(response_text: str, json_data: Optional[Dict] = None) -> Optional[Union[int, float, str]]:
    """å¢å¼ºçš„åˆ†æ•°æå–å‡½æ•°ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
    # é¦–å…ˆå°è¯•ä» JSON æ•°æ®ä¸­æå–
    if json_data and 'score' in json_data:
        score = json_data.get('score')
        if score and score != 'N/A':
            try:
                if isinstance(score, (int, float)):
                    return score
                score_str = str(score).strip()
                # å°è¯•è½¬æ¢ä¸ºæ•°å­—
                if score_str.replace('.', '').isdigit():
                    return float(score_str) if '.' in score_str else int(score_str)
            except:
                pass
    
    # åœ¨å‰ 1000 å­—ç¬¦ä¸­æœç´¢åˆ†æ•°
    search_text = response_text[:1000]
    
    # æ¨¡å¼1: Score: 8 æˆ– è¯„åˆ†ï¼š8.5
    patterns = [
        r'(?:Score|è¯„åˆ†|åˆ†æ•°|æŠ•èµ„è¯„åˆ†)[:ï¼š]\s*(\d+(?:\.\d+)?)',
        r'\[(\d+(?:\.\d+)?)åˆ†\]',
        r'è¯„åˆ†[ï¼š:]\s*(\d+(?:\.\d+)?)',
        r'(\d+(?:\.\d+)?)\s*åˆ†',
        r'(\d+(?:\.\d+)?)\s*/\s*10',  # x/10 æ ¼å¼
    ]
    
    for pattern in patterns:
        match = re.search(pattern, search_text, re.IGNORECASE)
        if match:
            try:
                score_val = float(match.group(1))
                if 1 <= score_val <= 10:
                    return int(score_val) if score_val.is_integer() else score_val
            except:
                continue
    
    # åœ¨æ•´ä¸ªæ–‡æœ¬ä¸­æœç´¢ x/10 æ ¼å¼ï¼ˆå…œåº•ç­–ç•¥ï¼‰
    x_10_pattern = r'(\d+(?:\.\d+)?)\s*/\s*10'
    matches = re.findall(x_10_pattern, response_text)
    if matches:
        for match in matches:
            try:
                score_val = float(match)
                if 1 <= score_val <= 10:
                    return int(score_val) if score_val.is_integer() else score_val
            except:
                continue
    
    return None


def parse_llm_response(response_text: str) -> Dict:
    """ç»Ÿä¸€è§£æ LLM å“åº”ï¼Œæå–æ‰€æœ‰ç»“æ„åŒ–ä¿¡æ¯"""
    result = {
        'tags': [],
        'json_data': None,
        'score': None,
        'evolution_suggestion': None,
        'body_content': response_text  # åŸå§‹å†…å®¹
    }
    
    # æå– Tags
    result['tags'] = extract_tags_from_response(response_text)
    
    # æå– JSON æ•°æ®
    result['json_data'] = extract_json_from_response(response_text)
    
    # æå–åˆ†æ•°ï¼ˆå¢å¼ºç‰ˆï¼‰
    result['score'] = extract_score_enhanced(response_text, result['json_data'])
    
    # æå–è¿›åŒ–å»ºè®®
    result['evolution_suggestion'] = extract_evolution_suggestion(response_text)
    
    # æå–æ­£æ–‡å†…å®¹ï¼ˆç§»é™¤ Tags å¤´éƒ¨ã€è¿›åŒ–å»ºè®®å’Œ JSONï¼‰
    body = response_text
    # ç§»é™¤ Tags å¤´éƒ¨
    body = re.sub(r'---TAGS:\s*\[[^\]]+\]---\s*\n?', '', body)
    # ç§»é™¤è¿›åŒ–å»ºè®®ç« èŠ‚
    body = re.sub(r'##\s*ğŸ§¬\s*è¿›åŒ–å»ºè®®\s*\n.*?(?=\n```json|\n```|$)', '', body, flags=re.DOTALL)
    # ç§»é™¤ JSON ä»£ç å—
    body = re.sub(r'```json\s*[\s\S]*?```', '', body)
    result['body_content'] = body.strip()
    
    return result


def save_evolution_suggestion(project_name: str, suggestion: str):
    """ä¿å­˜è¿›åŒ–å»ºè®®åˆ°æ—¥å¿—æ–‡ä»¶"""
    try:
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"\n\n---\n## {timestamp} - {project_name}\n\n{suggestion}\n"
        
        with open(EVOLUTION_LOG_FILE, 'a', encoding='utf-8') as f:
            f.write(log_entry)
        return True
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜è¿›åŒ–å»ºè®®å¤±è´¥: {e}")
        return False


def extract_json_from_response(response_text: str) -> Optional[Dict]:
    """ä» AI å“åº”ä¸­æå– JSON æ•°æ®å—"""
    try:
        # é¦–å…ˆå°è¯•åŒ¹é… ```json ... ``` ä»£ç å—ï¼ˆæ”¯æŒå¤šè¡Œï¼‰
        pattern = r'```json\s*([\s\S]*?)\s*```'
        matches = re.findall(pattern, response_text)
        
        if matches:
            # å–æœ€åä¸€ä¸ªåŒ¹é…ï¼ˆé€šå¸¸ JSON åœ¨æœ«å°¾ï¼‰
            json_str = matches[-1].strip()
            try:
                # è§£æ JSON
                json_data = json.loads(json_str)
                # éªŒè¯æ˜¯å¦åŒ…å«å¿…éœ€å­—æ®µ
                required_fields = ['project_name', 'industry', 'tags', 'stage', 'score', 'summary', 'risk_level']
                if all(field in json_data for field in required_fields):
                    return json_data
            except json.JSONDecodeError:
                # JSON è§£æå¤±è´¥ï¼Œå°è¯•æ¸…ç†åå†è§£æ
                # ç§»é™¤å¯èƒ½çš„æ³¨é‡Šæˆ–é¢å¤–å­—ç¬¦
                json_str = re.sub(r'//.*', '', json_str)  # ç§»é™¤å•è¡Œæ³¨é‡Š
                json_str = re.sub(r'/\*.*?\*/', '', json_str, flags=re.DOTALL)  # ç§»é™¤å¤šè¡Œæ³¨é‡Š
                try:
                    json_data = json.loads(json_str)
                    required_fields = ['project_name', 'industry', 'tags', 'stage', 'score', 'summary', 'risk_level']
                    if all(field in json_data for field in required_fields):
                        return json_data
                except:
                    pass
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»£ç å—ï¼Œå°è¯•ç›´æ¥æŸ¥æ‰¾ JSON å¯¹è±¡ï¼ˆå…œåº•æ–¹æ¡ˆï¼‰
        # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å®Œæ•´çš„ JSON å¯¹è±¡
        json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
        json_matches = re.findall(json_pattern, response_text, re.DOTALL)
        
        if json_matches:
            # ä»åå¾€å‰å°è¯•è§£æï¼ˆé€šå¸¸ JSON åœ¨æœ«å°¾ï¼‰
            for json_str in reversed(json_matches):
                try:
                    json_data = json.loads(json_str)
                    # éªŒè¯æ˜¯å¦åŒ…å«å¿…éœ€å­—æ®µ
                    required_fields = ['project_name', 'industry', 'tags', 'stage', 'score', 'summary', 'risk_level']
                    if all(field in json_data for field in required_fields):
                        return json_data
                except:
                    continue
        
        return None
    except Exception as e:
        # é™é»˜å¤±è´¥ï¼Œä¸ä¸­æ–­ä¸»æµç¨‹
        print(f"âš ï¸ æå– JSON å¤±è´¥: {e}")
        return None


def save_to_knowledge_base(json_data: Dict):
    """å°†æå–çš„ JSON æ•°æ®ä¿å­˜åˆ°é¡¹ç›®çŸ¥è¯†åº“ CSV æ–‡ä»¶"""
    try:
        # æ·»åŠ æ—¶é—´æˆ³
        json_data['timestamp'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # å°† tags åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼ˆCSV æ ¼å¼ï¼Œç”¨åˆ†å·åˆ†éš”ï¼‰
        if isinstance(json_data.get('tags'), list):
            json_data['tags'] = ';'.join(json_data['tags'])
        
        # å®šä¹‰ CSV åˆ—é¡ºåº
        columns = ['timestamp', 'project_name', 'industry', 'tags', 'stage', 'score', 'summary', 'risk_level']
        
        # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨
        for col in columns:
            if col not in json_data:
                json_data[col] = ''
        
        # åˆ›å»º DataFrame
        df_new = pd.DataFrame([json_data])
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if KNOWLEDGE_BASE_FILE.exists():
            # è¯»å–ç°æœ‰æ•°æ®
            df_existing = pd.read_csv(KNOWLEDGE_BASE_FILE, encoding='utf-8-sig')
            # åˆå¹¶æ•°æ®
            df_combined = pd.concat([df_existing, df_new], ignore_index=True)
        else:
            # åˆ›å»ºæ–°æ–‡ä»¶
            df_combined = df_new
        
        # æŒ‰ç…§åˆ—é¡ºåºé‡æ–°æ’åˆ—
        df_combined = df_combined[columns]
        
        # ä¿å­˜åˆ° CSVï¼ˆä½¿ç”¨ utf-8-sig ç¼–ç ä»¥æ”¯æŒä¸­æ–‡ï¼Œé¿å… Windows Excel ä¹±ç ï¼‰
        df_combined.to_csv(KNOWLEDGE_BASE_FILE, index=False, encoding='utf-8-sig')
        return True
    except Exception as e:
        # é™é»˜å¤±è´¥ï¼Œä¸ä¸­æ–­ä¸»æµç¨‹
        print(f"âš ï¸ ä¿å­˜çŸ¥è¯†åº“å¤±è´¥: {e}")
        return False


def load_knowledge_base() -> pd.DataFrame:
    """åŠ è½½é¡¹ç›®çŸ¥è¯†åº“"""
    try:
        if KNOWLEDGE_BASE_FILE.exists():
            df = pd.read_csv(KNOWLEDGE_BASE_FILE, encoding='utf-8-sig')
            # ç¡®ä¿ timestamp åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹ï¼Œä¾¿äºæ ¼å¼åŒ–
            if 'timestamp' in df.columns:
                df['timestamp'] = df['timestamp'].astype(str)
            return df
        else:
            return pd.DataFrame()
    except Exception as e:
        print(f"âš ï¸ åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {e}")
        return pd.DataFrame()


def format_timestamp_for_display(timestamp_str: str) -> str:
    """å°†æ—¶é—´æˆ³æ ¼å¼åŒ–ä¸º YYYY-MM-DD æ ¼å¼"""
    try:
        # å°è¯•è§£æå¤šç§æ—¶é—´æ ¼å¼
        if isinstance(timestamp_str, str):
            # å¤„ç† "YYYY-MM-DD HH:MM:SS" æ ¼å¼
            if ' ' in timestamp_str:
                date_part = timestamp_str.split(' ')[0]
                return date_part
            # å¤„ç† ISO æ ¼å¼
            elif 'T' in timestamp_str:
                date_part = timestamp_str.split('T')[0]
                return date_part
            # å¦‚æœå·²ç»æ˜¯ YYYY-MM-DD æ ¼å¼
            elif len(timestamp_str) >= 10:
                return timestamp_str[:10]
        return timestamp_str
    except:
        return timestamp_str


def get_recent_projects(limit: int = 5) -> List[Dict]:
    """è·å–æœ€è¿‘çš„é¡¹ç›®è®°å½•"""
    try:
        df = load_knowledge_base()
        if df.empty:
            return []
        
        # æŒ‰æ—¶é—´å€’åºæ’åˆ—ï¼Œå–æœ€æ–° N æ¡
        if 'timestamp' in df.columns and 'project_name' in df.columns:
            df_sorted = df.sort_values('timestamp', ascending=False)
            recent = df_sorted.head(limit)
            
            result = []
            for _, row in recent.iterrows():
                date_str = format_timestamp_for_display(str(row.get('timestamp', '')))
                project_name = str(row.get('project_name', 'æœªçŸ¥é¡¹ç›®'))
                result.append({
                    'date': date_str,
                    'name': project_name
                })
            return result
        return []
    except Exception as e:
        print(f"âš ï¸ è·å–æœ€è¿‘é¡¹ç›®å¤±è´¥: {e}")
        return []


def calculate_kb_statistics(df: pd.DataFrame) -> Dict:
    """è®¡ç®—çŸ¥è¯†åº“ç»Ÿè®¡æŒ‡æ ‡"""
    stats = {
        'total_projects': 0,
        'avg_score': 0.0,
        'top_industry': 'N/A',
        'industry_count': {}
    }
    
    try:
        if df.empty:
            return stats
        
        stats['total_projects'] = len(df)
        
        # è®¡ç®—å¹³å‡è¯„åˆ†
        if 'score' in df.columns:
            try:
                scores = pd.to_numeric(df['score'], errors='coerce').dropna()
                if len(scores) > 0:
                    stats['avg_score'] = round(scores.mean(), 1)
            except:
                pass
        
        # è®¡ç®—æœ€çƒ­èµ›é“
        if 'industry' in df.columns:
            industry_counts = df['industry'].value_counts()
            if len(industry_counts) > 0:
                stats['top_industry'] = industry_counts.index[0]
                stats['industry_count'] = industry_counts.to_dict()
        
        return stats
    except Exception as e:
        print(f"âš ï¸ è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡å¤±è´¥: {e}")
        return stats


def generate_file_id(file_name: str) -> str:
    """ä¸ºæ–‡ä»¶ç”Ÿæˆå”¯ä¸€ ID"""
    # ä½¿ç”¨æ–‡ä»¶åå’Œå½“å‰æ—¶é—´æˆ³ç”Ÿæˆå”¯ä¸€ ID
    content = f"{file_name}_{time.time()}"
    return hashlib.md5(content.encode()).hexdigest()[:12]


def process_single_file(
    uploaded_file,
    file_id: str,
    system_prompt: str,
    api_key: str,
    model: str,
    selected_mode: str,
    row_container,
    similar_projects: List[Dict] = None
) -> Optional[Dict]:
    """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼Œè¿”å›ç»“æœå­—å…¸"""
    try:
        # æå–æ–‡ä»¶æ–‡æœ¬
        file_text = extract_text_from_uploaded_file(uploaded_file)
        if not file_text:
            with row_container.container():
                st.error(f"âŒ æ— æ³•ä»æ–‡ä»¶ {uploaded_file.name} ä¸­æå–æ–‡æœ¬")
            return None
        
        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨ä¾§è¾¹æ é…ç½®çš„å€¼ï¼‰
        # æ³¨æ„ï¼šbase_url å’Œ api_key åº”è¯¥ä»ä¾§è¾¹æ çš„è¾“å…¥æ¡†è·å–
        # è¿™é‡Œéœ€è¦ä»è°ƒç”¨å‡½æ•°çš„å‚æ•°ä¸­è·å–ï¼Œæˆ–ä» session_state è¯»å–
        client = OpenAI(
            base_url=BASE_URL,  # Base URL æ˜¯å›ºå®šçš„ï¼Œä½¿ç”¨å¸¸é‡
            api_key=api_key
        )
        
        # system_prompt å·²ç»åœ¨å¤–éƒ¨å¢å¼ºï¼ˆåŒ…å« RAG ä¿¡æ¯ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
        enhanced_prompt = system_prompt
        
        # è°ƒç”¨ APIï¼ˆæµå¼ï¼‰
        stream = call_openrouter_api(client, enhanced_prompt, file_text, model)
        if not stream:
            with row_container.container():
                st.error(f"âŒ API è°ƒç”¨å¤±è´¥: {uploaded_file.name}")
            return None
        
        # æ”¶é›†å®Œæ•´å“åº”
        full_response = ""
        
        for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                full_response += chunk.choices[0].delta.content
        
        # è§£æ LLM å“åº”ï¼ˆæå– Tagsã€JSONã€åˆ†æ•°ã€è¿›åŒ–å»ºè®®ç­‰ï¼‰
        parsed = parse_llm_response(full_response)
        json_data = parsed.get('json_data')
        extracted_tags = parsed.get('tags', [])
        extracted_score = parsed.get('score')
        evolution_suggestion = parsed.get('evolution_suggestion')
        body_content = parsed.get('body_content', full_response)
        
        # å¦‚æœ JSON ä¸­æœ‰ tagsï¼Œä¼˜å…ˆä½¿ç”¨ JSON ä¸­çš„ï¼ˆæ›´å‡†ç¡®ï¼‰
        if json_data and json_data.get('tags'):
            extracted_tags = json_data.get('tags', [])
        
        # å¦‚æœ JSON ä¸­æœ‰ scoreï¼Œä¼˜å…ˆä½¿ç”¨ JSON ä¸­çš„
        if json_data and json_data.get('score') and json_data.get('score') != 'N/A':
            extracted_score = json_data.get('score')
        
        # ä¿å­˜åˆ° CSV çŸ¥è¯†åº“
        if json_data:
            save_to_knowledge_base(json_data)
        
        # ä¿å­˜å†å²è®°å½•ï¼ˆä¿å­˜å®Œæ•´å†…å®¹ï¼‰
        save_history_entry(selected_mode, uploaded_file.name, full_response)
        
        # ä¿å­˜è¿›åŒ–å»ºè®®
        if evolution_suggestion:
            project_name = json_data.get('project_name', uploaded_file.name) if json_data else uploaded_file.name
            save_evolution_suggestion(project_name, evolution_suggestion)
        
        # ç”Ÿæˆ Word æ–‡æ¡£ï¼ˆå†…å­˜ä¸­ï¼Œä½¿ç”¨æ¸…ç†åçš„æ­£æ–‡å†…å®¹ï¼‰
        word_buffer = io.BytesIO()
        temp_path = HISTORY_DIR / f"temp_{file_id}.docx"
        # Word å¯¼å‡ºæ—¶ä½¿ç”¨æ¸…ç†åçš„æ­£æ–‡ï¼ˆä¸åŒ…å« Tags å¤´éƒ¨å’Œè¿›åŒ–å»ºè®®ï¼‰
        markdown_to_docx(body_content, str(temp_path))
        
        with open(temp_path, "rb") as f:
            word_buffer.write(f.read())
        word_buffer.seek(0)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            temp_path.unlink()
        except:
            pass
        
        # è¿”å›ç»“æœå­—å…¸
        project_name = json_data.get('project_name', uploaded_file.name) if json_data else uploaded_file.name
        final_score = extracted_score if extracted_score is not None else (json_data.get('score', 'N/A') if json_data else 'N/A')
        
        return {
            'file_id': file_id,
            'file_name': uploaded_file.name,
            'markdown_content': body_content,  # ä½¿ç”¨æ¸…ç†åçš„æ­£æ–‡
            'full_response': full_response,  # ä¿ç•™å®Œæ•´å“åº”ç”¨äºå±•ç¤º
            'word_buffer': word_buffer,
            'json_data': json_data,
            'project_name': project_name,
            'score': final_score,
            'tags': extracted_tags,
            'evolution_suggestion': evolution_suggestion,
            'similar_projects': similar_projects if similar_projects else [],
            'status': 'completed',
            'timestamp': datetime.now().isoformat()
        }
    
    except Exception as e:
        row_container.error(f"âŒ å¤„ç†æ–‡ä»¶ {uploaded_file.name} æ—¶å‡ºé”™: {str(e)}")
        return None


def render_result_row(result: Dict, index: int) -> bool:
    """æ¸²æŸ“å•ä¸ªç»“æœè¡Œï¼Œè¿”å›æ˜¯å¦è¢«é€‰ä¸­"""
    col1, col2 = st.columns([0.05, 0.95])
    
    with col1:
        checkbox_key = f"select_{result['file_id']}"
        is_selected = st.checkbox(
            "",
            value=True,
            key=checkbox_key,
            label_visibility="collapsed"
        )
    
    with col2:
        # æ„å»º expander æ ‡é¢˜
        score_display = f"{result['score']}åˆ†" if isinstance(result['score'], (int, float)) else str(result['score'])
        expander_title = f"âœ… [{score_display}] {result['project_name']} (ç‚¹å‡»å±•å¼€è¯¦æƒ…)"
        
        with st.expander(expander_title, expanded=False):
            # ========== å¤§è„‘æ€è€ƒè·¯å¾„å±•ç¤º ==========
            st.info("ğŸ§  **å¤§è„‘æ€è€ƒè·¯å¾„**")
            
            # æ˜¾ç¤ºæå–åˆ°çš„æ ‡ç­¾
            tags = result.get('tags', [])
            if tags:
                tags_str = ', '.join([f"`{tag}`" for tag in tags])
                st.markdown(f"**æ ¸å¿ƒå…³é”®è¯**: {tags_str}")
            else:
                st.markdown("**æ ¸å¿ƒå…³é”®è¯**: æœªæå–åˆ°æ ‡ç­¾")
            
            # æ˜¾ç¤ºè®°å¿†æ¿€æ´»çŠ¶æ€
            similar_projects = result.get('similar_projects', [])
            if similar_projects and len(similar_projects) > 0:
                project_names = [proj.get('name', 'æœªçŸ¥é¡¹ç›®') for proj in similar_projects]
                common_tags_list = []
                for proj in similar_projects:
                    common_tags = proj.get('common_tags', [])
                    if common_tags:
                        common_tags_list.extend(common_tags)
                
                common_tags_str = ', '.join(set(common_tags_list)) if common_tags_list else 'æ— '
                st.success(f"ğŸ¯ **è®°å¿†æ¿€æ´»**: å‘ç°ä¸å†å²é¡¹ç›® `{', '.join(project_names)}` å­˜åœ¨å…³è”ï¼ˆå…±åŒæ ‡ç­¾: {common_tags_str}ï¼‰ï¼Œå·²è¿›è¡Œæ¨ªå‘å¯¹æ¯”ã€‚")
            else:
                st.info("ğŸŒ± **æ–°ç‰©ç§æ”¶å½•**: çŸ¥è¯†åº“ä¸­æš‚æ— åŒç±»ï¼Œå·²ä½œä¸ºç§å­å­˜å…¥å¤§è„‘ã€‚")
            
            st.divider()
            
            # æ˜¾ç¤ºæ¸…ç†åçš„ Markdown å†…å®¹ï¼ˆæ­£æ–‡ï¼‰
            cleaned_content = clean_markdown_for_display(result['markdown_content'])
            st.markdown(cleaned_content)
            
            # å•ç‹¬ä¸‹è½½æŒ‰é’®
            word_buffer = result.get('word_buffer')
            if word_buffer:
                word_buffer.seek(0)
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½æ­¤æŠ¥å‘Š",
                    data=word_buffer.read(),
                    file_name=f"{result['project_name']}_{datetime.now().strftime('%Y%m%d')}.docx",
                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                    key=f"download_row_{result['file_id']}"
                )
    
    return is_selected


def call_openrouter_api(client: OpenAI, system_prompt: str, user_content: str, model: str):
    """è°ƒç”¨ OpenRouter API è¿›è¡Œæµå¼å“åº”"""
    try:
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_content}
        ]
        
        stream = client.chat.completions.create(
            model=model,
            messages=messages,
            stream=True
        )
        
        return stream
    except Exception as e:
        st.error(f"âŒ API è°ƒç”¨é”™è¯¯: {e}")
        return None


# ==================== ä¾§è¾¹æ  ====================
with st.sidebar:
    st.header("âš™ï¸ æ§åˆ¶å°")
    
    # åˆå§‹åŒ– is_analyzing çŠ¶æ€
    if "is_analyzing" not in st.session_state:
        st.session_state.is_analyzing = False
    
    # é¡µé¢å¯¼èˆªï¼ˆå¦‚æœæ­£åœ¨åˆ†æåˆ™ç¦ç”¨ï¼‰
    page = st.sidebar.radio(
        "é¡µé¢å¯¼èˆª",
        options=["ğŸš€ AI åˆ†æå·¥ä½œå°", "ğŸ§  çŸ¥è¯†å¤§è„‘", "ğŸ“‚ å…¨é‡çŸ¥è¯†åº“", "ğŸ“œ å†å²è®°å½•", "ğŸ“¡ é›·è¾¾å€™é€‰æ± ", "ğŸ“„ å‘¨æŠ¥"],
        index=0,
        label_visibility="visible",
        disabled=st.session_state.is_analyzing
    )
    
    # å¦‚æœæ­£åœ¨åˆ†æï¼Œæ˜¾ç¤ºè­¦å‘Š
    if st.session_state.is_analyzing:
        st.warning("âš ï¸ åˆ†æä»»åŠ¡è¿è¡Œä¸­ï¼Œå¯¼èˆªå·²é”å®š...")
    
    st.divider()
    
    # API é…ç½®ï¼ˆæŒä¹…åŒ–ï¼‰
    st.subheader("API é…ç½®")
    
    # åŠ è½½é…ç½®ï¼ˆä»…åœ¨ç¬¬ä¸€æ¬¡æ—¶ï¼‰
    if "config_loaded" not in st.session_state:
        config = load_config()
        st.session_state["api_key_input"] = config.get("api_key", os.getenv("OPENROUTER_API_KEY", ""))
        st.session_state["base_url_input"] = config.get("base_url", BASE_URL)
        st.session_state["model_input"] = config.get("model", DEFAULT_MODEL)
        st.session_state["http_proxy_input"] = config.get("http_proxy", "http://127.0.0.1:7890")
        st.session_state["config_loaded"] = True
        
        # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œé¦–æ¬¡åŠ è½½æ—¶ä¿å­˜é»˜è®¤é…ç½®
        if not CONFIG_FILE.exists():
            save_config()
    
    # Base URLï¼ˆç¦ç”¨ï¼Œå›ºå®šå€¼ï¼Œä½†ä¿å­˜åœ¨é…ç½®ä¸­ï¼‰
    base_url = st.text_input(
        "Base URL",
        value=st.session_state.get("base_url_input", BASE_URL),
        disabled=True,
        key="base_url_input",
        help="OpenRouter API åŸºç¡€ URLï¼ˆå›ºå®šï¼‰"
    )
    
    # API Keyï¼ˆæ”¯æŒè‡ªåŠ¨ä¿å­˜ï¼‰
    api_key = st.text_input(
        "API Key",
        value=st.session_state.get("api_key_input", ""),
        type="password",
        key="api_key_input",
        on_change=save_config,
        help="ä»é…ç½®æ–‡ä»¶åŠ è½½æˆ–æ‰‹åŠ¨è¾“å…¥ï¼Œä¿®æ”¹åè‡ªåŠ¨ä¿å­˜"
    )
    
    # Modelï¼ˆæ”¯æŒè‡ªåŠ¨ä¿å­˜ï¼‰
    model = st.text_input(
        "Model",
        value=st.session_state.get("model_input", DEFAULT_MODEL),
        key="model_input",
        on_change=save_config,
        help="OpenRouter æ¨¡å‹åç§°ï¼Œä¿®æ”¹åè‡ªåŠ¨ä¿å­˜"
    )
    
    # HTTP Proxyï¼ˆæ”¯æŒè‡ªåŠ¨ä¿å­˜ï¼Œç”¨äºç½‘ç»œæœç´¢ï¼‰
    http_proxy = st.text_input(
        "HTTP Proxy",
        value=st.session_state.get("http_proxy_input", "http://127.0.0.1:7890"),
        key="http_proxy_input",
        on_change=save_config,
        help="HTTP ä»£ç†åœ°å€ï¼ˆç”¨äºç½‘ç»œæœç´¢ï¼Œç•™ç©ºåˆ™ä¸ä½¿ç”¨ä»£ç†ï¼‰ã€‚é»˜è®¤: http://127.0.0.1:7890",
        placeholder="http://127.0.0.1:7890"
    )
    
    # æ˜¾ç¤ºé…ç½®çŠ¶æ€æç¤º
    if CONFIG_FILE.exists():
        st.caption("ğŸ’¾ é…ç½®å·²è‡ªåŠ¨ä¿å­˜")
    
    st.divider()
    
    # åˆ†ææ¨¡å¼é€‰æ‹©ï¼ˆä»…åœ¨å·¥ä½œå°é¡µé¢éœ€è¦ï¼Œä¸”åœ¨æœªåˆ†ææ—¶æ˜¾ç¤ºï¼‰
    if page == "ğŸš€ AI åˆ†æå·¥ä½œå°" and not st.session_state.is_analyzing:
        st.subheader("ğŸ“‹ åˆ†ææ¨¡å¼")
        prompt_files = load_prompt_files()
        
        if not prompt_files:
            st.warning("âš ï¸ è¯·åœ¨ prompts æ–‡ä»¶å¤¹æ”¾å…¥ .txt æç¤ºè¯æ–‡ä»¶")
            selected_mode = None
            system_prompt = None
        else:
            mode_names = list(prompt_files.keys())
            selected_mode = st.selectbox(
                "é€‰æ‹©åˆ†ææ¨¡å¼",
                options=mode_names,
                index=0
            )
            base_prompt = prompt_files.get(selected_mode, "")
            # å­˜å‚¨åŸºç¡€æç¤ºè¯ï¼Œç¨ååœ¨è°ƒç”¨ API æ—¶å¢å¼º
            system_prompt = base_prompt
    else:
        # çŸ¥è¯†åº“é¡µé¢ä¸éœ€è¦è¿™äº›å˜é‡ï¼Œä½†éœ€è¦åˆå§‹åŒ–ä»¥é¿å…é”™è¯¯
        # å¦‚æœæ­£åœ¨åˆ†æï¼Œä» session_state æ¢å¤
        if st.session_state.is_analyzing and "selected_mode" in st.session_state:
            selected_mode = st.session_state.selected_mode
            system_prompt = st.session_state.get("system_prompt", "")
        else:
            selected_mode = None
            system_prompt = None
    
    st.divider()
    
    # è°ƒè¯•æ¨¡å¼å¼€å…³
    debug_mode = st.checkbox("ğŸ”§ è°ƒè¯•æ¨¡å¼", value=st.session_state.get("debug_mode", False), key="debug_mode_checkbox")
    st.session_state["debug_mode"] = debug_mode
    
    # è°ƒè¯•é¢æ¿ï¼ˆä»…åœ¨è°ƒè¯•æ¨¡å¼å¼€å¯æ—¶æ˜¾ç¤ºï¼‰
    if st.session_state.get("debug_mode", False):
        st.divider()
        st.subheader("ğŸ”§ è°ƒè¯•é¢æ¿")
        
        # Supabase è¿æ¥çŠ¶æ€
        st.markdown("**Supabase è¿æ¥çŠ¶æ€**")
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_anon_key = os.getenv("SUPABASE_ANON_KEY")
        
        if not supabase_url:
            st.error("âŒ æœªè®¾ç½® SUPABASE_URL")
        else:
            st.success(f"âœ… SUPABASE_URL: {supabase_url[:30]}..." if len(supabase_url) > 30 else f"âœ… SUPABASE_URL: {supabase_url}")
        
        if not supabase_anon_key:
            st.error("âŒ æœªè®¾ç½® SUPABASE_ANON_KEY")
        else:
            masked_key = supabase_anon_key[:6] + "***" if len(supabase_anon_key) > 6 else "***"
            st.success(f"âœ… SUPABASE_ANON_KEY: {masked_key}")
        
        # æ£€æŸ¥ Supabase å®¢æˆ·ç«¯
        if HAS_SUPABASE:
            try:
                test_client = get_supabase_client(use_service_role=False)
                if test_client:
                    st.success("âœ… Supabase å®¢æˆ·ç«¯å·²åˆ›å»º")
                else:
                    st.error("âŒ Supabase å®¢æˆ·ç«¯åˆ›å»ºå¤±è´¥")
            except Exception as e:
                st.error(f"âŒ Supabase å®¢æˆ·ç«¯é”™è¯¯: {e}")
        else:
            st.error("âŒ Supabase åº“æœªå®‰è£…")
        
        st.divider()
        
        # Deals è¯»å–çŠ¶æ€ï¼ˆä»…åœ¨"é›·è¾¾å€™é€‰æ± "é¡µé¢æ˜¾ç¤ºï¼‰
        if page == "ğŸ“¡ é›·è¾¾å€™é€‰æ± ":
            st.markdown("**Deals è¯»å–çŠ¶æ€**")
            deals_count = st.session_state.get("debug_deals_count")
            filtered_count = st.session_state.get("debug_filtered_deals_count")
            
            if deals_count is not None:
                st.info(f"ğŸ“Š è¯»å–æ¡æ•°: {deals_count}")
                if filtered_count is not None:
                    st.info(f"ğŸ” è¿‡æ»¤åæ¡æ•°: {filtered_count}")
            else:
                st.caption("ç­‰å¾…é¡µé¢åŠ è½½...")
        
        # Weekly Reports è¯»å–çŠ¶æ€ï¼ˆä»…åœ¨"å‘¨æŠ¥"é¡µé¢æ˜¾ç¤ºï¼‰
        if page == "ğŸ“„ å‘¨æŠ¥":
            st.markdown("**Weekly Reports è¯»å–çŠ¶æ€**")
            weekly_report_debug = st.session_state.get("debug_weekly_report")
            
            if weekly_report_debug == "no reports":
                st.warning("âš ï¸ no reports")
            elif isinstance(weekly_report_debug, dict):
                st.success(f"âœ… week_start: {weekly_report_debug.get('week_start', 'N/A')}")
                st.info(f"ğŸ“„ markdown é•¿åº¦: {weekly_report_debug.get('markdown_length', 0)} å­—ç¬¦")
            else:
                st.caption("ç­‰å¾…é¡µé¢åŠ è½½...")
        
        # Deal Actions æœ€è¿‘ä¸€æ¬¡å†™å…¥
        if page == "ğŸ“¡ é›·è¾¾å€™é€‰æ± ":
            st.divider()
            st.markdown("**Deal Actions æœ€è¿‘ä¸€æ¬¡å†™å…¥**")
            last_payload = st.session_state.get("last_action_payload")
            
            if last_payload:
                st.json(last_payload)
            else:
                st.caption("æš‚æ— å†™å…¥è®°å½•")
    
    st.divider()
    
    # AI è¿›åŒ–æ—¥å¿—
    with st.expander("ğŸ§¬ æŸ¥çœ‹ AI è¿›åŒ–æ—¥å¿—", expanded=False):
        try:
            if EVOLUTION_LOG_FILE.exists():
                with open(EVOLUTION_LOG_FILE, 'r', encoding='utf-8') as f:
                    log_content = f.read()
                if log_content.strip():
                    st.markdown(log_content)
                else:
                    st.info("æš‚æ— è¿›åŒ–å»ºè®®ï¼Œåˆ†æé¡¹ç›®åä¼šè‡ªåŠ¨ç”Ÿæˆã€‚")
            else:
                st.info("æš‚æ— è¿›åŒ–å»ºè®®ï¼Œåˆ†æé¡¹ç›®åä¼šè‡ªåŠ¨ç”Ÿæˆã€‚")
        except Exception as e:
            st.warning(f"âš ï¸ è¯»å–è¿›åŒ–æ—¥å¿—å¤±è´¥: {e}")


# ==================== ä¸»ç•Œé¢ ====================
# æ ¹æ®é¡µé¢å¯¼èˆªæ˜¾ç¤ºä¸åŒå†…å®¹
if page == "ğŸš€ AI åˆ†æå·¥ä½œå°":
    # ========== AI åˆ†æå·¥ä½œå°é¡µé¢ ==========
    st.title("ğŸš€ AI åˆ†æå·¥ä½œå°")
    st.caption("é«˜çº§æŠ•èµ„è¾…åŠ©ç³»ç»Ÿ - æ”¯æŒæ‰¹é‡åˆ†æä¸æŠ¥å‘Šå¯¼å‡º")

    # æ˜¾ç¤ºå½“å‰æ¨¡å¼
    if selected_mode:
        st.info(f"ğŸ“‹ **å½“å‰åˆ†ææ¨¡å¼**: {selected_mode}")
    else:
        st.warning("âš ï¸ è¯·å…ˆåœ¨ä¾§è¾¹æ é€‰æ‹©åˆ†ææ¨¡å¼")

    st.divider()

    # åˆå§‹åŒ– session state
    if "processed_results" not in st.session_state:
        st.session_state["processed_results"] = []
    if "task_queue" not in st.session_state:
        st.session_state["task_queue"] = []
    if "processing_status" not in st.session_state:
        st.session_state["processing_status"] = {}

    # åˆå§‹åŒ– MemoryManagerï¼ˆè½»é‡çº§ï¼Œæ— éœ€é¢å¤–ä¾èµ–ï¼‰
    # memory_manager å·²åœ¨æ¨¡å—åŠ è½½æ—¶è‡ªåŠ¨åˆå§‹åŒ–

    # æ‰¹é‡æ–‡ä»¶ä¸Šä¼ ï¼ˆåˆ†æä¸­æ—¶ç¦ç”¨ï¼‰
    uploaded_files = st.file_uploader(
        "ğŸ“„ ä¸Šä¼ åˆ†ææ–‡ä»¶ï¼ˆæ”¯æŒæ‰¹é‡ï¼‰",
        type=["pdf", "docx", "doc"],
        help="æ”¯æŒ PDF å’Œ DOCX æ ¼å¼ï¼Œå¯åŒæ—¶ä¸Šä¼ å¤šä¸ªæ–‡ä»¶è¿›è¡Œæ‰¹é‡åˆ†æ",
        accept_multiple_files=True,
        disabled=st.session_state.is_analyzing
    )

    # å¼€å§‹æ‰¹é‡åˆ†ææŒ‰é’®ï¼ˆä»…åœ¨æœªåˆ†ææ—¶æ˜¾ç¤ºï¼‰
    if uploaded_files and len(uploaded_files) > 0 and not st.session_state.is_analyzing:
        if st.button("ğŸš€ å¼€å§‹æ‰¹é‡åˆ†æ", type="primary", disabled=not (selected_mode and api_key)):
            if not api_key:
                st.error("âŒ è¯·è¾“å…¥ API Key")
                st.stop()
            
            if not selected_mode:
                st.error("âŒ è¯·é€‰æ‹©åˆ†ææ¨¡å¼")
                st.stop()
            
            # è®¾ç½®åˆ†æçŠ¶æ€å¹¶ä¿å­˜é…ç½®åˆ° session_state
            st.session_state.is_analyzing = True
            st.session_state.selected_mode = selected_mode
            st.session_state.system_prompt = system_prompt
            
            # åˆå§‹åŒ–ä»»åŠ¡é˜Ÿåˆ—
            task_queue = []
            for i, uploaded_file in enumerate(uploaded_files):
                file_id = generate_file_id(f"{uploaded_file.name}_{i}_{time.time()}")
                task_queue.append({
                    'file_id': file_id,
                    'file': uploaded_file,
                    'index': i
                })
            
            st.session_state.task_queue = task_queue
            st.rerun()
    
    # å¦‚æœæ­£åœ¨åˆ†æï¼Œæ‰§è¡Œæ‰¹é‡å¤„ç†å¾ªç¯
    if st.session_state.is_analyzing and "task_queue" in st.session_state:
        task_queue = st.session_state.task_queue
        if len(task_queue) > 0:
            st.subheader("ğŸ“‹ ä»»åŠ¡é˜Ÿåˆ—")
            
            # åˆå§‹åŒ–æ‰€æœ‰è¡Œçš„å ä½ç¬¦ï¼ˆä»…åœ¨ç¬¬ä¸€æ¬¡æ¸²æŸ“æ—¶ï¼‰
            if "row_containers" not in st.session_state:
                st.session_state.row_containers = []
                for task in task_queue:
                    row_container = st.empty()
                    with row_container.container():
                        st.markdown(f"â³ **ç­‰å¾…å¤„ç†**: {task['file'].name}")
                    st.session_state.row_containers.append({
                        'file_id': task['file_id'],
                        'container': row_container,
                        'status': 'waiting'
                    })
            
            # è·å–å½“å‰å¤„ç†ç´¢å¼•
            if "current_task_index" not in st.session_state:
                st.session_state.current_task_index = 0
            
            current_idx = st.session_state.current_task_index
            
            if current_idx < len(task_queue):
                task = task_queue[current_idx]
                row_info = st.session_state.row_containers[current_idx]
                
                # æ›´æ–°å½“å‰è¡Œçš„çŠ¶æ€
                with row_info['container'].container():
                    st.markdown(f"ğŸ”„ **æ­£åœ¨æ·±å…¥åˆ†æ (å…³è”çŸ¥è¯†åº“ä¸­...)**: {task['file'].name}...")
                
                # æå–æ–‡ä»¶æ–‡æœ¬
                file_text = extract_text_from_uploaded_file(task['file'])
                
                if file_text:
                    # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼ˆç”¨äºæ ‡ç­¾æå–å’Œç›¸ä¼¼é¡¹ç›®æŸ¥è¯¢ï¼‰
                    client = OpenAI(
                        base_url=base_url,  # ä½¿ç”¨ä¾§è¾¹æ é…ç½®çš„å€¼
                        api_key=api_key
                    )
                    
                    # åŸºäºæ ‡ç­¾åŒ¹é…æ£€ç´¢ç›¸ä¼¼é¡¹ç›®
                    similar_projects = []
                    if memory_manager.enabled and memory_manager.get_count() > 0:
                        similar_projects = memory_manager.query_similar(file_text, client, model, top_k=3)
                    
                    # æ£€ç´¢ä¸»åŠ¨å­¦ä¹ çŸ¥è¯†åº“ï¼ˆå¦‚æœçŸ¥è¯†åº“ä¸ä¸ºç©ºï¼‰
                    knowledge_context = ""
                    if knowledge_manager.get_count() > 0:
                        try:
                            # ä»æ–‡ä»¶æ–‡æœ¬ä¸­æå–å…³é”®è¯ç”¨äºæ£€ç´¢
                            # ç®€å•æå–ï¼šå–å‰ 200 å­—ç¬¦ä½œä¸ºæŸ¥è¯¢
                            query_text = file_text[:200]
                            knowledge_entries = knowledge_manager.query_knowledge(query_text, top_k=3)
                            
                            if knowledge_entries:
                                knowledge_context = "\n\n---\nã€ä¸»åŠ¨å­¦ä¹ çŸ¥è¯†åº“å‚è€ƒã€‘ä»¥ä¸‹æ˜¯ä»æœ€æ–°è¡Œä¸šåŠ¨æ€ä¸­å­¦ä¹ åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼š\n"
                                for entry in knowledge_entries:
                                    knowledge_context += f"- {entry.get('title', 'æ— æ ‡é¢˜')}ï¼š{entry.get('summary', 'æ— æ€»ç»“')}\n"
                                knowledge_context += "\nè¯·ç»“åˆè¿™äº›è¡Œä¸šåŠ¨æ€ï¼Œåˆ†æå½“å‰é¡¹ç›®çš„å¸‚åœºæ—¶æœºå’Œç«äº‰ç¯å¢ƒã€‚\n"
                        except Exception as e:
                            print(f"âš ï¸ çŸ¥è¯†åº“æ£€ç´¢å¤±è´¥: {e}")
                            # é™é»˜å¤±è´¥ï¼Œä¸å½±å“ä¸»æµç¨‹
                    
                    # å¢å¼º Promptï¼ˆåŒ…å«ç›¸ä¼¼é¡¹ç›®ä¿¡æ¯å’ŒçŸ¥è¯†åº“ä¸Šä¸‹æ–‡ï¼‰
                    enhanced_prompt = enhance_system_prompt(
                        st.session_state.system_prompt,
                        similar_projects if similar_projects else None
                    )
                    
                    # å¦‚æœæœ‰çŸ¥è¯†åº“ä¸Šä¸‹æ–‡ï¼Œæ·»åŠ åˆ° prompt
                    if knowledge_context:
                        enhanced_prompt = enhanced_prompt + knowledge_context
                    
                    # å¤„ç†æ–‡ä»¶
                    result = process_single_file(
                        uploaded_file=task['file'],
                        file_id=task['file_id'],
                        system_prompt=enhanced_prompt,
                        api_key=api_key,
                        model=model,
                        selected_mode=st.session_state.selected_mode,
                        row_container=row_info['container'],
                        similar_projects=similar_projects
                    )
                    
                    if result:
                        # æå– JSON æ•°æ®
                        json_data = result.get('json_data')
                        
                        # å­˜å‚¨åˆ°è½»é‡çº§è®°å¿†åº“
                        if json_data and memory_manager.enabled:
                            project_name = json_data.get('project_name', task['file'].name)
                            summary = json_data.get('summary', '')
                            score = result.get('score', json_data.get('score', 'N/A'))
                            # ä½¿ç”¨ä»å“åº”ä¸­æå–çš„ tagsï¼ˆå·²é€šè¿‡ parse_llm_response æå–ï¼‰
                            tags = result.get('tags', [])
                            
                            # å¦‚æœä»ç„¶æ²¡æœ‰æ ‡ç­¾ï¼Œå°è¯•ä» JSON ä¸­è·å–
                            if not tags and json_data.get('tags'):
                                tags = json_data.get('tags', [])
                            
                            memory_manager.add_memory(
                                name=project_name,
                                summary=summary,
                                full_text=result.get('full_response', result['markdown_content']),  # ä½¿ç”¨å®Œæ•´å“åº”
                                score=score,
                                tags=tags,
                                meta={
                                    'industry': json_data.get('industry', ''),
                                    'stage': json_data.get('stage', ''),
                                    'risk_level': json_data.get('risk_level', '')
                                }
                            )
                        
                        # æ›´æ–°è¡Œå®¹å™¨ä¸ºæœ€ç»ˆç»“æœ
                        row_info['container'].empty()
                        with row_info['container'].container():
                            col1, col2 = st.columns([0.05, 0.95])
                            
                            with col1:
                                checkbox_key = f"select_{result['file_id']}"
                                st.checkbox(
                                    "",
                                    value=True,
                                    key=checkbox_key,
                                    label_visibility="collapsed"
                                )
                            
                            with col2:
                                score_display = f"{result['score']}åˆ†" if isinstance(result['score'], (int, float)) else str(result['score'])
                                expander_title = f"âœ… [{score_display}] {result['project_name']} (ç‚¹å‡»å±•å¼€è¯¦æƒ…)"
                                
                                with st.expander(expander_title, expanded=False):
                                    # ========== å¤§è„‘æ€è€ƒè·¯å¾„å±•ç¤º ==========
                                    st.info("ğŸ§  **å¤§è„‘æ€è€ƒè·¯å¾„**")
                                    
                                    # æ˜¾ç¤ºæå–åˆ°çš„æ ‡ç­¾
                                    tags = result.get('tags', [])
                                    if tags:
                                        tags_str = ', '.join([f"`{tag}`" for tag in tags])
                                        st.markdown(f"**æ ¸å¿ƒå…³é”®è¯**: {tags_str}")
                                    else:
                                        st.markdown("**æ ¸å¿ƒå…³é”®è¯**: æœªæå–åˆ°æ ‡ç­¾")
                                    
                                    # æ˜¾ç¤ºè®°å¿†æ¿€æ´»çŠ¶æ€
                                    similar_projects = result.get('similar_projects', [])
                                    if similar_projects and len(similar_projects) > 0:
                                        project_names = [proj.get('name', 'æœªçŸ¥é¡¹ç›®') for proj in similar_projects]
                                        common_tags_list = []
                                        for proj in similar_projects:
                                            common_tags = proj.get('common_tags', [])
                                            if common_tags:
                                                common_tags_list.extend(common_tags)
                                        
                                        common_tags_str = ', '.join(set(common_tags_list)) if common_tags_list else 'æ— '
                                        st.success(f"ğŸ¯ **è®°å¿†æ¿€æ´»**: å‘ç°ä¸å†å²é¡¹ç›® `{', '.join(project_names)}` å­˜åœ¨å…³è”ï¼ˆå…±åŒæ ‡ç­¾: {common_tags_str}ï¼‰ï¼Œå·²è¿›è¡Œæ¨ªå‘å¯¹æ¯”ã€‚")
                                    else:
                                        st.info("ğŸŒ± **æ–°ç‰©ç§æ”¶å½•**: çŸ¥è¯†åº“ä¸­æš‚æ— åŒç±»ï¼Œå·²ä½œä¸ºç§å­å­˜å…¥å¤§è„‘ã€‚")
                                    
                                    st.divider()
                                    
                                    # æ˜¾ç¤ºæ¸…ç†åçš„ Markdown å†…å®¹ï¼ˆæ­£æ–‡ï¼‰
                                    cleaned_content = clean_markdown_for_display(result['markdown_content'])
                                    st.markdown(cleaned_content)
                                    
                                    word_buffer = result.get('word_buffer')
                                    if word_buffer:
                                        word_buffer.seek(0)
                                        st.download_button(
                                            label="ğŸ“¥ ä¸‹è½½æ­¤æŠ¥å‘Š",
                                            data=word_buffer.read(),
                                            file_name=f"{result['project_name']}_{datetime.now().strftime('%Y%m%d')}.docx",
                                            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                                            key=f"download_{result['file_id']}"
                                        )
                        
                        # ä¿å­˜ç»“æœåˆ° session state
                        if "processed_results" not in st.session_state:
                            st.session_state.processed_results = []
                        st.session_state.processed_results.append(result)
                
                # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªä»»åŠ¡
                st.session_state.current_task_index += 1
                st.rerun()
            else:
                # æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                st.success(f"âœ… æˆåŠŸåˆ†æ {len(st.session_state.processed_results)}/{len(task_queue)} ä¸ªæ–‡ä»¶ï¼")
                
                # æ¸…ç†çŠ¶æ€
                st.session_state.is_analyzing = False
                if "row_containers" in st.session_state:
                    del st.session_state.row_containers
                if "current_task_index" in st.session_state:
                    del st.session_state.current_task_index
                if "task_queue" in st.session_state:
                    del st.session_state.task_queue
                
                st.rerun()

    # ä» session state æ¸²æŸ“å·²å¤„ç†çš„ç»“æœï¼ˆåˆ·æ–°åä»ç„¶æ˜¾ç¤ºï¼‰
    if st.session_state.get("processed_results"):
        st.divider()
        st.subheader("ğŸ“Š åˆ†æç»“æœåˆ—è¡¨")
        
        selected_results = []
        for result in st.session_state["processed_results"]:
            is_selected = render_result_row(result, len(selected_results))
            if is_selected:
                selected_results.append(result)
        
        # æ‰¹é‡ä¸‹è½½æŒ‰é’®
        st.divider()
        if selected_results:
            if st.button("ğŸ“¥ ä¸‹è½½é€‰ä¸­é¡¹ç›®ï¼ˆZIP å‹ç¼©åŒ…ï¼‰", type="primary"):
                zip_buffer = io.BytesIO()
                
                with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                    for result in selected_results:
                        word_buffer = result.get('word_buffer')
                        if word_buffer:
                            word_buffer.seek(0)
                            file_name = f"{result['project_name']}_{result['file_id']}.docx"
                            zip_file.writestr(file_name, word_buffer.read())
                
                zip_buffer.seek(0)
                
                st.download_button(
                    label=f"â¬‡ï¸ ç‚¹å‡»ä¸‹è½½ ZIP æ–‡ä»¶ï¼ˆ{len(selected_results)} ä¸ªæŠ¥å‘Šï¼‰",
                    data=zip_buffer.read(),
                    file_name=f"batch_reports_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip",
                    mime="application/zip",
                    key="download_zip"
                )
        
        # æ¸…é™¤æ‰€æœ‰ç»“æœæŒ‰é’®
        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰ç»“æœ"):
            st.session_state["processed_results"] = []
            st.session_state["task_queue"] = []
            st.session_state["processing_status"] = {}
            st.rerun()

elif page == "ğŸ§  çŸ¥è¯†å¤§è„‘":
    # ========== çŸ¥è¯†å¤§è„‘é¡µé¢ ==========
    st.title("ğŸ§  çŸ¥è¯†å¤§è„‘")
    st.caption("ä¸»åŠ¨å­¦ä¹  Agent - RSS æŠ“å–ã€ç½‘ç»œæœç´¢ã€LLM æ€»ç»“ã€çŸ¥è¯†åº“æ£€ç´¢")
    
    # æ£€æŸ¥ä¾èµ–çŠ¶æ€
    if not HAS_DUCKDUCKGO or not HAS_FEEDPARSER:
        st.warning("âš ï¸ **è½¯ä¾èµ–æœªå®‰è£…**ï¼šè¯·å®‰è£…ä¾èµ–ä»¥å¯ç”¨è”ç½‘åŠŸèƒ½")
        st.info("å®‰è£…å‘½ä»¤ï¼š`pip install duckduckgo-search feedparser`")
        st.caption("ğŸ’¡ åŸºç¡€åˆ†æåŠŸèƒ½ä»ç„¶å¯ç”¨ï¼Œä½†ä¸»åŠ¨å­¦ä¹ åŠŸèƒ½å°†è¢«ç¦ç”¨")
        st.divider()
    
    # çŸ¥è¯†åº“çŠ¶æ€
    kb_count = knowledge_manager.get_count()
    st.metric("ğŸ“š çŸ¥è¯†åº“æ¡ç›®æ•°", kb_count)
    
    st.divider()
    
    # å¯åŠ¨ä¸“é¡¹å­¦ä¹ 
    st.subheader("ğŸš€ å¯åŠ¨ä¸“é¡¹å­¦ä¹ ")
    
    col1, col2 = st.columns(2)
    
    with col1:
        learning_mode = st.radio(
            "å­¦ä¹ æ¨¡å¼",
            options=["RSS æºå­¦ä¹ ", "ç½‘ç»œæœç´¢å­¦ä¹ "],
            disabled=not (HAS_DUCKDUCKGO and HAS_FEEDPARSER)
        )
    
    with col2:
        topic = st.text_input(
            "å­¦ä¹ ä¸»é¢˜/å…³é”®è¯",
            placeholder="ä¾‹å¦‚ï¼šAI Agent, å…·èº«æ™ºèƒ½, SaaS...",
            disabled=not (HAS_DUCKDUCKGO and HAS_FEEDPARSER)
        )
    
    # RSS æºé…ç½®
    if learning_mode == "RSS æºå­¦ä¹ ":
        st.subheader("ğŸ“¡ RSS æºé…ç½®")
        rss_sources = st.text_area(
            "RSS æºåˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ª URLï¼‰",
            placeholder="https://example.com/rss1\nhttps://example.com/rss2",
            disabled=not HAS_FEEDPARSER,
            height=100
        )
        
        max_items = st.number_input("æ¯ä¸ªæºæœ€å¤šæŠ“å–æ¡ç›®æ•°", min_value=1, max_value=50, value=10, disabled=not HAS_FEEDPARSER)
        
        if st.button("ğŸ“¥ å¼€å§‹ RSS å­¦ä¹ ", type="primary", disabled=not (HAS_FEEDPARSER and rss_sources and topic)):
            if not api_key:
                st.error("âŒ è¯·è¾“å…¥ API Key")
            else:
                with st.spinner("æ­£åœ¨æŠ“å– RSS å¹¶æ€»ç»“..."):
                    client = OpenAI(base_url=base_url, api_key=api_key)
                    rss_urls = [url.strip() for url in rss_sources.split('\n') if url.strip()]
                    
                    total_fetched = 0
                    total_summarized = 0
                    total_filtered = 0
                    
                    for rss_url in rss_urls:
                        try:
                            items = knowledge_manager.fetch_rss(rss_url, max_items)
                            total_fetched += len(items)
                            
                            for item in items:
                                content = f"{item.get('title', '')}\n{item.get('summary', '')}"
                                summary = knowledge_manager.summarize_with_llm(
                                    content, client, model, topic
                                )
                                
                                # åªæœ‰å½“ summary ä¸ä¸º Noneï¼ˆå†…å®¹ç›¸å…³ï¼‰æ—¶æ‰ä¿å­˜
                                if summary:
                                    knowledge_manager.add_knowledge(
                                        title=item.get('title', 'æ— æ ‡é¢˜'),
                                        content=content,
                                        source=rss_url,
                                        summary=summary,
                                        tags=[topic] if topic else []
                                    )
                                    total_summarized += 1
                                else:
                                    total_filtered += 1
                        except Exception as e:
                            st.warning(f"âš ï¸ å¤„ç† RSS æºå¤±è´¥ ({rss_url}): {e}")
                    
                    if total_summarized > 0:
                        st.success(f"âœ… å®Œæˆï¼æŠ“å– {total_fetched} æ¡ï¼Œé€šè¿‡éªŒè¯å¹¶ä¿å­˜ {total_summarized} æ¡ï¼Œè¿‡æ»¤æ‰ {total_filtered} æ¡æ— å…³å†…å®¹")
                    else:
                        st.warning(f"âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆä¿¡æ¯ï¼æŠ“å– {total_fetched} æ¡ï¼Œä½†æ‰€æœ‰ç»“æœéƒ½è¢«è¿‡æ»¤ä¸ºä¸ç›¸å…³å†…å®¹ã€‚è¯·å°è¯•æ›´ç²¾ç¡®çš„ä¸»é¢˜å…³é”®è¯ã€‚")
                    st.rerun()
    
    # ç½‘ç»œæœç´¢å­¦ä¹ 
    elif learning_mode == "ç½‘ç»œæœç´¢å­¦ä¹ ":
        st.subheader("ğŸ” ç½‘ç»œæœç´¢å­¦ä¹ ")
        
        search_query = st.text_input(
            "æœç´¢å…³é”®è¯",
            value=topic if topic else "",
            placeholder="ä¾‹å¦‚ï¼šAI Agent æŠ•èµ„è¶‹åŠ¿",
            disabled=not HAS_DUCKDUCKGO
        )
        
        max_results = st.number_input("æœ€å¤šæœç´¢ç»“æœæ•°", min_value=1, max_value=20, value=5, disabled=not HAS_DUCKDUCKGO)
        
        if st.button("ğŸ” å¼€å§‹æœç´¢å­¦ä¹ ", type="primary", disabled=not (HAS_DUCKDUCKGO and search_query)):
            if not api_key:
                st.error("âŒ è¯·è¾“å…¥ API Key")
            else:
                # ä½¿ç”¨ st.status æ˜¾ç¤ºè¯¦ç»†è¿›åº¦
                with st.status("æ­£åœ¨è¿›è¡Œæ·±åº¦å­¦ä¹ ...", expanded=True) as status:
                    client = OpenAI(base_url=base_url, api_key=api_key)
                    
                    try:
                        # æ­¥éª¤1: å¼ºåˆ¶ç¿»è¯‘ä¸ºè‹±æ–‡æœç´¢è¯ï¼ˆå…³é”®ä¿®å¤ï¼‰
                        status.write("ğŸ”„ æ­¥éª¤1/4: æ­£åœ¨å°†å…³é”®è¯è½¬åŒ–ä¸ºè‹±æ–‡...")
                        
                        # å¼ºåˆ¶è°ƒç”¨ LLM ç¿»è¯‘å‡½æ•°ï¼Œç¡®ä¿ä½¿ç”¨è‹±æ–‡æœç´¢
                        english_query = knowledge_manager.optimize_query_with_llm(
                            search_query, 
                            client, 
                            model
                        )
                        
                        # å…³é”®ä¿®å¤ï¼šéªŒè¯ç¿»è¯‘ç»“æœï¼Œç¡®ä¿æ˜¯æœ‰æ•ˆçš„è‹±æ–‡æŸ¥è¯¢
                        if not english_query or not english_query.strip():
                            english_query = search_query  # å¦‚æœç¿»è¯‘å¤±è´¥ï¼Œä½¿ç”¨åŸæŸ¥è¯¢
                            status.write(f"âš ï¸ ç¿»è¯‘å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢: {search_query}")
                        else:
                            english_query = english_query.strip()
                            
                            # éªŒè¯ï¼šå¦‚æœç¿»è¯‘åçš„æŸ¥è¯¢ä»ç„¶æ˜¯ä¸­æ–‡ï¼ˆæˆ–åªæœ‰ä¸€ä¸ªå­—ç¬¦ï¼‰ï¼Œå¼ºåˆ¶é‡æ–°ç¿»è¯‘
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
                            has_chinese = any('\u4e00' <= char <= '\u9fff' for char in english_query)
                            if has_chinese or len(english_query) <= 2:
                                status.write(f"âš ï¸ ç¿»è¯‘ç»“æœå¼‚å¸¸ï¼ˆå¯èƒ½ä»æ˜¯ä¸­æ–‡ï¼‰ï¼Œå¼ºåˆ¶é‡æ–°ç¿»è¯‘...")
                                # å¼ºåˆ¶ä½¿ç”¨æ›´ç›´æ¥çš„ç¿»è¯‘ Prompt
                                try:
                                    translation_prompt = f"Translate this to a concise English search query. Input: '{search_query}'. Output: ONLY the English query (3-5 words max)."
                                    translation_response = client.chat.completions.create(
                                        model=model,
                                        messages=[
                                            {"role": "system", "content": "You are a search query translator. Output ONLY the English search query, no explanations."},
                                            {"role": "user", "content": translation_prompt}
                                        ],
                                        temperature=0.3,
                                        max_tokens=30
                                    )
                                    english_query = translation_response.choices[0].message.content.strip()
                                    english_query = english_query.strip('"').strip("'")
                                    # å¦‚æœè¿˜æ˜¯æœ‰é—®é¢˜ï¼Œè‡³å°‘ç¡®ä¿ä¸æ˜¯å•å­—ç¬¦
                                    if len(english_query) <= 2:
                                        status.write(f"âŒ ç¿»è¯‘ä¸¥é‡å¤±è´¥ï¼Œä½¿ç”¨åŸæŸ¥è¯¢ä½œä¸ºå¤‡é€‰")
                                        english_query = search_query
                                except Exception as e:
                                    status.write(f"âš ï¸ å¼ºåˆ¶ç¿»è¯‘å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸæŸ¥è¯¢")
                                    english_query = search_query
                        
                        # UI åé¦ˆï¼šæ˜¾ç¤ºç¿»è¯‘ç»“æœ
                        status.write(f"ğŸ¯ å·²é”å®šè‹±æ–‡æœç´¢è¯: **{english_query}**")
                        status.write(f"ğŸ“ åŸå§‹æŸ¥è¯¢: {search_query} â†’ è‹±æ–‡æŸ¥è¯¢: {english_query}")
                        
                        # è·å–ä»£ç†é…ç½®
                        proxy_config = st.session_state.get("http_proxy_input", "http://127.0.0.1:7890")
                        # å¦‚æœä»£ç†ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œè®¾ä¸º None
                        proxy_config = proxy_config.strip() if proxy_config and proxy_config.strip() else None
                        
                        # æ­¥éª¤2: ä½¿ç”¨è‹±æ–‡æŸ¥è¯¢è¿›è¡Œæœç´¢ï¼ˆå¼ºåˆ¶ç¾å›½åŒºï¼Œä¼˜å…ˆæ–°é—»æºï¼‰
                        status.write("ğŸ” æ­¥éª¤2/4: æ­£åœ¨å…¨çƒç½‘ç»œæœç´¢ (Region: US-EN, ä¼˜å…ˆæ–°é—»æº)...")
                        
                        # æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼Œç¡®è®¤ä¼ å…¥çš„å‚æ•°
                        print(f"[DEBUG] è°ƒç”¨ search_webï¼Œå‚æ•° query={english_query}, max_results={max_results}")
                        status.write(f"ğŸ¯ æœç´¢å…³é”®è¯: '{english_query}'")
                        
                        results = knowledge_manager.search_web(
                            english_query,  # å¼ºåˆ¶ä½¿ç”¨ç¿»è¯‘åçš„è‹±æ–‡æŸ¥è¯¢
                            max_results, 
                            proxy=proxy_config,
                            client=None,  # ä¸å†éœ€è¦åœ¨è¿™é‡Œç¿»è¯‘ï¼Œå› ä¸ºå·²ç»ç¿»è¯‘å®Œæˆ
                            model=None,
                            region="us-en"  # å¼ºåˆ¶ç¾å›½åŒº
                        )
                        
                        # å¦‚æœæ²¡æœ‰ç»“æœï¼Œå¯èƒ½æ˜¯ç½‘ç»œè¿æ¥é—®é¢˜
                        if not results:
                            status.write("âŒ ç½‘ç»œè¿æ¥é”™è¯¯ï¼šæœªæœç´¢åˆ°ç»“æœ")
                            st.error("ç½‘ç»œè¿æ¥é”™è¯¯ï¼šæœªæœç´¢åˆ°ç»“æœã€‚è¯·æ£€æŸ¥ 1. ä»£ç†ç«¯å£æ˜¯å¦æ­£ç¡®; 2. VPNæ˜¯å¦å¼€å¯ã€‚")
                        else:
                            # ç»Ÿè®¡æ–°é—»å’Œæ–‡æœ¬ç»“æœæ•°é‡
                            news_results = [r for r in results if r.get('source_type') == 'news']
                            text_results = [r for r in results if r.get('source_type') == 'text']
                            status.write(f"âœ… æˆåŠŸæœç´¢åˆ° {len(results)} æ¡ç»“æœ (æ–°é—»: {len(news_results)}, æ–‡æœ¬: {len(text_results)})")
                            
                            # æ˜¾ç¤ºç»“æœæ¥æºåŸŸåï¼ˆå»é‡ï¼‰
                            domains = set()
                            for r in results:
                                url = r.get('url', '')
                                if url:
                                    try:
                                        from urllib.parse import urlparse
                                        domain = urlparse(url).netloc
                                        if domain:
                                            domains.add(domain)
                                    except:
                                        pass
                            if domains:
                                status.write(f"ğŸ“Š æ¥æºåŸŸå: {', '.join(list(domains)[:5])}{'...' if len(domains) > 5 else ''}")
                            
                            status.write("ğŸ“– æ­¥éª¤3/4: æ­£åœ¨åˆ†æå¹¶æå–çŸ¥è¯†...")
                            
                            total_summarized = 0
                            total_filtered = 0
                            
                            # é€æ¡å¤„ç†æœç´¢ç»“æœ
                            for i, result in enumerate(results):
                                title = result.get('title', 'æ— æ ‡é¢˜')
                                url = result.get('url', result.get('link', ''))
                                
                                # æå–åŸŸåç”¨äºæ˜¾ç¤º
                                domain = ""
                                if url:
                                    try:
                                        from urllib.parse import urlparse
                                        domain = urlparse(url).netloc
                                    except:
                                        pass
                                
                                domain_display = f" [{domain}]" if domain else ""
                                status.write(f"ğŸ“„ æ­£åœ¨é˜…è¯»ç¬¬ {i+1}/{len(results)} ç¯‡: {title[:40]}{domain_display}...")
                                
                                content = f"{result.get('title', '')}\n{result.get('snippet', '')}"
                                summary = knowledge_manager.summarize_with_llm(
                                    content, client, model, search_query
                                )
                                
                                # æ˜¾å¼å±•ç¤º LLM çš„åˆ¤æ–­
                                if summary:
                                    # æˆåŠŸæ€»ç»“
                                    print(f"[âœ… æˆåŠŸ] æˆåŠŸæå–è§è§£: {title} (æ¥æº: {domain})")
                                    status.write(f"âœ… [{i+1}] æˆåŠŸæå–è§è§£: {title[:40]}{domain_display}")
                                    
                                    # ä¿å­˜åˆ°çŸ¥è¯†åº“ï¼ˆä½¿ç”¨å½’ä¸€åŒ–åçš„ url å­—æ®µï¼‰
                                    success = knowledge_manager.add_knowledge(
                                        title=title,
                                        content=content,
                                        source=url,  # ä½¿ç”¨å½’ä¸€åŒ–åçš„ url
                                        summary=summary,
                                        tags=[search_query] if search_query else []
                                    )
                                    
                                    if success:
                                        total_summarized += 1
                                        # ä½¿ç”¨ toast æç¤ºï¼ˆå¦‚æœ Streamlit ç‰ˆæœ¬æ”¯æŒï¼‰
                                        try:
                                            st.toast(f"âœ… å·²ä¿å­˜: {title[:30]}...", icon="âœ…")
                                        except:
                                            pass  # å¦‚æœ toast ä¸å¯ç”¨ï¼Œé™é»˜è·³è¿‡
                                    else:
                                        status.write(f"âš ï¸ [{i+1}] ä¿å­˜å¤±è´¥: {title[:50]}")
                                else:
                                    # è¢«è¿‡æ»¤ä¸ºä¸ç›¸å…³
                                    print(f"[è¿‡æ»¤] åˆ¤å®šä¸ºæ— å…³: {title} (æ¥æº: {domain})")
                                    status.write(f"â­ï¸ [{i+1}] è·³è¿‡: {title[:40]} (åŸå› : æ— æŠ•èµ„ä»·å€¼/ä¸ç›¸å…³){domain_display}")
                                    total_filtered += 1
                                    # ä½¿ç”¨ toast æç¤º
                                    try:
                                        st.toast(f"â­ï¸ è·³è¿‡: {title[:30]}...ï¼ˆæ— å…³å†…å®¹ï¼‰", icon="â­ï¸")
                                    except:
                                        pass  # å¦‚æœ toast ä¸å¯ç”¨ï¼Œé™é»˜è·³è¿‡
                            
                            # æ­¥éª¤4: å®Œæˆ
                            status.write("ğŸ’¾ æ­¥éª¤4/4: ä¿å­˜çŸ¥è¯†åº“...")
                            
                            # ç¡®ä¿ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆadd_knowledge å†…éƒ¨å·²ç»ä¿å­˜ï¼Œä½†å†æ¬¡ç¡®è®¤ï¼‰
                            knowledge_manager._save_knowledge()
                            
                            # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
                            if total_summarized > 0:
                                status.write(f"âœ… å®Œæˆï¼å…±å¤„ç† {len(results)} æ¡ï¼ŒæˆåŠŸä¿å­˜ {total_summarized} æ¡ï¼Œè¿‡æ»¤ {total_filtered} æ¡")
                                st.success(f"âœ… å®Œæˆï¼å…±å¤„ç† {len(results)} æ¡ï¼ŒæˆåŠŸä¿å­˜ {total_summarized} æ¡ï¼Œè¿‡æ»¤ {total_filtered} æ¡")
                            else:
                                status.write(f"âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆä¿¡æ¯ï¼æ‰€æœ‰ç»“æœéƒ½è¢«è¿‡æ»¤ä¸ºä¸ç›¸å…³å†…å®¹")
                                st.warning(f"âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆä¿¡æ¯ï¼æœç´¢ {len(results)} æ¡ï¼Œä½†æ‰€æœ‰ç»“æœéƒ½è¢«è¿‡æ»¤ä¸ºä¸ç›¸å…³å†…å®¹ã€‚è¯·å°è¯•æ›´ç²¾ç¡®çš„æœç´¢å…³é”®è¯ã€‚")
                        
                    except Exception as e:
                        error_msg = f"âŒ æœç´¢å­¦ä¹ å¤±è´¥: {str(e)}"
                        print(error_msg)
                        try:
                            status.write(error_msg)
                        except:
                            pass  # å¦‚æœ status å·²å¤±æ•ˆï¼Œé™é»˜è·³è¿‡
                        st.error(f"{error_msg}ã€‚è¯·æ£€æŸ¥ 1. ä»£ç†ç«¯å£æ˜¯å¦æ­£ç¡®; 2. VPNæ˜¯å¦å¼€å¯; 3. API Key æ˜¯å¦æœ‰æ•ˆã€‚")
                    
                    # å¿…é¡»åˆ·æ–°é¡µé¢ï¼Œè®©ç”¨æˆ·ç«‹åˆ»çœ‹åˆ°æ–°çŸ¥è¯†ï¼ˆæ— è®ºæˆåŠŸè¿˜æ˜¯å¤±è´¥ï¼‰
                    st.rerun()
    
    st.divider()
    
    # çŸ¥è¯†åº“æµè§ˆ
    col_header1, col_header2 = st.columns([0.9, 0.1])
    with col_header1:
        st.subheader("ğŸ“š çŸ¥è¯†åº“æµè§ˆ")
    with col_header2:
        # æ¸…ç©ºçŸ¥è¯†åº“æŒ‰é’®
        if kb_count > 0:
            if st.button("ğŸ—‘ï¸ æ¸…ç©º", type="secondary", help="æ¸…ç©ºæ‰€æœ‰å·²å­¦ä¹ çŸ¥è¯†ï¼ˆä¸å¯æ¢å¤ï¼‰"):
                if knowledge_manager.clear_all():
                    st.success("âœ… çŸ¥è¯†åº“å·²æ¸…ç©º")
                    st.rerun()
                else:
                    st.error("âŒ æ¸…ç©ºå¤±è´¥")
    
    if kb_count == 0:
        st.info("ğŸ“­ çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆå¯åŠ¨ä¸“é¡¹å­¦ä¹ ")
    else:
        # æœç´¢æ¡†
        search_query_kb = st.text_input("ğŸ” æœç´¢çŸ¥è¯†åº“", placeholder="è¾“å…¥å…³é”®è¯æœç´¢...")
        
        if search_query_kb:
            results = knowledge_manager.query_knowledge(search_query_kb, top_k=10)
            if results:
                st.success(f"æ‰¾åˆ° {len(results)} æ¡ç›¸å…³æ¡ç›®")
                for idx, entry in enumerate(results):
                    with st.expander(f"ğŸ“„ {entry.get('title', 'æ— æ ‡é¢˜')}", expanded=False):
                        st.caption(f"æ¥æº: {entry.get('source', 'æœªçŸ¥')}")
                        st.caption(f"æ—¶é—´: {entry.get('timestamp', '')}")
                        if entry.get('tags'):
                            st.caption(f"æ ‡ç­¾: {', '.join(entry['tags'])}")
                        st.markdown(f"**æ€»ç»“**: {entry.get('summary', 'æ— æ€»ç»“')}")
                        if entry.get('content'):
                            with st.expander("æŸ¥çœ‹å®Œæ•´å†…å®¹", expanded=False):
                                st.text(entry['content'])
            else:
                st.info("æœªæ‰¾åˆ°ç›¸å…³æ¡ç›®")
        else:
            # æ˜¾ç¤ºæœ€è¿‘çš„çŸ¥è¯†æ¡ç›®
            st.caption("æœ€è¿‘å­¦ä¹ åˆ°çš„çŸ¥è¯†ï¼ˆæœ€æ–° 10 æ¡ï¼‰")
            recent_entries = list(reversed(knowledge_manager.knowledge_base[-10:]))
            for idx, entry in enumerate(recent_entries):
                with st.expander(f"ğŸ“„ {entry.get('title', 'æ— æ ‡é¢˜')}", expanded=False):
                    st.caption(f"æ¥æº: {entry.get('source', 'æœªçŸ¥')}")
                    st.caption(f"æ—¶é—´: {entry.get('timestamp', '')}")
                    if entry.get('tags'):
                        st.caption(f"æ ‡ç­¾: {', '.join(entry['tags'])}")
                    st.markdown(f"**æ€»ç»“**: {entry.get('summary', 'æ— æ€»ç»“')}")
                    if entry.get('content'):
                        with st.expander("æŸ¥çœ‹å®Œæ•´å†…å®¹", expanded=False):
                            st.text(entry['content'])

elif page == "ğŸ“‚ å…¨é‡çŸ¥è¯†åº“":
    # ========== å…¨é‡çŸ¥è¯†åº“é¡µé¢ ==========
    st.title("ğŸ“‚ å…¨é‡é¡¹ç›®çŸ¥è¯†åº“")
    st.caption("æŸ¥çœ‹å’Œç®¡ç†æ‰€æœ‰å·²åˆ†æçš„é¡¹ç›®æ•°æ®")
    
    try:
        # åŠ è½½çŸ¥è¯†åº“æ•°æ®
        df_kb = load_knowledge_base()
        
        if df_kb.empty:
            st.info("ğŸ“­ çŸ¥è¯†åº“ä¸ºç©ºï¼Œæš‚æ— é¡¹ç›®æ•°æ®ã€‚è¯·å…ˆåœ¨å·¥ä½œå°åˆ†æé¡¹ç›®ï¼Œæ•°æ®ä¼šè‡ªåŠ¨ç§¯ç´¯åˆ°è¿™é‡Œã€‚")
        else:
            # æ ¼å¼åŒ–æ—¥æœŸåˆ—
            if 'timestamp' in df_kb.columns:
                df_kb['timestamp'] = df_kb['timestamp'].apply(format_timestamp_for_display)
            
            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            stats = calculate_kb_statistics(df_kb)
            
            # æ˜¾ç¤ºç»Ÿè®¡æŒ‡æ ‡
            st.divider()
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("ğŸ“Š å·²æ”¶å½•é¡¹ç›®", stats['total_projects'])
            
            with col2:
                st.metric("â­ å¹³å‡è¯„åˆ†", f"{stats['avg_score']:.1f}" if stats['avg_score'] > 0 else "N/A")
            
            with col3:
                st.metric("ğŸ”¥ æœ€çƒ­èµ›é“", stats['top_industry'])
            
            with col4:
                if stats['industry_count']:
                    top_count = list(stats['industry_count'].values())[0]
                    st.metric("ğŸ† æœ€é«˜èµ›é“é¡¹ç›®æ•°", top_count)
                else:
                    st.metric("ğŸ† æœ€é«˜èµ›é“é¡¹ç›®æ•°", 0)
            
            st.divider()
            
            # æ˜¾ç¤ºå…¨é‡æ•°æ®è¡¨æ ¼
            st.subheader("ğŸ“‹ é¡¹ç›®æ•°æ®è¡¨")
            st.caption("ğŸ’¡ æç¤º: è¡¨æ ¼æ”¯æŒæ’åºã€æœç´¢å’Œç­›é€‰åŠŸèƒ½")
            
            # é€‰æ‹©è¦æ˜¾ç¤ºçš„åˆ—
            display_columns = ['timestamp', 'project_name', 'industry', 'tags', 'stage', 'score', 'summary', 'risk_level']
            available_columns = [col for col in display_columns if col in df_kb.columns]
            
            if available_columns:
                # ä½¿ç”¨ st.dataframe å±•ç¤ºå…¨é‡æ•°æ®ï¼Œå¼€å¯æ’åºå’Œæœç´¢
                st.dataframe(
                    df_kb[available_columns],
                    use_container_width=True,
                    hide_index=True,
                    height=600  # å¢åŠ è¡¨æ ¼é«˜åº¦ä»¥ä¾¿æŸ¥çœ‹æ›´å¤šæ•°æ®
                )
            else:
                st.dataframe(
                    df_kb,
                    use_container_width=True,
                    hide_index=True,
                    height=600
                )
            
            # å¯¼å‡ºåŠŸèƒ½
            st.divider()
            st.subheader("ğŸ“¥ æ•°æ®å¯¼å‡º")
            col1, col2 = st.columns(2)
            
            with col1:
                # å¯¼å‡º CSV
                csv = df_kb.to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½ CSV æ–‡ä»¶",
                    data=csv,
                    file_name=f"project_database_{datetime.now().strftime('%Y%m%d')}.csv",
                    mime="text/csv",
                    help="ä¸‹è½½å®Œæ•´é¡¹ç›®æ•°æ®åº“ï¼ˆCSV æ ¼å¼ï¼Œæ”¯æŒ Excel æ‰“å¼€ï¼‰"
                )
            
            with col2:
                # å¯¼å‡º JSON
                json_str = df_kb.to_json(orient='records', force_ascii=False, indent=2)
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½ JSON æ–‡ä»¶",
                    data=json_str,
                    file_name=f"project_database_{datetime.now().strftime('%Y%m%d')}.json",
                    mime="application/json",
                    help="ä¸‹è½½å®Œæ•´é¡¹ç›®æ•°æ®åº“ï¼ˆJSON æ ¼å¼ï¼‰"
                )
            
            # èµ›é“åˆ†å¸ƒåˆ†æ
            if stats['industry_count'] and len(stats['industry_count']) > 0:
                st.divider()
                st.subheader("ğŸ“ˆ èµ›é“åˆ†å¸ƒ")
                industry_df = pd.DataFrame([
                    {'èµ›é“': industry, 'é¡¹ç›®æ•°': count}
                    for industry, count in stats['industry_count'].items()
                ]).sort_values('é¡¹ç›®æ•°', ascending=False)
                
                st.dataframe(
                    industry_df,
                    use_container_width=True,
                    hide_index=True
                )
    
    except Exception as e:
        st.error(f"âŒ åŠ è½½çŸ¥è¯†åº“æ•°æ®å¤±è´¥: {str(e)}")
        st.info("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥ project_database.csv æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®")

elif page == "ğŸ“œ å†å²è®°å½•":
    # ========== å†å²è®°å½•é¡µé¢ ==========
    st.title("ğŸ“œ å†å²è®°å½•")
    st.caption("æŸ¥çœ‹æ‰€æœ‰å·²ä¿å­˜çš„åˆ†æå†å²è®°å½•")
    
    try:
        history_entries = load_history_entries()
        
        # æ˜¾ç¤ºçŸ¥è¯†åº“ç»Ÿè®¡
        kb_count = memory_manager.get_count()
        
        if not history_entries:
            st.info("ğŸ“­ æš‚æ— å†å²è®°å½•ã€‚åˆ†æé¡¹ç›®åï¼Œè®°å½•ä¼šè‡ªåŠ¨ä¿å­˜åˆ°è¿™é‡Œã€‚")
        else:
            col1, col2 = st.columns(2)
            with col1:
                st.metric("ğŸ“š æ€»è®°å½•æ•°", len(history_entries))
            with col2:
                st.metric("ğŸ§  çŸ¥è¯†åº“å·²æ”¶å½•é¡¹ç›®", kb_count)
            st.divider()
            
            # æ˜¾ç¤ºå†å²è®°å½•åˆ—è¡¨
            for idx, entry in enumerate(history_entries):
                datetime_str = entry.get("datetime", entry.get("timestamp", ""))
                mode_name = entry.get("mode", "æœªçŸ¥æ¨¡å¼")
                file_name = entry.get("file_name", "æœªçŸ¥æ–‡ä»¶")
                
                # æ ¼å¼åŒ–æ˜¾ç¤ºæ—¶é—´
                try:
                    if "T" in datetime_str:
                        dt = datetime.fromisoformat(datetime_str)
                        display_time = dt.strftime("%Y-%m-%d %H:%M")
                    else:
                        display_time = datetime_str
                except:
                    display_time = datetime_str
                
                with st.expander(f"ğŸ“„ {display_time} | {mode_name} | {file_name}", expanded=False):
                    col1, col2 = st.columns([1, 1])
                    
                    with col1:
                        st.caption(f"ğŸ“‹ **åˆ†ææ¨¡å¼**: {mode_name}")
                        st.caption(f"ğŸ“ **æ–‡ä»¶å**: {file_name}")
                        st.caption(f"ğŸ•’ **åˆ†ææ—¶é—´**: {display_time}")
                    
                    with col2:
                        # ä¸‹è½½æŒ‰é’®
                        content = entry.get("content", "")
                        if content:
                            # ç”Ÿæˆ Word æ–‡æ¡£ï¼ˆä¸´æ—¶ï¼‰
                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            temp_filename = f"{timestamp}_{Path(file_name).stem}.docx"
                            temp_path = HISTORY_DIR / temp_filename
                            
                            if markdown_to_docx(content, str(temp_path)):
                                with open(temp_path, "rb") as f:
                                    st.download_button(
                                        label="ğŸ“¥ ä¸‹è½½ Word æŠ¥å‘Š",
                                        data=f.read(),
                                        file_name=temp_filename,
                                        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                                        key=f"hist_download_{idx}"
                                    )
                                
                                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                                try:
                                    temp_path.unlink()
                                except:
                                    pass
                    
                    st.divider()
                    
                    # æ˜¾ç¤ºåˆ†æå†…å®¹ï¼ˆæ¸…ç† JSON ä»£ç å—ï¼‰
                    if content:
                        cleaned_content = clean_markdown_for_display(content)
                        st.markdown(cleaned_content)
                
                if idx < len(history_entries) - 1:
                    st.divider()
    
    except Exception as e:
        st.error(f"âŒ åŠ è½½å†å²è®°å½•å¤±è´¥: {str(e)}")
        st.info("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥ history_data æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨ä¸”åŒ…å«æœ‰æ•ˆçš„ JSON æ–‡ä»¶")

elif page == "ğŸ“¡ é›·è¾¾å€™é€‰æ± ":
    # ========== é›·è¾¾å€™é€‰æ± é¡µé¢ ==========
    st.title("ğŸ“¡ é›·è¾¾å€™é€‰æ± ")
    st.caption("æŸ¥çœ‹å’Œç®¡ç†ä»é›·è¾¾æŠ“å–ä¸­å‘ç°çš„æ½œåœ¨æŠ•èµ„é¡¹ç›®")
    
    if not HAS_SUPABASE:
        st.error("âŒ Supabase å®¢æˆ·ç«¯æœªå®‰è£…ã€‚è¯·è¿è¡Œ: `pip install supabase`")
        st.stop()
    
    # è·å– Supabase å®¢æˆ·ç«¯ï¼ˆå‰ç«¯åªä½¿ç”¨ ANON_KEYï¼‰
    supabase_client = get_supabase_client(use_service_role=False)
    
    if not supabase_client:
        st.error("âŒ Supabase è¿æ¥å¤±è´¥ã€‚è¯·æ£€æŸ¥ç¯å¢ƒå˜é‡ï¼š")
        st.code("""
SUPABASE_URL=your_supabase_url
SUPABASE_ANON_KEY=your_supabase_anon_key
        """)
        st.info("ğŸ’¡ è¯·åœ¨ `.env` æ–‡ä»¶ä¸­è®¾ç½®è¿™äº›ç¯å¢ƒå˜é‡ï¼Œæˆ–ä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡ã€‚")
        st.stop()
    
    # åˆå§‹åŒ– session state
    if "deals_filter" not in st.session_state:
        st.session_state.deals_filter = "å…¨éƒ¨"
    if "deals_search" not in st.session_state:
        st.session_state.deals_search = ""
    
    # è¿‡æ»¤å’Œæœç´¢é€‰é¡¹
    col_filter, col_search = st.columns([1, 2])
    with col_filter:
        filter_option = st.selectbox(
            "ç­›é€‰æ¡ä»¶",
            options=["å…¨éƒ¨", "æœªå¤„ç†", "å·²æ ‡è®°"],
            index=0 if st.session_state.deals_filter == "å…¨éƒ¨" else (1 if st.session_state.deals_filter == "æœªå¤„ç†" else 2),
            key="deals_filter_select"
        )
        st.session_state.deals_filter = filter_option
    
    with col_search:
        search_query = st.text_input(
            "ğŸ” å…³é”®å­—æœç´¢",
            value=st.session_state.deals_search,
            placeholder="æœç´¢é¡¹ç›®åç§°æˆ–ç®€ä»‹...",
            key="deals_search_input"
        )
        st.session_state.deals_search = search_query
    
    st.divider()
    
    try:
        # å…ˆè·å–æ‰€æœ‰ dealsï¼ˆæŒ‰ updated_at å€’åºï¼Œé™åˆ¶ 50 æ¡ï¼‰
        # æ˜ç¡®æŒ‡å®šéœ€è¦çš„å­—æ®µï¼ˆè‡³å°‘ id,title,canonical_name,one_liner,hostname,website,url,tags,score,created_atï¼‰
        response = supabase_client.table("deals")\
            .select("id,title,canonical_name,one_liner,description,hostname,website,url,tags,score,created_at,updated_at,evidence_urls,dedupe_key")\
            .order("updated_at", desc=True)\
            .limit(50)\
            .execute()
        
        all_deals = response.data if hasattr(response, 'data') else []
        
        # è°ƒè¯•æ¨¡å¼ï¼šè®°å½• deals è¯»å–ç»“æœ
        if st.session_state.get("debug_mode", False):
            st.session_state["debug_deals_count"] = len(all_deals)
        
        # è·å–æ‰€æœ‰ deal_actionsï¼ˆç”¨äºåˆ¤æ–­å“ªäº›å·²å¤„ç†ï¼‰
        actions_response = supabase_client.table("deal_actions")\
            .select("deal_id, action")\
            .execute()
        
        deal_actions_map = {}
        if hasattr(actions_response, 'data'):
            for action in actions_response.data:
                deal_id = action.get('deal_id')
                if deal_id:
                    if deal_id not in deal_actions_map:
                        deal_actions_map[deal_id] = []
                    deal_actions_map[deal_id].append(action.get('action'))
        
        # åº”ç”¨è¿‡æ»¤
        filtered_deals = []
        for deal in all_deals:
            deal_id = deal.get('id') or deal.get('dedupe_key', '')
            
            # å…³é”®å­—æœç´¢ï¼ˆå¯¹ title/canonical_name/one_liner/hostname/website/url åšåŒ¹é…ï¼‰
            if search_query:
                search_lower = search_query.lower()
                title = (deal.get('title') or '').lower()
                canonical_name = (deal.get('canonical_name') or '').lower()
                one_liner = (deal.get('one_liner') or deal.get('description') or '').lower()
                hostname = (deal.get('hostname') or '').lower()
                website = (deal.get('website') or '').lower()
                url = (deal.get('url') or '').lower()
                
                # å¦‚æœæœç´¢å…³é”®è¯ä¸åœ¨ä»»ä½•å­—æ®µä¸­ï¼Œè·³è¿‡
                if (search_lower not in title and 
                    search_lower not in canonical_name and 
                    search_lower not in one_liner and
                    search_lower not in hostname and
                    search_lower not in website and
                    search_lower not in url):
                    continue
            
            # ç­›é€‰æ¡ä»¶
            if filter_option == "æœªå¤„ç†":
                if deal_id in deal_actions_map:
                    continue  # å·²å¤„ç†ï¼Œè·³è¿‡
            elif filter_option == "å·²æ ‡è®°":
                if deal_id not in deal_actions_map:
                    continue  # æœªå¤„ç†ï¼Œè·³è¿‡
            
            filtered_deals.append(deal)
        
        # è°ƒè¯•æ¨¡å¼ï¼šè®°å½•è¿‡æ»¤åç»“æœ
        if st.session_state.get("debug_mode", False):
            st.session_state["debug_filtered_deals_count"] = len(filtered_deals)
        
        # æ˜¾ç¤ºç»Ÿè®¡
        st.metric("å€™é€‰é¡¹ç›®", len(filtered_deals))
        
        if not filtered_deals:
            st.info("ğŸ“­ æš‚æ— ç¬¦åˆæ¡ä»¶çš„é¡¹ç›®")
        else:
            # æ˜¾ç¤ºåˆ—è¡¨
            for deal in filtered_deals:
                # ä½¿ç”¨ deal.id ä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼ˆä¸è¦ç”¨ enumerate çš„ idxï¼‰
                deal_id = deal.get('id') or deal.get('dedupe_key', '')
                
                # ä¸»æ ‡é¢˜ä¼˜å…ˆçº§å…œåº•ï¼šdeal.title ?? deal.canonical_name ?? deal.one_liner ?? deal.hostname ?? deal.website ?? deal.url ?? "(untitled)"
                main_title = (deal.get('title') or 
                             deal.get('canonical_name') or 
                             deal.get('one_liner') or 
                             deal.get('hostname') or 
                             deal.get('website') or 
                             deal.get('url') or 
                             "(untitled)")
                
                canonical_name = deal.get('canonical_name') or deal.get('title', '')
                one_liner = deal.get('one_liner') or deal.get('description', '')
                website = deal.get('website') or deal.get('url', '')
                updated_at = deal.get('updated_at') or deal.get('created_at', '')
                
                # åˆ¤æ–­æ˜¯å¦å·²å¤„ç†
                has_action = deal_id in deal_actions_map
                action_badge = ""
                if has_action:
                    actions = deal_actions_map[deal_id]
                    action_badge = f" | å·²æ ‡è®°: {', '.join(actions)}"
                
                # ä½¿ç”¨ expander æ˜¾ç¤ºï¼ˆst.expander ä¸æ”¯æŒ key å‚æ•°ï¼‰
                with st.expander(f"ğŸ“Œ {main_title}{action_badge}", expanded=False):
                    # åŸºæœ¬ä¿¡æ¯
                    col1, col2 = st.columns([2, 1])
                    
                    with col1:
                        st.markdown(f"**é¡¹ç›®åç§°**: {canonical_name}")
                        if website:
                            st.markdown(f"**ç½‘ç«™**: [{website}]({website})" if website.startswith('http') else f"**ç½‘ç«™**: {website}")
                        if one_liner:
                            # æˆªæ–­æ˜¾ç¤º
                            one_liner_display = one_liner[:200] + "..." if len(one_liner) > 200 else one_liner
                            st.markdown(f"**ç®€ä»‹**: {one_liner_display}")
                        if updated_at:
                            try:
                                if "T" in updated_at:
                                    dt = datetime.fromisoformat(updated_at.replace('Z', '+00:00'))
                                    updated_str = dt.strftime("%Y-%m-%d %H:%M")
                                else:
                                    updated_str = updated_at
                            except:
                                updated_str = updated_at
                            st.caption(f"ğŸ•’ æ›´æ–°æ—¶é—´: {updated_str}")
                    
                    with col2:
                        # æ˜¾ç¤ºè¯¦æƒ…
                        evidence_urls = deal.get('evidence_urls', [])
                        if evidence_urls:
                            st.markdown("**è¯æ®é“¾æ¥**:")
                            if isinstance(evidence_urls, list):
                                for url in evidence_urls[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                                    st.markdown(f"- [{url[:50]}...]({url})" if len(url) > 50 else f"- [{url}]({url})")
                            else:
                                st.text(str(evidence_urls)[:100])
                    
                    st.divider()
                    
                    # ä¸‰ä¸ªæŒ‰é’®ï¼ˆå¹¶æ’ï¼‰
                    col_btn1, col_btn2, col_btn3, col_btn4 = st.columns(4)
                    
                    with col_btn1:
                        if st.button(f"âœ… Intro", key=f"intro_{deal_id}", type="primary"):
                            try:
                                action_data = {
                                    "deal_id": deal_id,
                                    "action": "intro",
                                    "notes": "",
                                    "created_at": datetime.utcnow().isoformat()
                                }
                                supabase_client.table("deal_actions").insert(action_data).execute()
                                
                                # è°ƒè¯•æ¨¡å¼ï¼šè®°å½•å†™å…¥çš„ payload
                                if st.session_state.get("debug_mode", False):
                                    payload_copy = action_data.copy()
                                    if payload_copy.get("notes") and len(payload_copy["notes"]) > 50:
                                        payload_copy["notes"] = payload_copy["notes"][:50] + "..."
                                    st.session_state["last_action_payload"] = payload_copy
                                    st.sidebar.success(f"âœ… {action_data['action']} | deal_id: {deal_id[:8]}...")
                                
                                st.success("âœ… å·²è®°å½•: Intro")
                                st.rerun()
                            except Exception as e:
                                st.error(f"âŒ è®°å½•å¤±è´¥: {e}")
                    
                    with col_btn2:
                        if st.button(f"ğŸ‘€ Watch", key=f"watch_{deal_id}", type="secondary"):
                            try:
                                action_data = {
                                    "deal_id": deal_id,
                                    "action": "watch",
                                    "notes": "",
                                    "created_at": datetime.utcnow().isoformat()
                                }
                                supabase_client.table("deal_actions").insert(action_data).execute()
                                
                                # è°ƒè¯•æ¨¡å¼ï¼šè®°å½•å†™å…¥çš„ payload
                                if st.session_state.get("debug_mode", False):
                                    payload_copy = action_data.copy()
                                    if payload_copy.get("notes") and len(payload_copy["notes"]) > 50:
                                        payload_copy["notes"] = payload_copy["notes"][:50] + "..."
                                    st.session_state["last_action_payload"] = payload_copy
                                    st.sidebar.success(f"âœ… {action_data['action']} | deal_id: {deal_id[:8]}...")
                                
                                st.success("âœ… å·²è®°å½•: Watch")
                                st.rerun()
                            except Exception as e:
                                st.error(f"âŒ è®°å½•å¤±è´¥: {e}")
                    
                    with col_btn3:
                        if st.button(f"âŒ Pass", key=f"pass_{deal_id}", type="secondary"):
                            try:
                                action_data = {
                                    "deal_id": deal_id,
                                    "action": "pass",
                                    "notes": "",
                                    "created_at": datetime.utcnow().isoformat()
                                }
                                supabase_client.table("deal_actions").insert(action_data).execute()
                                
                                # è°ƒè¯•æ¨¡å¼ï¼šè®°å½•å†™å…¥çš„ payload
                                if st.session_state.get("debug_mode", False):
                                    payload_copy = action_data.copy()
                                    if payload_copy.get("notes") and len(payload_copy["notes"]) > 50:
                                        payload_copy["notes"] = payload_copy["notes"][:50] + "..."
                                    st.session_state["last_action_payload"] = payload_copy
                                    st.sidebar.success(f"âœ… {action_data['action']} | deal_id: {deal_id[:8]}...")
                                
                                st.success("âœ… å·²è®°å½•: Pass")
                                st.rerun()
                            except Exception as e:
                                st.error(f"âŒ è®°å½•å¤±è´¥: {e}")
                    
                    with col_btn4:
                        # æ˜¾ç¤ºå·²æœ‰çš„æ“ä½œè®°å½•
                        if has_action:
                            actions = deal_actions_map[deal_id]
                            st.caption(f"å·²æ“ä½œ: {', '.join(actions)}")
    
    except Exception as e:
        st.error(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
        st.exception(e)

elif page == "ğŸ“„ å‘¨æŠ¥":
    # ========== å‘¨æŠ¥é¡µé¢ ==========
    st.title("ğŸ“„ å‘¨æŠ¥")
    st.caption("æŸ¥çœ‹æœ€æ–°ç”Ÿæˆçš„é›·è¾¾å‘¨æŠ¥")
    
    if not HAS_SUPABASE:
        st.error("âŒ Supabase å®¢æˆ·ç«¯æœªå®‰è£…ã€‚è¯·è¿è¡Œ: `pip install supabase`")
        st.stop()
    
    # è·å– Supabase å®¢æˆ·ç«¯ï¼ˆå‰ç«¯åªä½¿ç”¨ ANON_KEYï¼‰
    supabase_client = get_supabase_client(use_service_role=False)
    
    if not supabase_client:
        st.error("âŒ Supabase è¿æ¥å¤±è´¥ã€‚è¯·æ£€æŸ¥ç¯å¢ƒå˜é‡ï¼š")
        st.code("""
SUPABASE_URL=your_supabase_url
SUPABASE_ANON_KEY=your_supabase_anon_key
        """)
        st.info("ğŸ’¡ è¯·åœ¨ `.env` æ–‡ä»¶ä¸­è®¾ç½®è¿™äº›ç¯å¢ƒå˜é‡ï¼Œæˆ–ä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡ã€‚")
        st.stop()
    
    try:
        # è·å–æœ€æ–°ä¸€æ¡å‘¨æŠ¥ï¼ˆæŒ‰ week_start å€’åºï¼‰
        response = supabase_client.table("weekly_reports")\
            .select("*")\
            .order("week_start", desc=True)\
            .limit(1)\
            .execute()
        
        reports = response.data if hasattr(response, 'data') else []
        
        # è°ƒè¯•æ¨¡å¼ï¼šè®°å½•å‘¨æŠ¥è¯»å–ç»“æœ
        if st.session_state.get("debug_mode", False):
            if not reports:
                st.session_state["debug_weekly_report"] = "no reports"
            else:
                report = reports[0]
                week_start = report.get('week_start', '')
                # ä¼˜å…ˆè¯»å– markdown å­—æ®µï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å›é€€åˆ° contentï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
                content = report.get('markdown', '') or report.get('content', '')
                st.session_state["debug_weekly_report"] = {
                    "week_start": week_start,
                    "markdown_length": len(content) if content else 0
                }
        
        if not reports:
            st.info("ğŸ“­ æš‚æ— å‘¨æŠ¥ã€‚é›·è¾¾æŠ“å–ä»»åŠ¡è¿è¡Œåï¼Œå‘¨æŠ¥ä¼šè‡ªåŠ¨ç”Ÿæˆã€‚")
            st.info("ğŸ’¡ å‘¨æŠ¥ä¼šåœ¨æ¯å‘¨ä¸€å’Œå‘¨ä¸‰è‡ªåŠ¨ç”Ÿæˆï¼ˆé€šè¿‡ GitHub Actionsï¼‰ã€‚")
        else:
            report = reports[0]
            week_start = report.get('week_start', '')
            # ä¼˜å…ˆè¯»å– markdown å­—æ®µï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å›é€€åˆ° contentï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
            content = report.get('markdown', '') or report.get('content', '')
            created_at = report.get('created_at', '')
            
            # è§£æ week_start æ—¥æœŸ
            try:
                if "T" in week_start:
                    week_dt = datetime.fromisoformat(week_start.replace('Z', '+00:00'))
                    week_start_display = week_dt.strftime("%Y-%m-%d")
                    week_end_display = (week_dt + timedelta(days=6)).strftime("%Y-%m-%d")
                else:
                    week_start_display = week_start
                    week_end_display = "æœªçŸ¥"
            except:
                week_start_display = week_start
                week_end_display = "æœªçŸ¥"
            
            # æ˜¾ç¤ºå‘¨æŠ¥ä¿¡æ¯
            col1, col2 = st.columns([2, 1])
            with col1:
                st.markdown(f"**å‘¨æŠ¥æœŸé—´**: {week_start_display} è‡³ {week_end_display}")
            
            with col2:
                if created_at:
                    try:
                        if "T" in created_at:
                            created_dt = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                            created_display = created_dt.strftime("%Y-%m-%d %H:%M")
                        else:
                            created_display = created_at
                    except:
                        created_display = created_at
                    st.caption(f"ç”Ÿæˆæ—¶é—´: {created_display}")
            
            st.divider()
            
            # ä¸‹è½½æŒ‰é’®
            download_filename = f"weekly_report_{week_start_display.replace('-', '')}.md"
            st.download_button(
                label="â¬‡ï¸ ä¸‹è½½å‘¨æŠ¥ (Markdown)",
                data=content,
                file_name=download_filename,
                mime="text/markdown",
                key="download_weekly_report"
            )
            
            st.divider()
            
            # æ¸²æŸ“ Markdown å†…å®¹
            st.markdown(content)
    
    except Exception as e:
        st.error(f"âŒ åŠ è½½å‘¨æŠ¥å¤±è´¥: {e}")
        st.exception(e)
