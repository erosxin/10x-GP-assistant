"""ä¸€æ¬¡æ€§å›å¡«è„šæœ¬ï¼šå¡«å…… deals è¡¨çš„ canonical_name, one_liner, evidence_urls å­—æ®µ

ä½¿ç”¨æ–¹æ³•ï¼š
    python backfill_deals_fields.py

è¯´æ˜ï¼š
    - åªæ›´æ–° canonical_name/one_liner/evidence_urls ä¸ºç©ºçš„è®°å½•
    - canonical_name = titleï¼ˆæˆ–æ¸…æ´—åçš„ titleï¼‰
    - one_liner = left(description, 120)
    - evidence_urls = array[url]
"""

# é¦–å…ˆåŠ è½½ç¯å¢ƒå˜é‡ï¼ˆåœ¨ä»»ä½•å…¶ä»–å¯¼å…¥æˆ–è¯»å–ç¯å¢ƒå˜é‡ä¹‹å‰ï¼‰
from pathlib import Path
from dotenv import load_dotenv
load_dotenv(dotenv_path=Path(__file__).parent / ".env", override=False)

import os
import re
import hashlib
from datetime import datetime
from urllib.parse import urlparse, urlunparse, parse_qs, urlencode
from db.supabase_db import get_supabase_client


def clean_canonical_name(title: str) -> str:
    """æ¸…æ´—æ ‡é¢˜ï¼Œæå–è§„èŒƒåç§°ï¼ˆå»æ‰ç½‘ç«™åç¼€/åˆ†éš”ç¬¦ååŠæ®µï¼‰"""
    if not title:
        return ""
    
    # å¸¸è§åˆ†éš”ç¬¦æ¨¡å¼ï¼šå»é™¤ " - Company", " | TechCrunch", " - The Verge" ç­‰
    patterns = [
        r'\s*[-â€“â€”]\s*[^|]+$',  # åŒ¹é… " - XXX" åˆ°æœ«å°¾
        r'\s*\|\s*[^|]+$',      # åŒ¹é… " | XXX" åˆ°æœ«å°¾
        r'\s*::\s*[^:]+$',      # åŒ¹é… " :: XXX" åˆ°æœ«å°¾
    ]
    
    cleaned = title
    for pattern in patterns:
        cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)
    
    # å»é™¤é¦–å°¾ç©ºæ ¼
    cleaned = cleaned.strip()
    
    # å¦‚æœæ¸…æ´—åä¸ºç©ºï¼Œè¿”å›åŸå§‹æ ‡é¢˜
    return cleaned if cleaned else title


def normalize_url(u: str) -> str:
    """è§„èŒƒåŒ– URLï¼šå»æ‰è¿½è¸ªå‚æ•°ï¼Œç»Ÿä¸€æ ¼å¼"""
    if not u:
        return ""
    
    u = u.strip()
    if not u:
        return ""
    
    # æ—  scheme æ—¶è¡¥ https://
    if not u.startswith(('http://', 'https://')):
        u = 'https://' + u
    
    try:
        parsed = urlparse(u)
        
        # scheme å¼ºåˆ¶ä¸º https
        scheme = 'https'
        
        # netloc å°å†™ï¼Œå»æ‰å‰ç¼€ www.
        netloc = parsed.netloc.lower()
        if netloc.startswith('www.'):
            netloc = netloc[4:]
        
        # path å»æ‰æœ«å°¾ /ï¼ˆæ ¹è·¯å¾„é™¤å¤–ï¼‰
        path = parsed.path
        if path != '/' and path.endswith('/'):
            path = path[:-1]
        
        # query å‚æ•°è¿‡æ»¤
        query_dict = parse_qs(parsed.query, keep_blank_values=False)
        
        # è¿‡æ»¤è¿½è¸ªå‚æ•°
        tracking_params = [
            'ref', 'ref_src', 'fbclid', 'gclid', 'igshid', 
            'mc_cid', 'mc_eid', 'spm', 'source', 'mkt_tok'
        ]
        filtered_query = {}
        for key, values in query_dict.items():
            if key.startswith('utm_'):
                continue
            if key.lower() in [p.lower() for p in tracking_params]:
                continue
            filtered_query[key] = values[0] if values else ''
        
        # æŒ‰ key æ’åºåé‡ç»„
        if filtered_query:
            sorted_params = sorted(filtered_query.items())
            query = urlencode(sorted_params)
        else:
            query = ''
        
        # ä¸¢å¼ƒ fragment
        fragment = ''
        
        normalized = urlunparse((scheme, netloc, path, parsed.params, query, fragment))
        return normalized
        
    except Exception as e:
        print(f"âš ï¸ URL è§„èŒƒåŒ–å¤±è´¥ ({u[:50]}...): {e}")
        return u.strip()


def get_hostname(url: str) -> str:
    """ä» URL æå–ä¸»æœºå"""
    try:
        parsed = urlparse(url)
        return parsed.netloc or ""
    except:
        return ""


def compute_dedupe_key(url: str, hostname: str, canonical_name: str, title: str) -> str:
    """è®¡ç®— dedupe_keyï¼ˆä»¥è§„èŒƒåŒ– URL ä¸ºä¸»ï¼‰"""
    normalized_url = normalize_url(url)
    if normalized_url:
        url_hash = hashlib.sha1(normalized_url.encode()).hexdigest()
        return f"url:{url_hash}"
    
    key_str = f"{hostname}|{canonical_name or title}".lower().strip()
    key_hash = hashlib.sha1(key_str.encode()).hexdigest()
    return f"ht:{key_hash}"


def generate_one_liner(description: str) -> str:
    """ä» description ç”Ÿæˆ one_linerï¼ˆå»æ¢è¡Œï¼Œæˆªæ–­åˆ°120å­—ï¼‰"""
    if not description:
        return ""
    
    # å»é™¤æ¢è¡Œå’Œå¤šä½™ç©ºæ ¼
    cleaned = " ".join(description.split())
    
    # æˆªæ–­åˆ°120å­—
    if len(cleaned) > 120:
        # å°½é‡åœ¨æ ‡ç‚¹ç¬¦å·å¤„æˆªæ–­
        truncated = cleaned[:120]
        for punct in ['.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ', ';', 'ï¼›']:
            last_punct = truncated.rfind(punct)
            if last_punct > 80:  # å¦‚æœæ ‡ç‚¹ä½ç½®åœ¨80å­—ä¹‹å
                return truncated[:last_punct + 1]
        return truncated[:117] + "..."
    else:
        return cleaned


def main():
    """ä¸»å‡½æ•°ï¼šå›å¡« deals è¡¨çš„ä¸‰ä¸ªå­—æ®µ"""
    print("=" * 60)
    print("å¼€å§‹å›å¡« deals è¡¨çš„ canonical_name, one_liner, evidence_urls å­—æ®µ")
    print("=" * 60)
    
    # è·å– Supabase å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨ service role key ä»¥ä¾¿å†™å…¥ï¼‰
    client = get_supabase_client(use_service_role=True)
    if not client:
        print("âŒ æ— æ³•åˆ›å»º Supabase å®¢æˆ·ç«¯ï¼ˆè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡ï¼‰")
        return 1
    
    try:
        # è¯»å–æ‰€æœ‰ dealsï¼ˆåªè¯»å–éœ€è¦çš„å­—æ®µï¼‰
        print("\nğŸ“– è¯»å– deals æ•°æ®...")
        response = client.table("deals")\
            .select("id,title,description,url,canonical_name,one_liner,evidence_urls,hostname,dedupe_key")\
            .execute()
        
        all_deals = response.data if hasattr(response, 'data') else []
        print(f"    âœ… å…±è¯»å– {len(all_deals)} æ¡è®°å½•")
        
        # ç­›é€‰éœ€è¦æ›´æ–°çš„è®°å½•ï¼ˆurl/evidence_urls/dedupe_key éœ€è¦è§„èŒƒåŒ–æˆ–ä¸ºç©ºï¼‰
        deals_to_update = []
        for deal in all_deals:
            canonical_name = deal.get("canonical_name", "").strip() if deal.get("canonical_name") else ""
            one_liner = deal.get("one_liner", "").strip() if deal.get("one_liner") else ""
            evidence_urls = deal.get("evidence_urls", [])
            evidence_urls = evidence_urls if isinstance(evidence_urls, list) and len(evidence_urls) > 0 else []
            url = deal.get("url", "")
            dedupe_key = deal.get("dedupe_key", "").strip() if deal.get("dedupe_key") else ""
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆä¸‰ä¸ªå­—æ®µä¸ºç©ºï¼Œæˆ– url åŒ…å«è¿½è¸ªå‚æ•°ï¼Œæˆ– dedupe_key ä¸ºç©ºï¼‰
            url_needs_normalize = url and any(param in url for param in ['utm_', 'ref=', 'fbclid=', 'gclid='])
            evidence_needs_normalize = evidence_urls and any(any(param in str(u) for param in ['utm_', 'ref=', 'fbclid=']) for u in evidence_urls)
            
            if not canonical_name or not one_liner or not evidence_urls or url_needs_normalize or evidence_needs_normalize or not dedupe_key:
                deals_to_update.append(deal)
        
        print(f"    ğŸ“ éœ€è¦æ›´æ–°çš„è®°å½•æ•°: {len(deals_to_update)}")
        
        if not deals_to_update:
            print("\nâœ… æ‰€æœ‰è®°å½•éƒ½å·²è§„èŒƒåŒ–ï¼Œæ— éœ€æ›´æ–°")
            return 0
        
        # æ‰¹é‡æ›´æ–°
        updated_count = 0
        failed_count = 0
        url_updated_count = 0
        evidence_updated_count = 0
        dedupe_key_updated_count = 0
        
        print(f"\nğŸ”„ å¼€å§‹æ‰¹é‡æ›´æ–°...")
        for idx, deal in enumerate(deals_to_update, 1):
            deal_id = deal.get("id")
            title = deal.get("title", "")
            description = deal.get("description", "")
            url = deal.get("url", "")
            
            if not deal_id:
                print(f"    âš ï¸ è·³è¿‡æ—  ID çš„è®°å½•: {title[:50]}")
                failed_count += 1
                continue
            
            # è§„èŒƒåŒ– URL
            normalized_url = normalize_url(url) if url else ""
            
            # è§„èŒƒåŒ– evidence_urls
            existing_evidence_urls = deal.get("evidence_urls", [])
            if isinstance(existing_evidence_urls, list) and len(existing_evidence_urls) > 0:
                normalized_evidence_urls = []
                seen = set()
                for u in existing_evidence_urls:
                    normalized = normalize_url(str(u))
                    if normalized and normalized not in seen:
                        normalized_evidence_urls.append(normalized)
                        seen.add(normalized)
            else:
                normalized_evidence_urls = [normalized_url] if normalized_url else []
            
            # ç”Ÿæˆä¸‰ä¸ªå­—æ®µï¼ˆå¦‚æœä¸ºç©ºï¼‰
            # canonical_name: ä¼˜å…ˆç”¨æ¸…æ´—åçš„ title
            if not deal.get("canonical_name") or not deal.get("canonical_name", "").strip():
                canonical_name = clean_canonical_name(title) if title else ""
            else:
                canonical_name = deal.get("canonical_name")
            
            # one_liner: ä» description æˆªæ–­
            if not deal.get("one_liner") or not deal.get("one_liner", "").strip():
                one_liner = generate_one_liner(description) if description else ""
            else:
                one_liner = deal.get("one_liner")
            
            # è®¡ç®— dedupe_key
            hostname = deal.get("hostname", "") or get_hostname(url)
            new_dedupe_key = compute_dedupe_key(url, hostname, canonical_name, title)
            
            # æ„å»ºæ›´æ–°æ•°æ®ï¼ˆæ€»æ˜¯æ›´æ–° url, evidence_urls, dedupe_key ä»¥ç¡®ä¿è§„èŒƒåŒ–ï¼‰
            update_data = {}
            
            # æ€»æ˜¯æ›´æ–° urlï¼ˆè§„èŒƒåŒ–ï¼‰
            if normalized_url:
                update_data["url"] = normalized_url
                if normalized_url != url:
                    url_updated_count += 1
            
            # æ€»æ˜¯æ›´æ–° evidence_urlsï¼ˆè§„èŒƒåŒ–ï¼‰
            update_data["evidence_urls"] = normalized_evidence_urls
            if normalized_evidence_urls != existing_evidence_urls:
                evidence_updated_count += 1
            
            # æ€»æ˜¯æ›´æ–° dedupe_key
            update_data["dedupe_key"] = new_dedupe_key
            if new_dedupe_key != deal.get("dedupe_key", ""):
                dedupe_key_updated_count += 1
            
            # æ›´æ–°ä¸‰ä¸ªå­—æ®µï¼ˆå¦‚æœä¸ºç©ºï¼‰
            if not deal.get("canonical_name") or not deal.get("canonical_name", "").strip():
                update_data["canonical_name"] = canonical_name
            if not deal.get("one_liner") or not deal.get("one_liner", "").strip():
                update_data["one_liner"] = one_liner
            
            # æ·»åŠ  updated_at
            update_data["updated_at"] = datetime.utcnow().isoformat()
            
            try:
                # æ›´æ–°è®°å½•
                client.table("deals")\
                    .update(update_data)\
                    .eq("id", deal_id)\
                    .execute()
                
                updated_count += 1
                if idx % 10 == 0:
                    print(f"    âœ… å·²æ›´æ–° {idx}/{len(deals_to_update)} æ¡è®°å½•...")
            except Exception as e:
                print(f"    âŒ æ›´æ–°å¤±è´¥ (id: {deal_id}, title: {title[:50]}): {e}")
                failed_count += 1
        
        print(f"\nğŸ“Š å›å¡«å®Œæˆç»Ÿè®¡:")
        print(f"  - æ€»è®°å½•æ•°: {len(all_deals)}")
        print(f"  - éœ€è¦æ›´æ–°: {len(deals_to_update)}")
        print(f"  - æˆåŠŸæ›´æ–°: {updated_count}")
        print(f"  - æ›´æ–°å¤±è´¥: {failed_count}")
        print(f"  - URL è§„èŒƒåŒ–: {url_updated_count} æ¡")
        print(f"  - evidence_urls è§„èŒƒåŒ–: {evidence_updated_count} æ¡")
        print(f"  - dedupe_key ç”Ÿæˆ/æ›´æ–°: {dedupe_key_updated_count} æ¡")
        
        if failed_count > 0:
            print(f"\nâš ï¸ æœ‰ {failed_count} æ¡è®°å½•æ›´æ–°å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯æ—¥å¿—")
            return 1
        else:
            print(f"\nâœ… æ‰€æœ‰è®°å½•å›å¡«æˆåŠŸï¼")
            return 0
            
    except Exception as e:
        print(f"\nâŒ å›å¡«è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    import sys
    sys.exit(main())
